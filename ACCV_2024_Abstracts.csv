Title,Abstract,URL
Generative Self-Supervised Learning for Medical Image Classification,"This paper introduces the generative self-supervised learning method in medical image recognition. We use the generative models in two main ways: 1) creating diversified training data and 2) learning domain-aligned pretext knowledge for self-supervised learning. In general, gathering real-world medical data can be quite difficult, so we generate synthetic training data using the diffusion model with elaborated prompts. We also propose a domain-aligned generative approach for our self-supervised learning algorithm. Our approach learns the robust visual representation from the masked autoencoder model with adaptive instance normalization. It minimizes the domain gap between our synthetic training data and real-world data when training the masked autoencoder model. In this self-supervised learning process, we rely solely on generative data, allowing our approach to achieve state-of-the-art performance without utilizing any real-world medical data. We demonstrate that our approach surpasses the previous best results by significant margins of CheXpert, COVIDx, and ChestX-ray14 datasets. These results highlight the potential of generated data in medical image recognition, a field that has historically faced data scarcity. We open-source our implementation of the generative self-supervised learning method at: https://github.com/inhyukpark2/gen-ssl.",https://openaccess.thecvf.com/content/ACCV2024/html/Park_Generative_Self-Supervised_Learning_for_Medical_Image_Classification_ACCV_2024_paper.html
Depth Attention for Robust RGB Tracking,"RGB video object tracking is a vital task in computer vision, yet its effectiveness is often limited by the lack of depth information, which is crucial for handling very challenging scenarios e.g., occlusions. In this work, we unveil a new framework that leverages monocular depth estimation to counter occlusions and motion blur in RGB video tracking. Specifically, our work introduces following contributions. (a) To our knowledge, we are the first to  propose a Gaussian attention mechanism and provide a simple framework that allows seamlessly integration of depth insights with cutting-edge tracking algorithms, without RGB-Depth cameras, elevating accuracy and robustness. (b) We provide rigorous mathematical proofs to reveal the benefits of our method and offer a Fourier analysis to provide additional insights. (c) We provide extensive experiments on six challenging tracking benchmarks. Results demonstrate that our method provides consistent gains over several strong baselines. We believe that our method will open up new possibilities for more sophisticated VOT solutions in real-world scenarios. Our code and models will be publicly released.",https://openaccess.thecvf.com/content/ACCV2024/html/Liu_Depth_Attention_for_Robust_RGB_Tracking_ACCV_2024_paper.html
MedBLIP: Bootstrapping Language-Image Pre-training from 3D Medical Images and Texts,"Vision language pretraining (VLP) models have proven effective in numerous computer vision applications. In this paper, we focus on developing a VLP model for the medical domain to facilitate computer-aided diagnoses (CAD) based on image scans and text descriptions from electronic health records. To achieve this, we introduce MedBLIP, a lightweight CAD system that bootstraps VLP from off-the-shelf frozen pre-trained image encoders and large language models. We incorporate a MedQFormer module to bridge the gap between 3D medical images and 2D pre-trained image encoders and language models. To evaluate the effectiveness of our MedBLIP, we have collected over 30,000 image volumes from five public Alzheimer's disease (AD) datasets: ADNI, NACC, OASIS, AIBL, and MIRIAD. On this large-scale AD dataset, our model demonstrates impressive performance in zero-shot classification of healthy, mild cognitive impairment (MCI), and AD subjects, and also shows its capability in medical visual question answering (VQA) on the M3D-VQA-AD dataset. The code and pre-trained models are available at https://github.com/Qybc/MedBLIP.",https://openaccess.thecvf.com/content/ACCV2024/html/Chen_MedBLIP_Bootstrapping_Language-Image_Pre-training_from_3D_Medical_Images_and_Texts_ACCV_2024_paper.html
Neural Active Structure-from-Motion in Dark and Textureless Environment,"Active 3D measurement, especially structured light (SL) has been widely used in various fields for its robustness against textureless or equivalent surfaces by low light illumination. In addition, reconstruction of large scenes by moving the SL system has become popular,  however, there have been few practical techniques to obtain the system's precise pose information only from images, since most conventional techniques are based on image features, which cannot be retrieved under textureless environments. In this paper, we propose a simultaneous shape reconstruction and pose estimation technique for SL systems from an image set where sparsely projected patterns onto the scene are observed (i.e. no scene texture information), which we call Active SfM. To achieve this, we propose a full optimization framework of the volumetric shape that employs neural signed distance fields (Neural-SDF) for SL with the goal of not only reconstructing the scene shape but also estimating the poses for each motion of the system. Experimental results show that the proposed method is able to achieve accurate shape reconstruction as well as pose estimation from images where only projected patterns are observed.",https://openaccess.thecvf.com/content/ACCV2024/html/Ichimaru_Neural_Active_Structure-from-Motion_in_Dark_and_Textureless_Environment_ACCV_2024_paper.html
LoGDesc: Local geometric features aggregation for robust point cloud registration,"This paper introduces a new hybrid descriptor for 3D point matching and point cloud registration, combining local geometrical properties and learning-based feature propagation for each point's neighborhood structure description. The proposed architecture first extracts prior geometrical information by computing each point's planarity, anisotropy, and omnivariance using a Principal Components Analysis (PCA). This prior information is completed by a descriptor based on the normal vectors estimated thanks to constructing a neighborhood based on triangles. The final geometrical descriptor is propagated between the points using local graph convolutions and attention mechanisms. The new feature extractor is evaluated on ModelNet40, Bunny Stanford dataset, KITTI and MVP (Multi-View Partial)-RG for point cloud registration and shows interesting results, particularly on noisy and low overlapping point clouds.",https://openaccess.thecvf.com/content/ACCV2024/html/Slimani_LoGDesc_Local_geometric_features_aggregation_for_robust_point_cloud_registration_ACCV_2024_paper.html
OmniFusion: Exemplar-based Video Colorization Using OmniMotion and Diffusion Priors,"Exemplar-based video colorization is a challenging task that involves the consistent propagation of colors across frames and the reasonable inference of colors from grayscale within frames. This paper proposes a novel video colorization method called OmniFusion, which iteratively completes the video colorization through two following steps. In the inter-frame propagation step, OmniMotion establishes correspondences between pixels across video frames. Any grayscale pixel can be queried whether a corresponding pixel and color are available from the exemplar according to such correspondences. Consequently, the processed images may still contain regions lacking color. In the intra-frame inpainting step, diffusion model provides these grayscale regions in a frame with plausible colors. The colorized frame is then fed into the first step as an exemplar, accepting queries from all uncolored pixels. This iterative process continues until all pixels are colorized. Evaluations indicate that OmniFusion achieves excellent performance in video colorization, surpassing existing methods in terms of color fidelity and visual quality.",https://openaccess.thecvf.com/content/ACCV2024/html/Fang_OmniFusion_Exemplar-based_Video_Colorization_Using_OmniMotion_and_Diffusion_Priors_ACCV_2024_paper.html
Contrastive Learning using Synthetic Images Generated from Real Images,"The effectiveness of pre-training using large-scale natural image datasets has been demonstrated for situations in which there are limited available real images. However, some research has shown that models pre-trained using natural images cannot achieve sufficient performance on non-natural images taken under special circumstances or with special measurement devices. Although more general pre-training methods that use synthetic images such as random pattern images or noise images are a promising approach for such cases, their effectiveness depends on downstream tasks. To deal with this problem, we propose a contrastive learning framework using synthetic images generated from real images of downstream tasks to directly learn feature representations suitable for downstream tasks of non-natural images. Image classification experiments are performed on five non-natural image datasets mimicking real-world application with little available data, and these demonstrate that the proposed method achieves higher average classification accuracy compared with pre-training using ImageNet1k or existing synthetic images with an improvement of over 6.5 points.",https://openaccess.thecvf.com/content/ACCV2024/html/Sasaya_Contrastive_Learning_using_Synthetic_Images_Generated_from_Real_Images_ACCV_2024_paper.html
iS-MAP: Neural Implicit Mapping and Positioning for Structural Environments,"This work presents iS-MAP, a neural implicit RGB-D SLAM approach based on multi-scale hybrid representation in structural environments. iS-MAP encodes the scene using an efficient hybrid feature representation, which combines a 3D hash grid and multi-scale 2D feature planes. This hybrid representation is then decoded into TSDF and RGB values, leading to robust reconstruction and multilevel detail understanding. Additionally, we introduce Manhattan matching loss and structural consistency loss to fully incorporate the prior constraints of structured planes and lines. Compared with only color and depth losses, our structured losses are capable of guiding network optimization at the semantic level, resulting in more reasonable scene regularization. Experimental results on synthetic and real-world scene datasets demonstrate that our approach performs either better or competitive to existing neural implicit RGB-D SLAM methods in mapping and tracking accuracy, and predicts the most plausible reconstruction results for the unobserved structural regions. The source code will be released soon.",https://openaccess.thecvf.com/content/ACCV2024/html/Wang_iS-MAP_Neural_Implicit_Mapping_and_Positioning_for_Structural_Environments_ACCV_2024_paper.html
A Universal Structure of YOLO Series Small Object Detection Models,"The YOLO series detection models play a crucial role in target detection tasks. However, these models are typically trained on datasets with standard angles. For datasets like Visdron2021 and Tinyperson, there are challenges related to small, dense, and numerous objects that conventional object detection models struggle to detect effectively. Therefore, we propose a universal structure for all YOLO series models to enhance their capability to detect small objects. We first use a large-scale feature map as a new detection branch to address the issue of feature loss with small objects. Secondly, we have developed a detail-guide-block (DGB) to enhance the model's ability in detailed detection, along with a feature-refine-module (FRM) aimed at mitigating the problem of feature flattening caused by upsampling. Finally, we removed the fourth detection branch that did not significantly improve detection accuracy, which can to some extent improve the execution speed of the model and reduce its complexity. We have ported our structure on YOLOX, YOLOv7, and YOLOv8, and conducted extensive experiments on Visdrone2021 and Tinypeson datasets. The experimental data demonstrate that our improved models consistently outperform the original model in terms of performance.",https://openaccess.thecvf.com/content/ACCV2024/html/Hu_A_Universal_Structure_of_YOLO_Series_Small_Object_Detection_Models_ACCV_2024_paper.html
Decoupled DETR For Few-shot Object Detection,"The efficient technique for dealing with severe data-hungry issues in object detection, known as Few-shot object detection (FSOD), has been widely explored. However, FSOD encounters some notable challenges such as the model's natural bias towards pre-training data and the inherent defects present in the existing models. In this paper, we introduce improved methods for the FSOD problem based on DETR structures: (i) To reduce bias from pre-training classes (i.e. many-shot base classes), we investigate the impact of decoupling the parameters of pre-training classes and fine-tuning classes (i.e. few-shot novel classes) in various ways. As a result, we propose a ""base-novel categories decoupled DETR (DeDETR)"" network for FSOD. (ii) To further improve the efficiency of the DETR's skip connection structure, we explore varied skip connection types in the DETR's encoder and decoder. Subsequently, we introduce a unified decoder module that dynamically blends decoder layers to generate the output feature. Our model's effectiveness is evaluated using PASCAL VOC and MSCOCO datasets. Our results indicate that our proposed module consistently improves performance by 5% to 10% in both fine-tuning and meta-learning frameworks and has surpassed the top scores achieved in recent studies.",https://openaccess.thecvf.com/content/ACCV2024/html/Shangguan_Decoupled_DETR_For_Few-shot_Object_Detection_ACCV_2024_paper.html
GaitW: Enhancing Gait Recognition in the Wild using Dynamic Information,"Success of modern deep neural networks (DNNs) for gait recognition on in-the-lab datasets such as CASIA-B and OU-MVLP have encouraged the community to aim for more challenging, and in-the-wild  datasets such as GREW and Gait3D. The new datasets contain large variations in silhouettes due to change in camera pose, clothing, accessories, as well as occlusion, thus posing huge challenges to existing techniques and training strategies for gait recognition. We posit that to achieve high accuracy in in-the-wild datasets, explicitly leveraging dynamic information in gait samples during training is imperative. We propose a novel transformer based architecture for gait recognition specifically leveraging such dynamic information. The novel contributions include: (1) We propose interleaved spatial and temporal encoders to attend to positioning of various body parts in a frame, and movement of a body part across the sample, respectively. (2) We propose a novel dynamic information inspired curriculum, where we first determine the hardness of a sample based on the disparity between representations of its frame-wise silhouettes (FWSs) and GEI. The model is trained using easier samples first, followed by progressively difficult samples. (3) We propose mask-annealing for silhouettes using Gait Energy Images (GEIs), which attends to silhouette contours and allows a model to learn robust silhouette shape representation. We report a significant improvement in accuracy (in %) of 96.9, 92.9, 81.2, and 67.7 on benchmark CASIA-B, OU-MVLP, GREW, and Gait3D datasets respectively using our technique, against the current state-of-the-art (SOTA) accuracy of 96.9, 92.4, 77.4, and 67.0 by MSGR [43] (TMM23), HSTGait(ICCV23), SkeletonGait (AAAI24), and  QAGait (AAAI24) respectively. In a significant departure from the current trend, and as evident from the above numbers, the proposed  technique sets up simultaneous SOTA on most prominent in-the-lab as well as in-the-wild datasets. Complete source code and trained models of our method will be publicly available.",https://openaccess.thecvf.com/content/ACCV2024/html/Thapar_GaitW_Enhancing_Gait_Recognition_in_the_Wild_using_Dynamic_Information_ACCV_2024_paper.html
ELLAR: An Action Recognition Dataset for Extremely Low-Light Conditions with Dual Gamma Adaptive Modulation,"In this paper, we address the challenging problem of action recognition in extremely low-light environments. Currently, available datasets built under low-light settings are not truly representative of extremely dark conditions because they have a sufficient signal-tonoise ratio, making them visible with simple low-light image enhancement methods. Due to the lack of datasets captured under extremely low-light conditions, we present a new dataset with more than 12K video samples, named Extremely Low-Light condition Action Recognition (ELLAR). This dataset is constructed to reflect the characteristics of extremely low-light conditions where the visibility of videos is corrupted by overwhelming noise and blurs. ELLAR also covers a diverse range of dark settings within the scope of extremely low-light conditions. Furthermore, we propose a simple yet strong baseline method, leveraging a Mixture of Experts in gamma intensity correction, which enables models to be flexible and adaptive to a range of low illuminance levels. Our approach significantly surpasses state-of-the-art results by 3.39% top1 accuracy on ELLAR dataset. The dataset and code are available at https://github.com/knu-vis/ELLAR.",https://openaccess.thecvf.com/content/ACCV2024/html/Ha_ELLAR_An_Action_Recognition_Dataset_for_Extremely_Low-Light_Conditions_with_ACCV_2024_paper.html
EmoTalker: Audio Driven Emotion Aware Talking Head Generation,"Talking head synthesis aims to create videos of a person speaking with accurately synchronized lip movements and natural facial expressions that correspond to the driving audio. However, previous approaches have used reference frames or extra labels to control emotions and facial expressions, which disentangle utterance and expression and ignore the impact of audio fluctuations on face motions, e.g., head pose, facial expressions and emotions. In this work, we present EmoTalker, which generates arbitrary identities with diverse and natural facial expressions from audio, without relying on driving frames or emotion labels as input. To achieve this, we present frames as a sequence of 3D motion coefficients of 3DMM representation and separate them into lip-related coefficients and the remaining (head pose, expressions) as facial motions. To model lip movement, we start with a pre-trained audio encoder and map it to the corresponding lip representation. While for facial motions, we employ a two-stage training strategy: 1) We first project facial motions into a finite space of the codebook embedded with emotion-aware facial expression priors. 2) Moreover, a cross-modal Transformer is devised to explicitly model the correlations between audio and different types of facial motions. Experimental results and user studies show our model achieves state-of-the-art performance on the emotional audio-visual dataset and produces more realistic talking head videos with synchronized lip movement and vivid facial expressions. Our codes are available at \href https://github.com/xiaoqian-shen/EmoTalker  https://github.com/xiaoqian-shen/EmoTalker .",https://openaccess.thecvf.com/content/ACCV2024/html/Shen_EmoTalker_Audio_Driven_Emotion_Aware_Talking_Head_Generation_ACCV_2024_paper.html
Bringing Masked Autoencoders Explicit Contrastive Properties for Point Cloud Self-Supervised Learning,"Contrastive learning (CL) for Vision Transformers (ViTs) in image domains has achieved performance comparable to CL for traditional convolutional backbones. However, in 3D point cloud pretraining with ViTs, masked autoencoder (MAE) modeling remains dominant. This raises the question: Can we take the best of both worlds? To answer this question, we first empirically validate that integrating MAE-based point cloud pre-training with the standard contrastive learning paradigm, even with meticulous design, can lead to a decrease in performance. To address this limitation, we reintroduce CL into the MAE-based point cloud pre-training paradigm by leveraging the inherent contrastive properties of MAE. Specifically, rather than relying on extensive data augmentation as commonly used in the image domain, we randomly mask the input tokens twice to generate contrastive input pairs. Subsequently, a weight-sharing encoder and two identically structured decoders are utilized to perform masked token reconstruction. Additionally, we propose that for an input token masked by both masks simultaneously, the reconstructed features should be as similar as possible. This naturally establishes an explicit contrastive constraint within the generative MAE-based pre-training paradigm, resulting in our proposed method, Point-CMAE. Consequently, Point-CMAE effectively enhances the representation quality and transfer performance compared to its MAE counterpart. Experimental evaluations across various downstream applications, including classification, part segmentation, and few-shot learning, demonstrate the efficacy of our framework in surpassing state-of-the-art techniques under standard ViTs and single-modal settings. The source code and trained models are available at: https://github.com/Amazingren/Point-CMAE.",https://openaccess.thecvf.com/content/ACCV2024/html/Ren_Bringing_Masked_Autoencoders_Explicit_Contrastive_Properties_for_Point_Cloud_Self-Supervised_ACCV_2024_paper.html
Dense Trajectory Fields: Consistent and Efficient Spatio-Temporal Pixel Tracking,"In this paper, we present Dense Trajectory Fields (DTF), a novel low-level holistic approach inspired by optical-flow and trajectory approaches, focusing on both spatial and temporal aspects at once. DTF contains the dense and long-term trajectories of all pixels from a reference frame, over an entire input sequence. We solve it through DTF-Net, a fast and lightweight neural network, comprising 3 main components: (1) a joint iterative refinement of image and motion features over residual layers, (2) token-based Reciprocal Attention clusters and, (3) a Refinement Network that builds patch-to-patch cost-volumes around salient centroid trajectories. We extend the recent Kubric dataset to provide dense ground-truth over all pixels, to train our network. We conduct experiments showing that usual optical-flow and trajectory methods exhibit inconsistencies either temporally or spatially, where DTF-Net offers a better compromise while keeping faster, giving a coherent motion over the entire sequence.",https://openaccess.thecvf.com/content/ACCV2024/html/Tournadre_Dense_Trajectory_Fields_Consistent_and_Efficient_Spatio-Temporal_Pixel_Tracking_ACCV_2024_paper.html
Dual Memory Networks Guided Reverse Distillation for Unsupervised Anomaly Detection,"Visual anomaly detection, which is essential for industrial applications, is typically framed as a one-class classification assignment. Recent techniques employing the teacher-student framework for this task have proven effective in both accuracy and processing time. However, they often assume that real-world anomalies are uncommon, emphasizing anomaly-free data while neglecting the importance of aberrant data. We contend that such a paradigm is suboptimal as it fails to differentiate between regular and irregular situations adequately. To overcome this issue, we proposed a novel Dual Memory Guided Reverse Distillation (DM-GRD) framework to learn feature representations for both standardand abnormal data. Specifically, to obtain anomalous patterns, original images are first augmented with a simple Fourier transformation followed by Perlin noise. A teacher network then randomly receives arbitrary images to extract high-level features. To combat ""forgetting"" and ""over generalization"" difficulties in a student network, two memory banks are introduced to independently store typical and atypical features while maximizing the distance margins between them. Next, a multi-scale feature fusion module is trained to integrate valuable information from the memory banks. Finally, a student network ingests this data to match the instructor network for the same images. Experiments on three industrial benchmark datasets reveal that DM-GRD outperforms current state-of-the-art memory bank and knowledge distillation alternatives, showcasing the robust generalization capability of the proposed framework. The code is publicly available at https://github.com/SKKUAutoLab/DM-GRD.",https://openaccess.thecvf.com/content/ACCV2024/html/Tran_Dual_Memory_Networks_Guided_Reverse_Distillation_for_Unsupervised_Anomaly_Detection_ACCV_2024_paper.html
BiEfficient: Bidirectionally Prompting Vision-Language Models for Parameter-Efficient Video Recognition,"Vision-language models (VLMs) pre-trained on large-scale image-text pairs have shown great success in various image tasks. However, how to efficiently transfer such powerful VLMs into video domain is still an open problem. Given that full finetuning VLMs for video tasks could be computationally expensive, recent studies turn their focus on parameter-efficient finetuning (PEFT). The great potential of VLMs lies in leveraging the bidirectional semantic connections between the two modalities of vision and language. Nevertheless, most current PEFT methods use the vision-only framework and usually ignore the semantic connections between vision and language. In this paper, we propose a novel method called BiEfficient, which use bidirectional prompting schemes to efficiently transfer the VLM to video recognition task with a small number of tunable parameters: 1) Vision-to-Language: we propose two prompt mechanisms, Pre-Prompt and Post-Prompt, which act before and after the text encoder respectively to generate discriminative video-level text representation for each input video. 2) Language-to-Vision: we propose Word-Guided Visual-Prompt, which enhances the temporal modeling of videos using textual knowledge in an almost parameter-free manner. Experiments on Kinetics-400, UCF-101, HMDB-51 demonstrate that the proposed method can achieve comparable or even better performance to the full finetuning methods with much fewer tunable parameters across closed-set and zero-shot video recognition benchmarks.",https://openaccess.thecvf.com/content/ACCV2024/html/He_BiEfficient_Bidirectionally_Prompting_Vision-Language_Models_for_Parameter-Efficient_Video_Recognition_ACCV_2024_paper.html
Event-based Image Enhancement Under High Dynamic Range Scenarios,"Event cameras, as bio-inspired vision sensors with a high dynamic range, are capable of addressing the problems of local overexposure or underexposure that conventional frame-based cameras encounter in scenarios with high dynamic range or fluctuating lighting conditions. Due to the modality gap between the two types of cameras, simple direct fusion is not feasible. Additionally, the ghosting artifacts caused by the deviation in the camera positions and frame-rates also affects the quality of final fused image. To solve the problems, this paper proposes a joint framework that combines locally poor-exposed frames with event streams captured by the event camera to enhance the images with detailed textures in high dynamic range scenarios. Specifically, a lightweight multi-scale receptive field block is employed for rapid modality conversion from event streams to frames. Besides, a dual-branch fusion module is proposed to align features and remove ghosting artifacts. Experimental results demonstrate that the proposed method effectively mitigates information loss in both highly bright and dark regions of images across a range of extreme lighting conditions, generating the both realistic and natural images.",https://openaccess.thecvf.com/content/ACCV2024/html/Weng_Event-based_Image_Enhancement_Under_High_Dynamic_Range_Scenarios_ACCV_2024_paper.html
PARNet: Aortic Reconstruction from Orthogonal X-rays Using Pre-Trained Generative Adversarial Networks,"The three-dimensional reconstruction of the aorta plays a crucial role in assisting minimally invasive vascular interventions to treat coronary artery disease, aiding surgeons in finding the optimal procedural angles for locating and delivering intervention devices. However, existing reconstruction methods face challenges such as weak imaging capability for low-density tissues in X-rays, limiting the accurate capture and reconstruction of the aorta and other blood vessels. To address these challenges, we propose PARNet, a deep-learning approach for 3D aortic reconstruction from orthogonal X-rays. PARNet leverages pre-training information to extract global and local features using Aortic Reconstruction with Background X-rays (ARB) module and Aortic Reconstruction with Mask X-rays (ARMask) module, respectively, thereby enhancing the model's reconstruction performance with more aortic details. Additionally, customized loss functions are introduced to adapt to the low-density characteristics of the aorta. The results demonstrate that our method outperforms existing approaches, producing results that are visually closest to the ground truth on mainstream datasets.",https://openaccess.thecvf.com/content/ACCV2024/html/Cao_PARNet_Aortic_Reconstruction_from_Orthogonal_X-rays_Using_Pre-Trained_Generative_Adversarial_ACCV_2024_paper.html
Multi-path Segmentation Network Based on CNN and Transformer for Skin Lesion Image,"Skin lesion segmentation is a challenging task in computer-aided di-agnosis, which is crucial for the early diagnosis of skin cancer. Convolutional Neural Networks (CNNs) have been successful in medical image segmentation tasks; however, their effective receptive fields in deep convolutional layers are limited to a local range and follow Gaussian distribution, thereby failing to ob-tain global information. Advanced Transformer shows great potential in model-ing long-range dependencies and obtaining global representations. Therefore, we propose a multi-path segmentation model (MSNet) based on a combination of CNN and Transformer, which is dedicated to facilitating the task of skin le-sion segmentation. Regarding different task requirements, we design MSNet-1 for the real-time tasks, and MSNet-2 for the tasks that require high accuracy. Moreover, we develop an efficient residual module (ERM) in MSNet, which can effectively integrate multi-level features and provide accurate feature repre-sentations. Pixel attention and coordinate attention are also introduced to en-hance the perceptual ability of the network and improve the predicting accuracy of the segmentation results. Finally, we conduct extensive experiments on three public skin lesion datasets and one thyroid nodule dataset. The experimental re-sults demonstrate that MSNet not only possesses the SOTA segmentation per-formance and excellent generalization ability, but also has lightweight and real-time characteristics, and it has broad application prospects in various scenarios.",https://openaccess.thecvf.com/content/ACCV2024/html/Nie_Multi-path_Segmentation_Network_Based_on_CNN_and_Transformer_for_Skin_ACCV_2024_paper.html
Hi-NeRF: Hybridizing 2D Inpainting with Neural Radiance Fields for 3D Scene Inpainting,"Recent developments in Neural Radiance Fields (NeRF) have showcased notable progress in the synthesis of novel views. Nevertheless, there is limited research on inpainting 3D scenes using implicit representations. Traditional approaches utilizing 3D networks for direct 3D inpainting often falter in high-resolution settings, mainly due to GPU memory constraints. This paper introduces Hi-NeRF, an innovative 3D inpainting approach designed to remove arbitrary 3D objects by hybridizing 2D inpainting strategies with NeRF techniques. Recognizing that prevailing 2D inpainting methods often fail to grasp the 3D geometric intricacies of scenes, we leverage the unique capability of NeRF in capturing these structures. Additionally, we propose a multi-view perceptual loss (MVPL) to harness multi-view data, ensuring that 2D inpainting and implicit 3D representations can mutually compensate for each other. Furthermore, we refine the output from the Segment Anything Model (SAM) using image dilation to produce accurate multi-view masks. To finalize the process, we employ Instant-NGP to efficiently retrieve 3D-consistent scenes from 3D-consistent inpainted images. As there is no multi-view 3D scene datasets with corresponding masks, we construct both real-world and synthetic scenes for the multi-view 3D scene inpainting task, which serves as a benchmark dataset. Experimental results on both indoor and outdoor scenes highlight the superiority of our approach over the existing 2D inpainting methods and NeRF-based baselines.",https://openaccess.thecvf.com/content/ACCV2024/html/Huang_Hi-NeRF_Hybridizing_2D_Inpainting_with_Neural_Radiance_Fields_for_3D_ACCV_2024_paper.html
Adaptive Bias Discovery for Learning Debiased Classifier,"Training deep neural networks with empirical risk minimization (ERM) often captures dataset biases, hindering generalization to new or unseen data. Previous solutions either require prior knowledge of biases or utilize training intentionally biased models as auxiliaries; however, they still suffer from multiple biases. To address this, we introduce Adaptive Bias Discovery (ABD), a novel learning framework designed to mitigate the impact of multiple unknown biases. ABD trains an auxiliary model to be adapted to biases based on the debiased parameters from the debiasing phase, allowing it to navigate through multiple biases. Then, samples are reweighted based on the discovered biases to update debiased parameters. Extensive evaluations of synthetic experiments and real-world datasets demonstrate that ABD consistently outperforms existing methods, particularly in real-world applications where multiple unknown biases are prevalent.",https://openaccess.thecvf.com/content/ACCV2024/html/Bae_Adaptive_Bias_Discovery_for_Learning_Debiased_Classifier_ACCV_2024_paper.html
FocusNet: Cascaded Lightweight Networks and Ascending Feature Enhancement for Efficient Salient Object Detection,"Existing salient object detection methods typically depend on large, pretrained backbone networks for feature extraction. While this enhances performance, their large size and high number of parameters make them less practical for widespread use in real-world applications. In contrast, lightweight backbone networks are smaller and have fewer parameters, yet they typically fall short in delivering strong feature extraction and precise localization. To address this issue, this paper introduces FocusNet, an innovative lightweight solution. By cascading lightweight networks through the FOCUS module, we simulate the human eyes focusing process, dividing localization and feature extraction into two independent and sequential steps, significantly enhancing the feature extraction capabilities. Additionally, utilizing the Ascending Feature Enhancement (AFE) strategy, we progressively saturate the localization of salient objects from shallow to deep layers, significantly improving localization accuracy. Extensive experiments on five public datasets demonstrate that our method, while maintaining a low parameter(3.15M), performs comparably to methods based on large backbone networks.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhou_FocusNet_Cascaded_Lightweight_Networks_and_Ascending_Feature_Enhancement_for_Efficient_ACCV_2024_paper.html
InstantGeoAvatar: Effective Geometry and Appearance Modeling of Animatable Avatars from Monocular Video,"We present InstantGeoAvatar, a method for efficient and effective learning from monocular video of detailed 3D geometry and appearance of animatable implicit human avatars. Our key observation is that the optimization of a hash grid encoding to represent a signed distance function (SDF) of the human subject is fraught with instabilities and bad local minima. We thus propose a principled geometry-aware SDF regularization scheme that seamlessly fits into the volume rendering pipeline and adds negligible computational overhead. Our regularization scheme significantly outperforms previous approaches for training SDFs on hash grids. We obtain competitive results in geometry reconstruction and novel view synthesis in as little as five minutes of training time, a significant reduction from the several hours required by previous work. InstantGeoAvatar represents a significant leap forward towards achieving interactive reconstruction of virtual avatars.",https://openaccess.thecvf.com/content/ACCV2024/html/Budria_InstantGeoAvatar_Effective_Geometry_and_Appearance_Modeling_of_Animatable_Avatars_from_ACCV_2024_paper.html
Neural Substitution for Branch-level Network Re-parameterization,"We propose the neural substitution method for network re-parameterization at the branch-level connectivity. This method learns different network topologies to maximize the benefit of the ensemble effect, as re-parameterization allows for the integration of multiple layers during inference following their individual training. Additionally, we introduce a guiding method to incorporate non-linear activation functions into a linear transformation during re-parameterization. Because branch-level connectivity necessitates multiple non-linear activation functions, they must be infused into a single activation with our guided activation method during re-parameterization. Incorporating the non-linear activation function is significant because it overcomes the limitation of the current re-parameterization method, which only works at block-level connectivity. Restricting re-parameterization to block-level connectivity limits the use of network topology, making it challenging to learn a variety of feature representations. On the other hand, the proposed approach learns a considerably richer representation than existing methods due to the unlimited topology, with branch-level connectivity, providing a generalized framework to be applied with other methods. We provide comprehensive experimental evidence for the proposed re-parameterization approach. Our code is available at https://github.com/SoongE/neural_substitution.",https://openaccess.thecvf.com/content/ACCV2024/html/Oh_Neural_Substitution_for_Branch-level_Network_Re-parameterization_ACCV_2024_paper.html
ATTIQA: Generalizable Image Quality Feature Extractor using Attribute-aware Pretraining,"In no-reference image quality assessment (NR-IQA), the challenge of limited dataset sizes hampers the development of robust and generalizable models.  Conventional methods address this issue by utilizing large datasets to extract rich representations for IQA. Also, some approaches propose vision language models (VLM) based IQA, but the domain gap between generic VLM and IQA constrains their scalability. In this work, we propose a novel pretraining framework that constructs a generalizable representation for IQA by selectively extracting quality-related knowledge from VLM and leveraging the scalability of large datasets. Specifically, we carefully select optimal text prompts for five representative image quality attributes and use VLM to generate pseudo-labels. Numerous attribute-aware pseudo-labels can be generated with large image datasets, allowing our IQA model to learn rich representations about image quality. Our approach achieves state-of-the-art performance on multiple IQA datasets and exhibits remarkable generalization capabilities. Leveraging these strengths, we propose several applications, such as evaluating image generation models and training image enhancement models, demonstrating our model's real-world applicability.",https://openaccess.thecvf.com/content/ACCV2024/html/Kwon_ATTIQA_Generalizable_Image_Quality_Feature_Extractor_using_Attribute-aware_Pretraining_ACCV_2024_paper.html
Seeing Through Expert's Eyes: Leveraging Radiologist Eye Gaze and Speech Report with Graph Neural Networks for Chest X-ray Image Classification,"Recently, integrating eye-tracking techniques and texts into disease classification has gained traction. To address the unmet needs such as heterogeneous data alignment, information propagation and aggregation, and expert knowledge embedding, we propose an innovative expert-guided Graph Neural Network (GNN) that uses radiologists' eye-gaze data and transcribed audio reports with X-ray images during training. By distilling expert knowledge from gaze data and diagnosis reports, our GNN can achieve high accuracy using only X-ray images during inference. This approach provides a robust framework for disease diagnosis, embedded with the radiologists' insights, addressing challenges in aligning heterogeneous data, propagating local information for global decisions, and leveraging expert knowledge effectively. Additionally, the attention maps on x-ray images which are generated from the GNN model visualize the Region of Interest (ROI) for the diagnosed disease. Evaluated on two benchmark chest x-ray datasets, the proposed method outperforms state-of-the-art x-ray image classification methods.",https://openaccess.thecvf.com/content/ACCV2024/html/Sultana_Seeing_Through_Experts_Eyes_Leveraging_Radiologist_Eye_Gaze_and_Speech_ACCV_2024_paper.html
Instance-Dependent Noise Refinement in Segment Anything Model for Weakly Supervised Object Detection,"We propose a new framework for Weakly Supervised Object Detection (WSOD), a domain that traditionally relies on image-level labels. In addressing the inherent limitations of current WSOD methods, particularly their reliance on image-level annotations that result in inaccurate bounding box selections, we develop a framework that iteratively utilizes weak supervision and refines it to progressively enhance the supervision of the object detector throughout the training process. Specifically, we employ the Segment Anything Model (SAM) to generate initial pseudo-labels bounding boxes from the point prompts generated by Class Activation Mapping (CAM). Our approach tackles the challenge of label noise, where pseudo-labels bounding boxes might only capture parts of objects. We enhance our ability to distinguish between complete and partial detected objects by leveraging an instance-dependent, particularly part-based noise correction model. Our method is inspired by learning methods focusing on part-based representations for object detection and recognition, as well as from human perception, which typically simplifies complex visual information into simpler, constituent parts. Our experiments, conducted in various settings beyond WSOD, including Semi-Supervised Object Detection (SSOD) and Weakly Supervised Instance Segmentation (WSIS), validate the efficacy of our approach.",https://openaccess.thecvf.com/content/ACCV2024/html/Taherkhani_Instance-Dependent_Noise_Refinement_in_Segment_Anything_Model_for_Weakly_Supervised_ACCV_2024_paper.html
HARD : Hardware-Aware lightweight Real-time semantic segmentation model Deployable from Edge to GPU,"The two-branch model ensures high performance in semantic segmentation. However, the additional branch causes the fusion between high-resolution and low-resolution contexts to corrupt the surrounding context and increases the computational overhead. Existing methods with many parameters and high computational costs are not well-suited for the low-power devices used in applications like autonomous driving and robotics. This study proposes a robust semantic segmentation architecture with any kind of device, from GPUs to edge devices. We introduce five variations called HARD. HARD achieves fast inference speeds while maintaining good performance on any kind of device. Notably, the proposed Dual Atrous Pooling Module (DAP) can effectively fuse contexts of variable resolutions without decreasing inference speed. Besides, a lightweight decoder named Serialized Atrous Module (SA) is proposed to extract global context. The proposed models are evaluated on both GPU and embedded computing devices from NVIDIA and ARM Cortex-M CPU. In experiments on Cityscapes, CamVid, and COCO-Stuff datasets, the proposed variations of HARDs achieve 73.8, 76.3, and 41.0 mIoU, which outperform existing SOTA models.",https://openaccess.thecvf.com/content/ACCV2024/html/Kwon_HARD__Hardware-Aware_lightweight_Real-time_semantic_segmentation_model_Deployable_from_ACCV_2024_paper.html
TAPS: Temporal Attention-based Pruning and Scaling for Efficient Video Action Recognition,"Video neural networks are computationally expensive. For real-time applications they require significant compute resources that are lacking on edge devices. Various methods were proposed to reduce the computational load of neural networks. Among them, dynamic approaches adapt the network architecture, its weights or the input resolution to the content of the input. Our proposed approach, showcased on the task of video action recognition, allows to dynamically reduce computations for a wide range of video processing networks by utilizing the redundancy between frames and channels. A per-layer lightweight policy network is used to make a per-filter decision regarding the filter's importance. Important filters are retained while others are scaled down or entirely skipped. Our method is the first to allow the policy network to gain a broader temporal context considering features aggregated over time. Temporal aggregation is done using self-attention between present, past and future (if available) input tensor descriptors. As demonstrated on a large variety of leading benchmarks such as Something-Something-V2, Mini-Kinetics, Jester and ActivityNet1.3, and over multiple network architectures, our method is able to enhance accuracy or save up to 70% of the FLOPs with no accuracy degradation, outperforming existing dynamic pruning methods by a large margin and setting a new bar for the accuracy-efficiency trade-off allowed by dynamic methods. We release the code and trained models at: https://github.com/tapsdyn/TAPS.",https://openaccess.thecvf.com/content/ACCV2024/html/Dinai_TAPS_Temporal_Attention-based_Pruning_and_Scaling_for_Efficient_Video_Action_ACCV_2024_paper.html
Vision language models are blind,"Large language models (LLMs) with vision capabilities (e.g., GPT-4o, Gemini 1.5, and Claude 3) are powering countless image-text processing applications, enabling unprecedented multimodal, human-machine interaction. Yet, we find that all state-of-the-art LLMs fail on absurdly simple tasks such as identifying (a) whether two circles overlap or whether two lines touch each other; (b) which letter is being circled in a word; and (c) counting the number of circles in a Olympic-like logo. Our findings suggest the tokenization of input images to LLMs is the source of problem, causing failures in real-world scenarios, such as determining if two streets intersect on a Manhattan map, identifying a stock price crossing a threshold line, and describing content within a bounding box in an image.",https://openaccess.thecvf.com/content/ACCV2024/html/Rahmanzadehgervi_Vision_language_models_are_blind_ACCV_2024_paper.html
Bridging the Projection Gap: Overcoming Projection Bias Through Parameterized Distance Learning,"Generalized zero-shot learning (GZSL) aims to recognize samples from both seen and unseen classes using only seen class samples for training. However, GZSL methods are prone to bias towards seen classes during inference due to the projection function being learned from seen classes. Most methods focus on learning an accurate projection, but bias in the projection is inevitable. We address this projection bias by proposing to learn a parameterized Mahalanobis distance metric for robust inference. Our key insight is that the distance computation during inference is critical, even with a biased projection. We make two main contributions - (1) We extend the VAEGAN (Variational Autoencoder & Generative Adversarial Networks) architecture with two branches to separately output the projection of samples from seen and unseen classes, enabling more robust distance learning.  (2) We introduce a novel loss function to optimize the Mahalanobis distance representation and reduce projection bias. Extensive experiments on four datasets show that our approach outperforms state-of-the-art GZSL techniques with improvements of up to 3.5 % on the harmonic mean metric.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_Bridging_the_Projection_Gap_Overcoming_Projection_Bias_Through_Parameterized_Distance_ACCV_2024_paper.html
RD-Diff: RLTransformer-based Diffusion Model with Diversity-Inducing Modulator for Human Motion Prediction,"Human Motion Prediction (HMP) is crucial for human-robot collaboration, surveillance, and autonomous driving applications. Recently, diffusion models have shown promising progress due to their ease of training and realistic generation capabilities. To enhance both accuracy and diversity of the diffusion model in HMP, we present RD-Diff: RLTransformer-based Diffusion model with Diversity-inducing modulator. First, to improve transformers effectiveness on the frequency representation of human motion transformed by Discrete Cosine Transform (DCT), we introduce a novel Regulated Linear Transformer (RLTransformer) with a specially designed linear-attention mechanism. Next, to further enhance the performance, we propose a Diversity- Inducing Modulator (DIM) to generate noise-modulated observation conditions for a pretrained diffusion model. Experimental results show that our RD-Diff establishes a new state-of-the-art performance on both accuracy and diversity compared to existing methods.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_RD-Diff_RLTransformer-based_Diffusion_Model_with_Diversity-Inducing_Modulator_for_Human_Motion_ACCV_2024_paper.html
CCNDF: Curvature Constrained Neural Distance Fields from 3D LiDAR Sequences,"Neural distance fields (NDF) have emerged as a powerful tool for solving  3D computer vision and robotics downstream problems. While significant progress has been made in learning NDF from point cloud data obtained through a LiDAR scanner, a crucial aspect that demands attention is the supervision of neural fields during training, as ground-truth NDFs are not available for large-scale outdoor scenes. The existing works have approximated signed distance to guide model learning. The efficiency of the trained model heavily depends on the approximation of the signed distance. To this end, we propose a novel methodology leveraging second-order derivatives of the NDF for a better approximation of the signed distance that leads to improved neural field learning. To assess the efficacy of our methodology, we conducted comparative evaluations against prevalent methods for mapping and localization tasks. Our results demonstrate the superiority of the proposed approach compared to the state-of-the-art techniques, highlighting its potential for advancing the capabilities of neural distance fields in computer vision and graphics applications.",https://openaccess.thecvf.com/content/ACCV2024/html/Singh_CCNDF_Curvature_Constrained_Neural_Distance_Fields_from_3D_LiDAR_Sequences_ACCV_2024_paper.html
Every Shot Counts: Using Exemplars for Repetition Counting in Videos,"Video repetition counting infers the number of repetitions of recurring actions or motion within a video. We propose an exemplar-based approach that discovers visual correspondence of video exemplars across repetitions within target videos. Our proposed Every Shot Counts (ESCounts) model is an attention-based encoder-decoder that encodes videos of varying lengths alongside exemplars from the same and different videos. In training, ESCounts regresses locations of high correspondence to the exemplars within the video. In tandem, our method learns a latent that encodes representations of general repetitive motions, which we use for exemplar-free, zero-shot inference. Extensive experiments over commonly used datasets (RepCount, Countix, and UCFRep) showcase ESCounts obtaining state-of-the-art performance across all three datasets. On RepCount, ESCounts increases the off-by-one from 0.39 to 0.56 and decreases the mean absolute error from 0.38 to 0.21. Detailed ablations further demonstrate the effectiveness of our method.",https://openaccess.thecvf.com/content/ACCV2024/html/Sinha_Every_Shot_Counts_Using_Exemplars_for_Repetition_Counting_in_Videos_ACCV_2024_paper.html
Co-Segmentation without any Pixel-level Supervision with Application to Large-Scale Sketch Classification,"This work proposes a novel method for object co-segmentation, i.e. pixel-level localization of a common object in a set of images, that uses no pixel-level supervision for training. Two pre-trained Vision Transformer (ViT) models are exploited: ImageNet classification-trained ViT, whose features are used to estimate rough object localization through intra-class token relevance, and a self-supervised DINO-ViT for intra-image token relevance. On recent challenging benchmarks, the method achieves state-of-the-art performance among methods trained with the same level of supervision (image labels) while being competitive with methods trained with pixel-level supervision (binary masks). The benefits of the proposed co-segmentation method are further demonstrated in the task of large-scale sketch recognition, that is, the classification of sketches into a wide range of categories. The limited amount of hand-drawn sketch training data is leveraged by exploiting readily available image-level-annotated datasets of natural images containing a large number of classes. To bridge the domain gap, the classifier is trained on a sketch-like proxy domain derived from edges detected on natural images. We show that sketch recognition significantly benefits when the classifier is trained on sketch-like structures extracted from the co-segmented area rather than from the full image.",https://openaccess.thecvf.com/content/ACCV2024/html/Ypsilantis_Co-Segmentation_without_any_Pixel-level_Supervision_with_Application_to_Large-Scale_Sketch_ACCV_2024_paper.html
EffiSeaNet: Pioneering Lightweight Network for Underwater Salient Object Detection,"Underwater salient object detection seeks to pinpoint the most vital elements in underwater environments, offering considerable promise for underwater exploration. Considering the preference for low-complexity algorithms in underwater applications to maximize overall system efficiency, this paper proposes EffiSeaNet, a lightweight network designed to provide an effective solution for salient object detection in underwater scenarios. On the one hand, EffiSeaNet incorporates a parameter-free image enhancement block to mitigate the effects of image degradation caused by water. This block effectively addresses issues such as color distortion and reduced visibility, which are common challenges in underwater environments. On the other hand, we develop a customized lightweight network structure incorporating a novel cross-layer fusion strategy to efficiently capture and merge features. This enhances the network's ability to handle the variability and complexity of underwater objects and scenes while maintaining a low computational load. Extensive experiments on three public datasets demonstrate that our innovative designs achieve remarkable results while maintaining a low model size and computational complexity. This efficiency and effectiveness make our approach highly suitable for practical underwater applications where resources are limited, yet high precision is essential. Our code and results will be accessible to the public.",https://openaccess.thecvf.com/content/ACCV2024/html/Wu_EffiSeaNet_Pioneering_Lightweight_Network_for_Underwater_Salient_Object_Detection_ACCV_2024_paper.html
Enhanced Super-Resolution Training via Mimicked Alignment for Real-World Scenes,"Image super-resolution methods have made significant strides with deep learning techniques and ample training data. However, they face challenges due to inherent misalignment between low-resolution (LR) and high-resolution (HR) pairs in real-world datasets. In this study, we propose a novel plug-and-play module designed to mitigate these misalignment issues by aligning LR inputs with HR images during training. Specifically, our approach involves mimicking a novel LR sample that aligns with HR while preserving the degradation characteristics of the original LR samples. This module seamlessly integrates with any SR model, enhancing robustness against misalignment. Importantly, it can be easily removed during inference, therefore without introducing any parameters on the conventional SR models. We comprehensively evaluate our method on synthetic and real-world datasets, demonstrating its effectiveness across a spectrum of SR models, including traditional CNNs and state-of-the-art Transformers.",https://openaccess.thecvf.com/content/ACCV2024/html/Elezabi_Enhanced_Super-Resolution_Training_via_Mimicked_Alignment_for_Real-World_Scenes_ACCV_2024_paper.html
RayEmb: Arbitrary Landmark Detection in X-Ray Images Using Ray Embedding Subspace,"Intra-operative 2D-3D registration of X-ray images with pre-operatively acquired CT scans is a crucial procedure in orthopedic surgeries. Anatomical landmarks pre-annotated in the CT volume can be detected in X-ray images to establish 2D-3D correspondences, which are then utilized for registration. However, registration often fails in certain view angles due to poor landmark visibility. We propose a novel method to address this issue by detecting arbitrary landmark points in X-ray images. Our approach represents 3D points as distinct subspaces, formed by feature vectors (referred to as ray embeddings) corresponding to intersecting rays. Establishing 2D-3D correspondences then becomes a task of finding ray embeddings that are close to a given subspace, essentially performing an intersection test. Unlike conventional methods for landmark estimation, our approach eliminates the need for manually annotating fixed landmarks. We trained our model using the synthetic images generated from CTPelvic1K CLINIC dataset, which contains 103 CT volumes, and evaluated it on the DeepFluoro dataset, comprising real X-ray images. Experimental results demonstrate the superiority of our method over conventional methods. The code is available at https://github.com/Pragyanstha/rayemb.",https://openaccess.thecvf.com/content/ACCV2024/html/Shrestha_RayEmb_Arbitrary_Landmark_Detection_in_X-Ray_Images_Using_Ray_Embedding_ACCV_2024_paper.html
FG-CXR: A Radiologist-Aligned Gaze Dataset for Enhancing Interpretability in Chest X-Ray Report Generation,"Developing an interpretable system for generating reports in chest X-ray (CXR) analysis is becoming increasingly crucial in Computer-aided Diagnosis (CAD) systems, enabling radiologists to comprehend the decisions made by these systems. Despite the growth of diverse datasets and methods focusing on report generation, there remains a notable gap in how closely these models generated reports align with the interpretations of real radiologists. In this study, we tackle this challenge by initially introducing the Fine-Grained CXR (FG-CXR) dataset, which provides fine-grained paired information between the captions generated by radiologists and the corresponding gaze attention heatmaps for each anatomy.  Unlike existing datasets that include a raw sequence of gaze alongside a report, with significant misalignment between gaze location and report content, our FG-CXR dataset offers a more grained alignment between gaze attention and diagnosis transcript. Furthermore, our analysis reveals that simply applying black-box image captioning methods to generate reports cannot adequately explain which information in CXR is utilized and how long needs to attend to accurately generate reports. Consequently, we propose a novel explainable radiologists attention generator network (Gen-XAI) that mimics the diagnosis process of radiologists, explicitly constraining its output to closely align with both radiologists gaze attention and transcript. Finally, we perform extensive experiments to illustrate the effectiveness of our method.",https://openaccess.thecvf.com/content/ACCV2024/html/Pham_FG-CXR_A_Radiologist-Aligned_Gaze_Dataset_for_Enhancing_Interpretability_in_Chest_ACCV_2024_paper.html
Learning 2D Human Poses for Better 3D Lifting via Multi-Model 3D-Guidance,"Recent advancements in 2D pose detectors have significantly improved 3D human pose estimation via the 2D-to-3D lifting approach. Despite these advancements, a substantial accuracy gap remains between using ground-truth 2D poses and detected 2D poses for 3D lifting. However, most methods focus solely on enhancing the 3D lifting network, using 2D pose detectors optimized for 2D accuracy without any refinement to better serve the 3D lifting process. To address this limitation, we propose a novel 3D-guided training method that leverages 3D loss to improve 2D pose estimation. Additionally, we introduce a multi-model training method to ensure robust generalization across various 3D lifting networks. Extensive experiments with three 2D pose detectors and four 3D lifting networks demonstrate our method's effectiveness. Our method achieves an average improvement of 4.6% in MPJPE on Human3.6M and 16.8% on Panoptic, enhancing 2D poses for accurate 3D lifting. The code is available at https://github.com/knu-vis/L2D-Pose.",https://openaccess.thecvf.com/content/ACCV2024/html/Lee_Learning_2D_Human_Poses_for_Better_3D_Lifting_via_Multi-Model_ACCV_2024_paper.html
Enhancing Photo Animation: Augmented Stylistic Modules and Prior Knowledge Integration,"Photo-to-animation translation presents a practical and captivating task within image style transfer. However, existing methods often fall short of achieving satisfactory results in cartoonization. This inadequacy primarily stems from two key factors: the absence of dedicated network architectures tailored for anime-style transfer and the inadequate incorporation of pertinent prior knowledge specific to cartoons. In response to these limitations, this paper introduces a novel deep neural network architecture designed to optimize photo-to-animation translation. Specifically, the proposed framework consists of two pivotal modules: the SCAN module and the Ada-CTSS module, operating at the feature and image levels, respectively, to enhance the desired anime-style effects. We also leverage prior knowledge, encompassing color, texture, and surface aspects, by integrating refined color preservation loss, grayscale style loss, and region smoothness loss. Moreover, to assess the efficacy of our approach, we devise a specialized style evaluation network, circumventing the reliance on conventional evaluation metrics. Through an extensive array of experiments, we demonstrate the superior capabilities of our method in generating high-quality cartoonized images, surpassing the performance of state-of-the-art methods.",https://openaccess.thecvf.com/content/ACCV2024/html/Lu_Enhancing_Photo_Animation_Augmented_Stylistic_Modules_and_Prior_Knowledge_Integration_ACCV_2024_paper.html
Sparse Domain Transfer via Elastic Net Regularization,"Transportation of samples across different domains is a central task in several machine learning problems. A sensible requirement for domain transfer tasks in computer vision and language domains is the sparsity of the transportation map, i.e., the transfer algorithm aims to modify the least number of input features while transporting samples across the source and target domains. In this work, we propose Elastic Net Optimal Transport (ENOT) to address the sparse distribution transfer problem. The ENOT framework utilizes the L1-norm and L2-norm regularization mechanisms to find a sparse and stable transportation map between the source and target domains. To compute the ENOT transport map, we consider the dual formulation of the ENOT optimization task and prove that the sparsified gradient of the optimal potential function in the ENOT's dual representation provides the ENOT transport map. Furthermore, we demonstrate the application of the ENOT framework to perform feature selection for sparse domain transfer. We present the numerical results of applying ENOT to several domain transfer problems for synthetic Gaussian mixtures and real image and text data. Our empirical results indicate the success of the ENOT framework in identifying a sparse domain transport map.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_Sparse_Domain_Transfer_via_Elastic_Net_Regularization_ACCV_2024_paper.html
Redefining Normal: A Novel Object-Level Approach for Multi-Object Novelty Detection,"In the realm of novelty detection, accurately identifying outliers in data without specific class information poses a significant challenge. While current methods excel in single-object scenarios, they struggle with multi-object situations due to their focus on individual objects. Our paper suggests a novel approach: redefining `normal' at the object level in training datasets. Rather than the usual image-level view, we consider the most dominant object in a dataset as the norm, offering a perspective that is more effective for real-world scenarios. Adapting to our object-level definition of `normal', we modify knowledge distillation frameworks, where a student network learns from a pre-trained teacher network. Our first contribution, DeFeND (Dense Feature Fine-tuning on Normal Data), integrates dense feature fine-tuning into the distillation process, allowing the teacher network to focus on object-level features with a self-supervised loss. The second is masked knowledge distillation, where the student network works with partially hidden inputs, honing its ability to deduce and generalize from incomplete data. This approach not only fares well in single-object novelty detection but also considerably surpasses existing methods in multi-object contexts.",https://openaccess.thecvf.com/content/ACCV2024/html/Salehi_Redefining_Normal_A_Novel_Object-Level_Approach_for_Multi-Object_Novelty_Detection_ACCV_2024_paper.html
TranSPORTmer: A Holistic Approach to Trajectory Understanding in Multi-Agent Sports,"Understanding trajectories in multi-agent scenarios requires addressing various tasks, including predicting future movements, imputing missing observations, inferring the status of unseen agents, and classifying different global states. Traditional data-driven approaches often handle these tasks separately with specialized models. We introduce TranSPORTmer, a unified transformer-based framework capable of addressing all these tasks, showcasing its application to the intricate dynamics of multi-agent sports scenarios like soccer and basketball. Using Set Attention Blocks, TranSPORTmer effectively captures temporal dynamics and social interactions in an equivariant manner. The model's tasks are guided by an input mask that conceals missing or yet-to-be-predicted observations. Additionally, we introduce a CLS extra agent to classify states along soccer trajectories, including passes, possessions, uncontrolled states, and out-of-play intervals, contributing to an enhancement in modeling trajectories. Evaluations on soccer and basketball datasets show that TranSPORTmer outperforms state-of-the-art task-specific models in player forecasting, player forecasting-imputation, ball inference, and ball imputation. https://youtu.be/8VtSRm8oGoE",https://openaccess.thecvf.com/content/ACCV2024/html/Capellera_TranSPORTmer_A_Holistic_Approach_to_Trajectory_Understanding_in_Multi-Agent_Sports_ACCV_2024_paper.html
Bridging Optimal Transport and Jacobian Regularization by Optimal Trajectory for Enhanced Adversarial Defense,"Deep neural networks, particularly in vision tasks, are notably susceptible to adversarial perturbations. To overcome this challenge, developing a robust classifier is crucial. In light of the recent advancements in the robustness of classifiers, we delve deep into the intricacies of adversarial training and Jacobian regularization, two pivotal defenses. Our work is the first carefully analyzes and characterizes these two schools of approaches, both theoretically and empirically, to demonstrate how each approach impacts the robust learning of a classifier. Next, we propose our novel Optimal Transport with Jacobian regularization method, dubbed OTJR, bridging the input Jacobian regularization with the a output representation alignment by leveraging the optimal transport theory. In particular, we employ the Sliced Wasserstein distance that can efficiently push the adversarial samples' representations closer to those of clean samples, regardless of the number of classes within the dataset. The SW distance provides the adversarial samples' movement directions, which are much more informative and powerful for the Jacobian regularization.  Our empirical evaluations set a new standard in the domain, with our method achieving commendable accuracies of 52.57% on CIFAR-10 and 28.36% on CIFAR-100 datasets under the AutoAttack. Further validating our model's practicality, we conducted real-world tests by subjecting internet-sourced images to online adversarial attacks. These demonstrations highlight our model's capability to counteract sophisticated adversarial perturbations, affirming its significance and applicability in real-world scenarios.",https://openaccess.thecvf.com/content/ACCV2024/html/Le_Bridging_Optimal_Transport_and_Jacobian_Regularization_by_Optimal_Trajectory_for_ACCV_2024_paper.html
Guide3D: A Bi-planar X-ray Dataset for Guidewire Segmentation and 3D Reconstruction,"Endovascular surgical tool reconstruction represents an important factor in advancing endovascular tool navigation, which is an important step in endovascular surgery. However, the lack of publicly available datasets significantly restricts the development and validation of novel machine learning approaches. Moreover, due to the need for specialized equipment such as biplanar scanners, most of the previous research employs monoplanar fluoroscopic technologies, hence only capturing the data from a single view and significantly limiting the reconstruction accuracy. To bridge this gap, we introduce Guide3D, a bi-planar X-ray dataset for 3D reconstruction. The dataset represents a collection of high resolution bi-planar, manually annotated fluoroscopic videos, captured in real-world settings. Validating our dataset within a simulated environment reflective of clinical settings confirms its applicability for real-world applications. Furthermore, we propose a new benchmark for guidewrite shape prediction, serving as a strong baseline for future work.  Guide3D not only addresses an essential need by offering a platform for advancing segmentation and 3D reconstruction techniques but also aids the development of more accurate and efficient endovascular surgery interventions. Our code and dataset will be made publicly available to encourage further studies.",https://openaccess.thecvf.com/content/ACCV2024/html/Jianu_Guide3D_A_Bi-planar_X-ray_Dataset_for_Guidewire_Segmentation_and_3D_ACCV_2024_paper.html
DeBiFormer: Vision Transformer with Deformable Agent Bi-level Routing Attention,"Vision Transformers with various attention modules have demonstrated superior performance on vision tasks. While using sparsity-adaptive attention, such as in DAT, has yielded strong results in image classification, the key-value pairs selected by deformable points lack semantic relevance when fine-tuning for semantic segmentation tasks. The query-aware sparsity attention in BiFormer seeks to focus each query on top-k routed regions. However, during attention calculation, the selected key-value pairs are influenced by too many irrelevant queries, reducing attention on the more important ones.To address these issues, we propose the Deformable Bi-level Routing Attention (DBRA) module, which optimizes the selection of key-value pairs using agent queries and enhances the interpretability of queries in attention maps. Based on this, we introduce the Deformable Bi-level Routing Attention Transformer (DeBiFormer), a novel general-purpose vision transformer built with the DBRA module. DeBiFormer has been validated on various computer vision tasks, including image classification, object detection, and semantic segmentation, providing strong evidence of its effectiveness.",https://openaccess.thecvf.com/content/ACCV2024/html/BaoLong_DeBiFormer_Vision_Transformer_with_Deformable_Agent_Bi-level_Routing_Attention_ACCV_2024_paper.html
MGNiceNet: Unified Monocular Geometric Scene Understanding,"Monocular geometric scene understanding combines panoptic segmentation and self-supervised depth estimation, focusing on real-time application in autonomous vehicles. We introduce MGNiceNet, a unified approach that uses a linked kernel formulation for panoptic segmentation and self-supervised depth estimation. MGNiceNet is based on the state-of-the-art real-time panoptic segmentation method RT-K-Net and extends the architecture to cover both panoptic segmentation and self-supervised monocular depth estimation. To this end, we introduce a tightly coupled self-supervised depth estimation predictor that explicitly uses information from the panoptic path for depth prediction. Furthermore, we introduce a panoptic-guided motion masking method to improve depth estimation without relying on video panoptic segmentation annotations. We evaluate our method on two popular autonomous driving datasets, Cityscapes and KITTI. Our model shows state-of-the-art results compared to other real-time methods and closes the gap to computationally more demanding methods. Source code and trained models are available at https://github.com/markusschoen/MGNiceNet.",https://openaccess.thecvf.com/content/ACCV2024/html/Schon_MGNiceNet_Unified_Monocular_Geometric_Scene_Understanding_ACCV_2024_paper.html
Window-based Channel Attention for Wavelet-enhanced Learned Image Compression,"Learned Image Compression (LIC) models have achieved superior rate-distortion performance than traditional codecs. Existing LIC models use CNN, Transformer, or Mixed CNN-Transformer as basic blocks. However, limited by the shifted window attention, Swin-Transformer-based LIC exhibits a restricted growth of receptive fields, affecting the ability to model large objects for image compression. To address this issue and improve the performance, we incorporate window partition into channel attention for the first time to obtain large receptive fields and capture more global information. Since channel attention hinders local information learning, it is important to extend existing attention mechanisms in Transformer codecs to the space-channel attention to establish multiple receptive fields, being able to capture global correlations with large receptive fields while maintaining detailed characterization of local correlations with small receptive fields. We also incorporate the discrete wavelet transform into our Spatial-Channel Hybrid (SCH) framework for efficient frequency-dependent down-sampling and further enlarging receptive fields. Experiment results demonstrate that our method achieves state-of-the-art performances, reducing BD-rate by 18.54%, 23.98%, 22.33%, and 24.71% on four standard datasets compared to VTM-23.1.",https://openaccess.thecvf.com/content/ACCV2024/html/Xu_Window-based_Channel_Attention_for_Wavelet-enhanced_Learned_Image_Compression_ACCV_2024_paper.html
Dual Prototype-driven Objectness Decoupling for Cross-Domain Object Detection in Urban Scene,"Unsupervised domain adaptation aims to mitigate the domain gap between the source and the target domains. Despite domain shifts, we have observed intrinsic knowledge that spans across domains for object detection in urban driving scenes. First, it includes consistent characteristics of objects within the same category of extracted ROIs. Second, it encompasses the similarity of patterns within the extracted ROIs, relating to the positions of the foreground and background during object detection. To utilize these, we present DuPDA, a method that effectively adapts object detectors to target domains by leveraging domain-invariant knowledge to separable objectness for training. Specifically, we construct categorical and regional prototypes, each of which operates through their specialized moving alignments. These prototypes serve as valuable references for training unlabeled target objects using similarity. Leveraging these prototypes, we determine and utilize a boundary that trains separately the foreground and background regions within the target ROIs, thereby transferring the knowledge to focus on each respective region. Our DuPDA surpasses previous state-of-the-art methods in various evaluation protocols on six benchmarks.",https://openaccess.thecvf.com/content/ACCV2024/html/Kim_Dual_Prototype-driven_Objectness_Decoupling_for_Cross-Domain_Object_Detection_in_Urban_ACCV_2024_paper.html
FOTV-HQS: A Fractional-Order Total Variation Model for LiDAR Super-Resolution with Deep Unfolding Network,"LiDAR super-resolution can improve the quality of point cloud data, which is critical for improving many downstream tasks such as object detection, identification, and tracking. Traditional LiDAR super-resolution models often struggle with issues like block artifacts, staircase edges, and misleading edges.  To address these challenges, a novel super-resolution model of LiDAR based on fractional-order total variation (FOTV) is proposed in this paper.  We propose a FOTV regularization optimization problem, utilizing an end-to-end trainable iterative network to capture data attributes.This enables the precise reconstruction of fine details and complex structures in point clouds. Specifically, the half quadratic splitting algorithm divides the problem into data fidelity and prior regularization subproblems. We then propose a deep unfolding network, which iteratively deals with the two subproblems within the FOTV-HQS framework. Numerous experiments have shown that our approach significantly reduces the number of parameters by up to 99.68% and maintains good performance, making it ideal for applications with limited compute and storage resources.",https://openaccess.thecvf.com/content/ACCV2024/html/Xi_FOTV-HQS_A_Fractional-Order_Total_Variation_Model_for_LiDAR_Super-Resolution_with_ACCV_2024_paper.html
NT-VOT211: A Large-Scale Benchmark for Night-time Visual Object Tracking,"Many current tracking benchmarks, such as OTB100, Nfs, UAV123, LaSOT, and GOT-10K, are approaching saturation i.e., they are approaching their maximum score capacity. As such, the leap in research progress towards realizing a robust and accurate tracking algorithm is mainly obstructed by the lack of a large-scale, well-annotated low-light condition dataset for rigorously benchmarking tracking algorithms. To this end, this paper presents NTB2024, a new benchmark dataset tailored for evaluating visual object tracking algorithms in the challenging low-light conditions. The dataset consists of 211 detailed videos, offering 211,000 annotated frames. It is among the largest tracking benchmark to-date that is specifically designed to address unique challenges such as adverse visibility, image blur, and low discrimination inherent to nighttime tracking scenarios. Through a comprehensive analysis of results obtained from 42 diverse tracking algorithms on NTB2024, we uncover the strengths and limitations of these algorithms, highlighting opportunities for enhancements in visual tracking, particularly in environments with suboptimal lighting. Besides, we public a leaderboard for revealing performance rankings, annotation tools, comprehensive meta-information and all the necessary code for reproducibility of results. We believe that our NTB2024 benchmark will be instrumental in facilitating visible scientific achievements and will also unlock avenues for new real-world tracking applications.",https://openaccess.thecvf.com/content/ACCV2024/html/Liu_NT-VOT211_A_Large-Scale_Benchmark_for_Night-time_Visual_Object_Tracking_ACCV_2024_paper.html
StreamMOTP: Streaming and Unified Framework for Joint 3D Multi-Object Tracking and Trajectory Prediction,"3D multi-object tracking and trajectory prediction are two crucial modules in autonomous driving systems. Generally, the two tasks are handled separately in traditional paradigms and a few methods have started to explore modeling these two tasks in a joint manner recently. However, these approaches suffer from the limitations of single-frame training and inconsistent coordinate representations between tracking and prediction tasks. In this paper, we propose a streaming and unified framework for joint 3D Multi-Object Tracking and trajectory Prediction (StreamMOTP) to address the above challenges. Firstly, we construct the model in a streaming manner and exploit a memory bank to preserve and leverage the long-term latent features for tracked objects more effectively. Secondly, a relative spatio-temporal positional encoding strategy is introduced to bridge the gap of coordinate representations between the two tasks and maintain the pose-invariance for trajectory prediction. Thirdly, we further improve the quality and consistency of predicted trajectories with a dual-stream predictor. We conduct extensive experiments on popular nuSences dataset and the experimental results demonstrate the effectiveness and superiority of StreamMOTP, which outperforms previous methods significantly on both tasks. Furthermore, the proposed framework has great potential and advantages in actual applications of autonomous driving.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhuang_StreamMOTP_Streaming_and_Unified_Framework_for_Joint_3D_Multi-Object_Tracking_ACCV_2024_paper.html
3D Adaptive Structural Convolution Network for Domain-Invariant Point Cloud Recognition,"Adapting deep learning networks for point cloud data recognition in self-driving vehicles faces challenges due to the variability in datasets and sensor technologies, emphasizing the need for adaptive techniques to maintain accuracy across different conditions. In this paper, we introduce the 3D Adaptive Structural Convolution Network (3D-ASCN), a cutting-edge framework for 3D point cloud recognition. It combines 3D convolution kernels, a structural tree structure, and adaptive neighborhood sampling for effective geometric feature extraction. This method obtains domain-invariant features and demonstrates robust, adaptable performance on a variety of point cloud datasets, ensuring compatibility across diverse sensor configurations without the need for parameter adjustments. This highlights its potential to significantly enhance the reliability and efficiency of self-driving vehicle technology.",https://openaccess.thecvf.com/content/ACCV2024/html/Kim_3D_Adaptive_Structural_Convolution_Network_for_Domain-Invariant_Point_Cloud_Recognition_ACCV_2024_paper.html
Knowledge Distillation Dealing with Sample-wise Long-tail Problem,"We discover that while knowledge distillation improves the overall performance of student models, the performance improvement for some samples in the tail is limited, which is a rarely addressed issue. These tail samples can lead to poor learning of the teacher's feature distribution in the corresponding regions of the feature space, thereby limiting the alignment between the student and the teacher. Since tail samples often lack clear label definitions in many tasks, we identify them by analyzing the average feature similarity from the teacher model. To improve knowledge distillation, we propose a Sample-wise Re-weighting (SRW) method, assigning different loss function weights to samples based on their average similarity. Experimental results show that our method enhances the performance of student models across different tasks and can be combined with various knowledge distillation methods. Additionally, our approach demonstrates advantages in foundational models such as Segmentation Anything Models (SAM) and Contrastive LanguageImage Pretraining (CLIP) models.",https://openaccess.thecvf.com/content/ACCV2024/html/Yu_Knowledge_Distillation_Dealing_with_Sample-wise_Long-tail_Problem_ACCV_2024_paper.html
Mamba-based Light Field Super-Resolution with Efficient Subspace Scanning,"Transformer-based methods have demonstrated impressive performance in 4D light field (LF) super-resolution by effectively modeling long-range spatial-angular correlations, but their quadratic complexity hinders the efficient processing of high resolution 4D inputs, resulting in slow inference speed and high memory cost. As a compromise, most prior work adopts a patch-based strategy, which fails to leverage the full information from the entire input LFs. The recently proposed selective state-space model, Mamba, has gained popularity for its efficient long-range sequence modeling. In this paper, we propose a Mamba-based Light Field Super-Resolution method, named MLFSR, by designing an efficient subspace scanning strategy. Specifically, we tokenize 4D LFs into subspace sequences and conduct bi-directional scanning on each subspace. Based on our scanning strategy, we then design the Mamba-based Global Interaction module to capture global information and the Spatial-Angular Modulator to complement local details. Additionally, we introduce a Transformer-to-Mamba loss to further enhance overall performance. Extensive experiments on public benchmarks demonstrate that MLFSR surpasses CNN-based models and rivals Transformer-based methods in performance while maintaining higher efficiency. With quicker inference speed and reduced memory demand, MLFSR facilitates full-image processing of high-resolution 4D LFs with enhanced performance. Our code is available at https://github.com/RSGao/MLFSR.",https://openaccess.thecvf.com/content/ACCV2024/html/Gao_Mamba-based_Light_Field_Super-Resolution_with_Efficient_Subspace_Scanning_ACCV_2024_paper.html
NewMove: Customizing text-to-video models with novel motions,"We introduce an approach for augmenting text-to-video generation models with novel motions, extending their capabilities beyond the motions contained in the original training data. By leveraging a few video samples demonstrating specific movements as input, our method learns and generalizes the input motion patterns for diverse, text-specified scenarios. Our method finetunes an existing text-to-video model to learn a novel mapping between the depicted motion in the input examples to a new unique token. To avoid overfitting to the new custom motion, we introduce an approach for regularization over videos. Leveraging the motion priors in a pretrained model, our method can learn a generalized motion pattern, that can be invoked with novel videos featuring multiple people doing the custom motion, or using the motion in combination with other motions. To validate our method, we quantitatively evaluate the learned custom motion and perform a systematic ablation study. We show that our method significantly outperforms prior appearance-based customization approaches when extended to the motion customization task.",https://openaccess.thecvf.com/content/ACCV2024/html/Materzynska_NewMove_Customizing_text-to-video_models_with_novel_motions_ACCV_2024_paper.html
QR-DETR : Query Routing for Detection Transformer,"Detection Transformer (DETR) predicts object bounding boxes and classes from learned object queries. However, DETR exhibits three major flaws: (1) Only a subset of object queries contribute to the final predictions, leading to inefficient utilization of computational resources. (2) The self-attention and feed-forward layers indiscriminately mix information across object queries without any guidance, potentially hindering effective learning of object representations. (3) At each decoder stack layers, a query can evolve either positively, refining its bounding box and class attributes correctly, or negatively, shifting to predict a different object or increasing its bounding box erroneously. This suggest that query informativeness is non-uniform, and enabling inter-query communication could impede the learning of specialized representations for individual queries. To address these concerns, we propose a learnable query routing method that introduces a routing model to identify the object queries requiring processing at each transformer decoder layer. The selected queries pass through the entire decoder stack, while others exit early. Subsequently, all queries are scattered to their original positions after feed-forward processing of the passed queries. This process prevents indiscriminate information sharing across all queries. Extensive experiments on the COCO dataset demonstrate the effectiveness of our method, showing a consistent increase in mAP across multiple DETR models.",https://openaccess.thecvf.com/content/ACCV2024/html/Senthivel_QR-DETR__Query_Routing_for_Detection_Transformer_ACCV_2024_paper.html
SurfOcc: Surface-based Feature Lifting for Vision-centric 3D Occupancy Prediction,"3D occupancy prediction has been an emerging trend in 3D perception for its superiority in preserving exquisite geometric and semantic details. However, existing vision-based approaches either leave features unrefined or neglect depth ambiguity due to defective 2D-to-3D feature lifting modules, leading to imprecise prediction results. In this paper, we introduce SurfOcc, a vision-centric 3D occupancy prediction framework which addresses these limitations fundamentally. SurfOcc decouples the learning process of observed surfaces and occluded regions while seamlessly integrating them into an end-to-end architecture. Specifically, we first propose surface-based feature lifting to precisely locate observed surfaces and enhance the selected surface voxels via cross-attention during feature lifting. Then we design a feature diffuser which incorporates both local and global features to diffuse the reliable surface features to occluded regions. Experiments show that SurfOcc achieves state-of-the-art performance with 13.75 mIoU on SemanticKITTI and 42.38 mIoU on Occ3D-nuScenes, which also demonstrates the potential of SurfOcc in handling occlusion situations. Code is available at https://github.com/sullicsullic/SurfOcc.",https://openaccess.thecvf.com/content/ACCV2024/html/Ye_SurfOcc_Surface-based_Feature_Lifting_for_Vision-centric_3D_Occupancy_Prediction_ACCV_2024_paper.html
Hierarchical Prompting for Diffusion Classifiers,"Recently, large-scale pre-trained text-to-image models like Stable Diffusion have demonstrated unparalleled capabilities, revolutionizing many tasks. Recent studies have found that these advanced generative models can be applied to discriminative tasks, showing strong accuracy and robustness in zero-shot recognition. However, the current pipeline suffers from impractical inference speed (about 1 minute per image). In this paper, we introduce Hierarchical Prompt Learning, a simple and effective pipeline to achieve high-speed classification for diffusion generators. Our method first proposes a hierarchical evaluation strategy, leveraging prior class tree taxonomy to reduce unnecessary class modeling. To handle the excessive sampling steps, we employ prompt learning, a parameter-efficient technique, to adapt downstream task-specific knowledge into the conditional text embedding. This allows our method to efficiently sample diffusion models in just 25 steps while maintaining high accuracy. The proposed hierarchical evaluation achieves up to 3.5x speedups compared to previous diffusion classifiers, and the combination with prompt learning achieves up to 20x speedups. Beyond efficiency, our method also maintains high performance in zero-shot and few-shot scenarios, both in-distribution and out-of-distribution. Moreover, our visualization analysis sheds light on what our diffusion prompts learn, providing insights into the model's decision-making process. Codes are available at https://github.com/PRIS-CV/Hierarchical-Prompting-for-Diffusion-Classifiers.",https://openaccess.thecvf.com/content/ACCV2024/html/Ning_Hierarchical_Prompting_for_Diffusion_Classifiers_ACCV_2024_paper.html
Enhanced Asymmetric Invertible Network for Neural Video Delivery,"Internet video streaming has experienced explosive growth over the past few years. Recently, super-resolution (SR) networks have been utilized to reduce the bandwidth and improve the quality of Internet video streaming. These methods first employ a predefined and immutable downscaling kernel, such as bicubic interpolation, to transform high-resolution (HR) video into low-resolution (LR) video. Subsequently, the LR video is partitioned into segments, which are streamed alongside corresponding models to the clients. The client subsequently executes inference models to perform SR on the LR segments. However, this normal downscaling is not an injective mapping because high-frequency information is lost. This creates the ill-posed problem of the inverse upscaling procedure and makes it highly difficult to get details back from downscaled LR videos. In this paper, we propose a novel method for video delivery. Specifically, we deliberately designed an Enhanced Asymmetric Invertible Network (EAIN) to produce high-quality LR videos while capturing the distribution of missing information using a latent variable that follows a specified distribution in the downscaling process. HR videos are available by passing a randomly extracted latent variable through the network in reverse with LR videos. Extensive experiments show that our methods significantly improve video streaming quality compared to state-of-the-art neural video delivery methods, paving the way for the application of neural video delivery techniques in practice. The code is available at https://github.com/Anonymous-ACCV-2024/EAIN.",https://openaccess.thecvf.com/content/ACCV2024/html/Tian_Enhanced_Asymmetric_Invertible_Network_for_Neural_Video_Delivery_ACCV_2024_paper.html
DepthSegNet24: A Label-Free Model for Robust Day-Night Depth and Semantics,"This paper presents a novel multi-task model combining self-supervised monocular depth estimation and knowledge-distilled semantic segmentation that can perform both tasks simultaneously and consistently in both daytime and nighttime conditions. By leveraging the joint self-supervised and supervised knowledge distillation learning, the model can learn consistent and complementary representations of the two tasks to improve the generalization ability without relying on annotated ground-truth data. To address the extremely varying lighting conditions between day and night, we first synthesize night and day images from their corresponding real day and night images, and then train the model with the day-night image pairs to provide explicit correspondences between the two lighting conditions for capturing the contextual and detailed information in both scenarios. We also augment the model with a light enhancement module and a daytime depth pseudo-labels for achieving more accurate and robust depth and segmentation. Experimental results on Oxford RobotCar and nuScenes demonstrate the robustness of our model in diverse challenging lighting conditions.",https://openaccess.thecvf.com/content/ACCV2024/html/Thanh_DepthSegNet24_A_Label-Free_Model_for_Robust_Day-Night_Depth_and_Semantics_ACCV_2024_paper.html
DiffLoss: Unleashing Diffusion Model as Constraint for Training Image Restoration Network,"Image restoration aims to enhance low quality images, producing high quality images that exhibit natural visual characteristics and fine semantic attributes. Recently, the diffusion model has emerged as a powerful technique for image generation, and it has been explicitly employed as a backbone in image restoration tasks, yielding excellent results. However, it suffers from the drawbacks of slow inference speed and large model parameters due to its intrinsic characteristics. In this paper, we introduce a new perspective that implicitly leverages the diffusion model to assist the training of image restoration network, called  DiffLoss, which drives the restoration results to be optimized for naturalness and semantic-aware visual effect. To achieve this, we utilize the mode coverage capability of the diffusion model to approximate the distribution of natural images and explore its ability to capture image semantic attributes. On the one hand, we extract intermediate noise to leverage its modeling capability of the distribution of natural images, which serves as a naturalness-oriented optimization space. On the other hand, we utilize the bottleneck features of diffusion model to harness its semantic attributes serving as a constraint on  semantic level. By combining these two designs, the overall loss function is able to improve the perceptual quality of image restoration, resulting in visually pleasing and semantically enhanced outcomes. To validate the effectiveness of our method, we conduct experiments on various common image restoration tasks and benchmarks. Extensive experimental results demonstrate that our approach enhances the visual quality and semantic perception of the restoration network. Code is available at https://github.com/JosephTiTan/DiffLoss.",https://openaccess.thecvf.com/content/ACCV2024/html/Tan_DiffLoss_Unleashing_Diffusion_Model_as_Constraint_for_Training_Image_Restoration_ACCV_2024_paper.html
3D Prompt Learning for RGB-D Tracking,"Due to the remarkable advancements in RGB visual tracking, there has been a growing interest in RGB-D tracking, owing to its robust performance even in challenging scenarios. To bridge the gap between RGB and RGB-D tracking, several 2D prompt learning methods have emerged, which primarily target on downstream task adaptation. In contrast, we introduce a novel prompt learning method for RGB-D tracking, termed as 3D Prompt Tracking (3DPT), which is able to capture essential 3D geometric information and transform base RGB trackers into RGB-D trackers through parameter efficient tuning. Compared to those counterparts using depth maps as 2D prompts, we propose to directly encode 3D features from point clouds into base models, leading to more superior discriminative powers, particularly when the target and background distractors share similar visual appearance. We achieve this goal through an elaborately designed Geometry Prompt (GP) block, which can effectively extract 3D features, and inject the 3D knowledge into the 2D base model. The GP block is generally applicable to recent visual trackers, yielding more robust tracking performance with reasonable computational overhead. Extensive experiments demonstrate that our 3D Prompt Tracking delivers promising performance and can generalize across three popular RGB-D tracking datasets, including DepthTrack, CDTB, and VOT-RGBD2022.",https://openaccess.thecvf.com/content/ACCV2024/html/Li_3D_Prompt_Learning_for_RGB-D_Tracking_ACCV_2024_paper.html
TANet: Triplet Attention Network for All-In-One Adverse Weather Image Restoration,"Adverse weather image restoration aims to remove unwanted degraded artifacts, such as haze, rain, and snow, caused by adverse weather conditions. Existing methods achieve remarkable results in addressing single-weather conditions. However, they face challenges when encountering unpredictable weather conditions, which often happens in real-world scenarios. Although different types of weather conditions exhibit different degraded patterns, they share common characteristics that are highly related and complementary, including occlusions caused by degraded patterns, color distortion, and contrast attenuation due to the scattering of atmospheric particles. Therefore, we focus on leveraging common knowledge across multiple weather conditions to restore images in a unified manner.  In this paper, we propose a Triplet Attention Network (TANet) to efficiently and effectively address all-in-one adverse weather image restoration. TANet incorporates three types of attention mechanisms: local pixel-wise attention and global strip-wise attention to address occlusions caused by non-uniform degraded patterns, and global distribution attention to address color distortion and contrast attenuation caused by atmospheric phenomena. By leveraging common knowledge shared among different weather conditions, TANet successfully addresses multiple weather conditions in a unified manner. Experimental results show that TANet efficiently and effectively achieves state-of-the-art performance in all-in-one adverse weather image restoration.",https://openaccess.thecvf.com/content/ACCV2024/html/Wang_TANet_Triplet_Attention_Network_for_All-In-One_Adverse_Weather_Image_Restoration_ACCV_2024_paper.html
PMTrack: Multi-object Tracking with Motion-Aware,"Tracking-by-detection typically involves associating detection boxes across frames in a video sequence. A common approach is to use Kalman filter for prediction and matching with detection boxes based on IoU. However, the Kalman filter is a linear prediction method, which, in scenarios involving camera motion or nonlinear object motion, will result in issues like ID switching or tracking loss. To address the problem, we propose a method that leverages phase correlation to calculates the translational relationship between adjacent frames, maping target positions into the current frame's coordinate system. This positional correction effectively compensates for the shifts caused by camera movement, significantly reducing ID switches. Furthermore, our method distinguishes between the motion and stationary states of trajectories, thereby  enhancing tracking stability and accuracy. Our experimental results demonstrate that the proposed approach attains real-time efficiency and excels in scenes with camera motion. It achieves an MOTA of 80.17%, IDF1 of 78.93%, and HOTA of 64.04% on the MOT17 test sets, surpassing mainstream works in terms of multiple performance indicators.",https://openaccess.thecvf.com/content/ACCV2024/html/Guo_PMTrack_Multi-object_Tracking_with_Motion-Aware_ACCV_2024_paper.html
Designing Extremely Memory-Efficient CNNs for On-device Vision Tasks,"In this paper, we introduce a memory-efficient CNN (convolutional neural network), which enables resource-constrained low-end embedded and IoT devices to perform on-device vision tasks, such as image classification and object detection, using extremely low memory, i.e., only 63 KB on ImageNet classification. Based on the bottleneck block of MobileNet, we propose three design principles that significantly curtail the peak memory usage of a CNN so that it can fit the limited KB memory of the low-end device. First, `input segmentation' divides an input image into a set of patches, including the central patch overlapped with the others, reducing the size (and memory requirement) of a large input image. Second, `patch tunneling' builds independent tunnel-like paths consisting of multiple bottleneck blocks per patch, penetrating through the entire model from an input patch to the last layer of the network, maintaining lightweight memory usage throughout the whole network. Lastly, `bottleneck reordering' rearranges the execution order of convolution operations inside the bottleneck block such that the memory usage remains constant regardless of the size of the convolution output channels. The experiment result shows that the proposed network classifies ImageNet with extremely low memory (i.e., 63 KB) while achieving competitive top-1 accuracy (i.e., 61.58%). To the best of our knowledge, the memory usage of the proposed network is far smaller than state-of-the-art memory-efficient networks, i.e., up to 89x and 3.1x smaller than MobileNet (i.e., 5.6 MB) and MCUNet (i.e., 196 KB), respectively.",https://openaccess.thecvf.com/content/ACCV2024/html/Lee_Designing_Extremely_Memory-Efficient_CNNs_for_On-device_Vision_Tasks_ACCV_2024_paper.html
A Multi-Phase Multi-Graph Approach for Focal Liver Lesion Classification on CT Scans,"Liver cancer remains a leading cause of global mortality, driving interest in computer-aided diagnosis for liver tumor detection. Existing methods typically focus on individual lesions and avoid the impact of neighboring tumors on diagnostic accuracy. This study introduces a novel multi-phase multi-graph (MPMG) approach to improve liver tumor classification using contrast-enhanced computed tomography (CECT) scans. The MPMG method models inter-lesion relationships, including the ratio of diameters, semantic similarity, physical distance, and neighbor influence score as graph edge embeddings, while multiphasic features extracted from a proposed deep convolutional neural network form the node representations. By analysing different edge embedding formations, we find through extensive experiments that the proposed MPMG model outperforms several state-of-the-art methods in liver tumor diagnosis.",https://openaccess.thecvf.com/content/ACCV2024/html/Sam_A_Multi-Phase_Multi-Graph_Approach_for_Focal_Liver_Lesion_Classification_on_ACCV_2024_paper.html
A Recipe for CAC: Mosaic-based Generalized Loss for Improved Class-Agnostic Counting,"Class agnostic counting (CAC) is a vision task that can be used to count the total occurrence number of any given reference objects in the query image. The task is usually formulated as a density map estimation problem through similarity computation among a few image samples of the reference object and the query image. In this paper, we point out a severe issue of the existing CAC framework: Given a multi-class setting, models don't consider reference images and instead blindly match all dominant objects in the query image. Moreover, the current evaluation metrics and dataset cannot be used to faithfully assess the model's generalization performance and robustness. To this end, we discover that the combination of mosaic augmentation with generalized loss is essential for addressing the aforementioned issue of CAC models to count objects of majority (i.e. dominant objects) regardless of the references. Furthermore, we introduce a new evaluation protocol and metrics for resolving the problem behind the existing CAC evaluation scheme and better benchmarking CAC models in a more fair manner. Besides, extensive evaluation results demonstrate that our proposed recipe can consistently improve the performance of different CAC models. The code is available at https://github.com/littlepenguin89106/MGCAC.",https://openaccess.thecvf.com/content/ACCV2024/html/Chou_A_Recipe_for_CAC_Mosaic-based_Generalized_Loss_for_Improved_Class-Agnostic_ACCV_2024_paper.html
RS-SAM: Integrating Multi-Scale Information for Enhanced Remote Sensing Image Segmentation,"The introduction of the Segment Anything Model (SAM) provides a powerful pre-trained model for image segmentation tasks. However, its utilization in remote sensing image segmentation encounters notable challenges. First, SAM is primarily trained on large-scale natural images as a general visual model, which hinders its direct application to remote sensing field. Second, due to the diversity of spatial objects in remote sensing images, the naive columnar ViT structure of SAM leads to poor segmentation performance. Finally, SAM is designed primarily to distinguish between foreground and background, resulting in a simple structure that struggles with precise semantic segmentation. To address the above issues, we introduce RS-SAM, a prompt-free adaptation of SAM in the realm of remote sensing, with multi-scale ViT backbone. More specifically, we start by crafting an adapter for the SAM encoder to transferring SAM to the domain of remote sensing. Next, we addressed the encoder's limitations by integrating a Multi-scale Neck for capturing objects in different sizes. Finally, to enhance the segmentation results, we propose a Multi-scale Progressive Refinement Module to aggregate multi-scale and low-level features. Through experiments conducted on three public remote sensing datasets, our model outperforms the baseline by 0.8% to 6.2% on the Dice metric, which fully proves the effectiveness of our method.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_RS-SAM_Integrating_Multi-Scale_Information_for_Enhanced_Remote_Sensing_Image_Segmentation_ACCV_2024_paper.html
VIPNet: Combining Viewpoint Information and Shape Priors for Instant Multi-View 3D Reconstruction,"While the multi-view 3D reconstruction task has made significant progress, existing methods simply fuse multi-view image features without effectively leveraging available auxiliary information, especially the viewpoint information for guiding and associating features of different views. To this end, we propose to enhance multi-view 3D reconstruction with the power of viewpoint information. Specifically, a simple-yet-effective viewpoint estimator is designed to learn and provide comprehensive viewpoint knowledge for locating and associating learned features from different views. Moreover, to improve the 3D reconstruction quality when 2D images of only very few viewpoints are available, we propose to learn the shape prior knowledge to provide sufficient shape information for compensating the limited 2D observations. Overall, we present VIPNet, benefiting from Viewpoint Information and Shape Prior learning for high-quality multi-view 3D reconstruction. Extensive experiments validate the effectiveness of the proposed VIPNet, which achieves state-of-the-art performance on challenging datasets and shows well generalization ability in real-world scenarios.",https://openaccess.thecvf.com/content/ACCV2024/html/Ye_VIPNet_Combining_Viewpoint_Information_and_Shape_Priors_for_Instant_Multi-View_ACCV_2024_paper.html
Leveraging Semantic Cues from Foundation Vision Models for Enhanced Local Feature Correspondence,"Visual correspondence is a crucial step in key computer vision tasks, including camera localization, image registration, and structure from motion. The most effective techniques for matching keypoints currently involve using learned sparse or dense matchers, which need pairs of images. These neural networks have a good general understanding of features from both images, but they often struggle to match points from different semantic areas. This paper presents a new method that uses semantic cues from foundation vision model features (like DINOv2) to enhance local feature matching by incorporating semantic reasoning into existing descriptors. Therefore, the learned descriptors do not require image pairs at inference time, allowing feature caching and fast matching using similarity search, unlike learned matchers. We present adapted versions of six existing descriptors, with an average increase in performance of 29% in camera localization, with comparable accuracy to existing matchers as LightGlue and LoFTR in two existing benchmarks. Both code and trained models are available at https://www.verlab.dcc.ufmg.br/descriptors/reasoning_accv24/",https://openaccess.thecvf.com/content/ACCV2024/html/Cadar_Leveraging_Semantic_Cues_from_Foundation_Vision_Models_for_Enhanced_Local_ACCV_2024_paper.html
SAMIF: Adapting Segment Anything Model for Image Inpainting Forensics,"Image inpainting technologies pose increasing threats to the security of image data through malicious use. Therefore, image inpainting forensics is crucial. The Segment Anything Model (SAM) is a powerful universal image segmentation model for various downstream tasks. However, the performance of SAM in inpainting forensics is significantly degraded due to the substantial disparity between natural and inpainted image domains. In this paper, we propose SAMIF, a SAM-based model for image inpainting forensics. First, based on SAM, a parallel convolutional neural network (CNN) branch is introduced to assist the SAM in extracting local noise information. Second, the cross-domain alignment fusion module (CAFM) is designed to better fuse the features of the two branches. Third, the artifact features generator (AFG) is designed between the encoder and decoder to disentangle the features extracted by the encoder. The auxiliary loss is introduced in AFG, which shortens the backpropagation path and guides the SAM branch to learn artifact features, thus enhancing the adaptability of SAM for the inpainting forensics task. Extensive experiments demonstrate that the proposed model achieves state-of-the-art results on five inpainting forensics datasets and exhibits excellent robustness and generalization capabilities.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_SAMIF_Adapting_Segment_Anything_Model_for_Image_Inpainting_Forensics_ACCV_2024_paper.html
Language-Guided Joint Audio-Visual Editing via One-Shot Adaptation,"In this paper, we introduce a novel task called language-guided joint audio-visual editing. Given an audio and image pair of a sounding event, this task aims at generating new audio-visual content by editing the given sounding event conditioned on the language guidance. For instance, we can alter the background environment of a sounding object while keeping its appearance unchanged, or we can add new sounds contextualized to the visual content. To address this task, we propose a new diffusion-based framework for audio-visual editing and introduce two key ideas. Firstly, we propose a one-shot adaptation approach to tailor generative diffusion models for audio-visual content editing. With as few as one audio-visual sample, we jointly transfer the audio and vision diffusion models to the target domain. After fine-tuning, our model enables consistent generation of this audio-visual sample. Secondly, we introduce a cross-modal semantic enhancement approach. We observe that when using language as content editing guidance, the vision branch may overlook editing requirements. This phenomenon, termed catastrophic neglect, hampers audio-visual alignment during content editing. We therefore enhance semantic consistency between language and vision to mitigate this issue. Extensive experiments validate the effectiveness of our method in language-based audio-visual editing and highlight its superiority over several baseline approaches. We recommend that readers visit our project page for more details: https://liangsusan-git.github.io/project/avedit/.",https://openaccess.thecvf.com/content/ACCV2024/html/Liang_Language-Guided_Joint_Audio-Visual_Editing_via_One-Shot_Adaptation_ACCV_2024_paper.html
Faster convergence and Uncorrelated gradients in Self-Supervised Online Continual Learning,"Self-Supervised Online Continual Learning (SSOCL) focuses on continuously training neural networks from data streams. This presents a more realistic Self-Supervised Learning (SSL) problem setting, where the goal is to learn directly from real-world data streams. However, common SSL requires multiple offline training sessions with fixed IID datasets to acquire appropriate feature representations. In contrast, SSOCL involves learning from a non-IID data stream where the data distribution changes over time, and new data is added sequentially. Consequently, the challenges are insufficient learning with changing data distributions and the learning inferior feature representations from non-IID data streams. In this study, we propose a method to address these challenges in SSOCL. The proposal method consists of a Multi-Crop Contrastive Loss, TCR Loss, and data selection based on cosine similarity to representative features. Multi-Crop Contrastive Loss and TCR Loss enable quick adaptation to changes in data distribution. Cosine similarity-based data selection ensures diverse data is stored in the replay buffer, facilitating learning from non-IID data streams. The proposed method shows superior accuracy compared to existing methods in evaluations using CIFAR-10, CIFAR-100, ImageNet-100, and CORe50.",https://openaccess.thecvf.com/content/ACCV2024/html/Imai_Faster_convergence_and_Uncorrelated_gradients_in_Self-Supervised_Online_Continual_Learning_ACCV_2024_paper.html
Dynamic Window Transformer for Image Super-Resolution,"Image super-resolution (SR) reconstruction is a critical task in image processing that aims to generate high-resolution (HR) images from low-resolution (LR) inputs. Recently, Swin-Transformer-based models have become mainstream in this field due to their efficient handling of computational complexity and scalability, where window-based mechanisms are employed to effectively extract local features and window interaction strategies are utilized to enhance global information integration. However, existing Swin-Transformer-based SR models employ the fixed-window strategy, confining attention in fixed areas. In this paper, we present Dynamic Window Transformer (DWT), a simple but novel method that can use windows of various shapes to effectively extract diverse features and achieve efficient global dependency modelling by utilizing image anisotropy. The core of our DWT is Dynamic-Window Self-Attention (DWSA), which dynamically selects the optimal window for different inputs to perform self-attention. We evaluate our model on various popular benchmark datasets and compare it with other state-of-the-art (SOTA) lightweight models. For example, our DWT achieves a PSNR of 26.56 dB on the Urban100 dataset, which is 0.09 dB higher than the SOTA model SwinIR.",https://openaccess.thecvf.com/content/ACCV2024/html/Xie_Dynamic_Window_Transformer_for_Image_Super-Resolution_ACCV_2024_paper.html
Direct Alignment for Robust NeRF Learning,"Differentiable volume rendering has evolved to be the prevalent optimization technique for creating implicit and explicit maps. Numerous efforts have explored the role of camera pose optimization and non-rigid tracking within Neural Radiance Fields (NeRFs). However, the relation between differentiable volume rendering and classical multi-view geometry remains under explored.  In this work, we investigate the role of direct alignment in radiance field estimation by incorporating a simple but effective loss while training NeRFs. Armed with good practices for direct alignment while leveraging the effectiveness of volumetric representation in occlusion handling, our proposed framework is able to reconstruct real scenes from sparse or dense views at a much higher accuracy.  We show despite relying on the photometric consistency, incorporating direct alignment improves view synthesis accuracy of NeRFs by 12% with known poses on LLFF dataset whereas joint optimization of pose and radiance field  gets a boost in view synthesis accuracy of over 18% with rotation and translation errors going down by 64% and 57% respectively.",https://openaccess.thecvf.com/content/ACCV2024/html/Garg_Direct_Alignment_for_Robust_NeRF_Learning_ACCV_2024_paper.html
More and Larger Auxiliary Feature-Guided Spatial-Temporal Super-Resolution for Rendered Sequences,"The post-processing rendered sequences improves the quality of the sequences and shortens the time of the rendering phase. However, most of the current post-processing methods for sequences are suitable for video. Directly transferring these methods to rendered sequences cannot obtain high-quality results. To address this problem, we propose an end-to-end spatial-temporal super-resolution network for rendered sequences, which improves rendering efficiency by simultaneously implementing frame interpolation (FI) and super-resolution. In the FI task, accurately inferring results that closer to the real motion state of the target frames is the key and difficult point to ensure the generation effect. For this issue, we design an auxiliary feature-guided interpolation (AFGI) module. By introducing the auxiliary features corresponding to the target frames, AFGI module provides the real motion state of the target frames to the network. In the part of aggregating contextual information, we propose a weighted aggregation upsampling (WAUpS) module. Aggregation is selectively performed based on the correlation between incoming information and the current frame. WAUpS module effectively prevents irrelevant information from affecting the super-resolution results, which is a problem with the direct aggregation methods used previously. At the same time, WAUpS module combines the upsampled features with the corresponding high-resolution auxiliary features. This addition provides the output with rich detail textures and other key information, further improving the overall processing quality. Experimental results show that compared with state-of-the-art (SOTA) methods, our method not only obtains high-quality rendered sequences processing results, but also effectively improves the rendering efficiency.",https://openaccess.thecvf.com/content/ACCV2024/html/Zheng_More_and_Larger_Auxiliary_Feature-Guided_Spatial-Temporal_Super-Resolution_for_Rendered_Sequences_ACCV_2024_paper.html
Strike the Balance: On-the-Fly Uncertainty based User Interactions for Long-Term Video Object Segmentation,"In this paper, we introduce a variant of video object segmentation VOS) that bridges interactive and semi-automatic approaches, termed Lazy Video Object Segmentation (ziVOS). In contrast, to both tasks, which handle video object segmentation in an off-line manner (i.e., pre-recorded sequences), we propose through ziVOS to target online recorded sequences. Here, we strive to strike a balance between performance and robustness for long-term scenarios by soliciting user feedbacks on-the-fly during the segmentation process. Hence, we aim to maximize the tracking duration of an object of interest, while requiring minimal user corrections to maintain tracking over an extended period. We propose Lazy-XMem as a competitive baseline, that estimates the uncertainty of the tracking state to determine whether a user interaction is necessary to refine the model's prediction. We introduce complementary metrics alongside those already established in the field, to quantitatively assess the performance of our method and the user's workload. We evaluate our approach using the recently introduced LVOS dataset, which offers numerous long-term videos. Our code is available at https://github.com/Vujas-Eteph/LazyXMem.",https://openaccess.thecvf.com/content/ACCV2024/html/Vujasinovic_Strike_the_Balance_On-the-Fly_Uncertainty_based_User_Interactions_for_Long-Term_ACCV_2024_paper.html
Unsupervised Video Summarization via Iterative Training and Simplified GAN,"This paper introduces a new, unsupervised method for auto- matic video summarization using ideas from generative adversarial net- works but eliminating the discriminator, having a simple loss function, and separating training of different parts of the model. An iterative training strategy is also applied by alternately training the reconstructor and the frame selector for multiple iterations. Furthermore, a trainable mask vector is added to the model in summary generation during training and evaluation. The method also includes an unsupervised model selection algorithm. Results from experiments on two public datasets (SumMe and TVSum) and four datasets we created (Soccer, LoL, MLB, and Short- MLB) demonstrate the effectiveness of each component on the model performance, particularly the iterative training strategy. Evaluations and comparisons with the state-of-the-art methods highlight the advantages of the proposed method in performance, stability, and training efficiency.",https://openaccess.thecvf.com/content/ACCV2024/html/Li_Unsupervised_Video_Summarization_via_Iterative_Training_and_Simplified_GAN_ACCV_2024_paper.html
Generalizable Structure-Aware INF: Biplanar-View CT Reconstruction via Disentangled Implicit Neural Field,"Structure-aware CT reconstruction from a single or biplanar X-rays produces patient-specific 3D insights into underlying structures, pushing the radiation hazards during diagnosis and treatment to a minimum.  Existing implicit neural fields (INF) methods have shown impressive performance in CT reconstruction, though they rely on multi-view X-rays and conduct subject-specific reconstruction. Additional online adaptation is required to handle novel subjects. In this paper, we present the generalizable structure-aware implicit neural fields (GSA-INF), a unified model that learns a generalizable structure-aware volume prior to INF decoding from sparse-view X-rays. Previous CoderNeRF views the latent code as a holistic shape prior to 3D reconstruction. In contrast, we present a new triplane generative model to learn a generalizable volume prior distribution, where the sampled triplane latent code produces voxel-level representation for INF decoding and CT reconstruction. Moreover, we introduce anatomical structure mask supervision by building a parallel INF-based decoding framework that enhances structure disentanglements when popping up a variety of structures from 2D X-rays. Our approach entails simultaneous INF-based CT reconstruction and volume-prior learning. In the online inference process, we can conditionally reconstruct CT from single- or biplanar-view X-rays and unconditionally generate CTs via sampling in the latent space. GSA-INF demonstrates robust and superior results over the compared methods.",https://openaccess.thecvf.com/content/ACCV2024/html/Huang_Generalizable_Structure-Aware_INF_Biplanar-View_CT_Reconstruction_via_Disentangled_Implicit_Neural_ACCV_2024_paper.html
MECFormer: Multi-task Whole Slide Image Classification with Expert Consultation Network,"Whole slide image (WSI) classification is a crucial problem for cancer diagnostics in clinics and hospitals. A WSI, acquired at gigapixel size, is commonly tiled into patches and processed by multiple-instance learning (MIL) models. Previous MIL-based models designed for this problem have only been evaluated on individual tasks for specific organs, and the ability to handle multiple tasks within a single model has not been investigated. In this study, we propose MECFormer, a generative Transformer-based model designed to handle multiple tasks within one model. To leverage the power of learning multiple tasks simultaneously and to enhance the model's effectiveness in focusing on each individual task, we introduce an Expert Consultation Network, a projection layer placed at the beginning of the Transformer-based model. Additionally, to enable flexible classification, autoregressive decoding is incorporated by a language decoder for WSI classification. Through extensive experiments on five datasets involving four different organs, one cancer classification task, and four cancer subtyping tasks, MECFormer demonstrates superior performance compared to individual state-of-the-art multiple-instance learning models.",https://openaccess.thecvf.com/content/ACCV2024/html/Bui_MECFormer_Multi-task_Whole_Slide_Image_Classification_with_Expert_Consultation_Network_ACCV_2024_paper.html
SRIL: Selective Regularization for Class-Incremental Learning,"Human intelligence gradually accepts new information and accumulates knowledge throughout the lifespan. However, deep learning models suffer from a catastrophic forgetting phenomenon, where they forget previous knowledge when acquiring new information. Class-Incremental Learning aims to create an integrated model that balances plasticity and stability to overcome this challenge. In this paper, we propose a selective regularization method that accepts new knowledge while maintaining previous knowledge. We first introduce an asymmetric feature distillation method for old and new classes inspired by cognitive science, using the gradient of classification and knowledge distillation losses to determine whether to perform pattern completion or pattern separation. We also propose a method to selectively interpolate the weight of the previous model for a balance between stability and plasticity, and we adjust whether to transfer through model confidence to ensure the performance of the previous class and enable exploratory learning. We validate the effectiveness of the proposed method, which surpasses the performance of existing methods through extensive experimental protocols using CIFAR-100, ImageNet-Subset, and ImageNet-Full.",https://openaccess.thecvf.com/content/ACCV2024/html/Han_SRIL_Selective_Regularization_for_Class-Incremental_Learning_ACCV_2024_paper.html
DeTurb: Atmospheric Turbulence Mitigation with Deformable 3D Convolutions and 3D Swin Transformers,"Atmospheric turbulence in long-range imaging significantly degrades the quality and fidelity of captured scenes due to random variations in both spatial and temporal dimensions. These distortions present a formidable challenge across various applications, from surveillance to astronomy, necessitating robust mitigation strategies. While model-based approaches achieve good results, they are very slow. Deep learning approaches show promise in image and video restoration but have struggled to address these spatiotemporal variant distortions effectively. This paper proposes a new framework that combines geometric restoration with an enhancement module. Random perturbations and geometric distortion are removed using a pyramid architecture with deformable 3D convolutions, resulting in aligned frames. These frames are then used to reconstruct a sharp, clear image via a multi-scale architecture of 3D Swin Transformers. The proposed framework demonstrates superior performance over the state of the art for both synthetic and real atmospheric turbulence effects, with reasonable speed and model size.",https://openaccess.thecvf.com/content/ACCV2024/html/Zou_DeTurb_Atmospheric_Turbulence_Mitigation_with_Deformable_3D_Convolutions_and_3D_ACCV_2024_paper.html
DepthBLIP-2: Leveraging Language to Guide BLIP-2 in Understanding Depth Information,"In recent years, visual language models have made significant advancements in the fields of computer vision and natural language processing. The BLIP-2 model effectively bridges modality gaps through its lightweight Q-Former, demonstrating excellent results with low training costs and highlighting the potential development directions for visual language models. However, applying BLIP-2 to more complex quantized target tasks, such as monocular depth estimation, presents challenges. In this paper, we propose a method for monocular depth estimation using BLIP-2. Our approach draws inspiration from DepthCLIP's use of language-guided models to comprehend depth information, leveraging the Q-Former module for modality fusion. Additionally, we introduce an adaptive depth bin to enhance the model's robustness against quantized distances. We name our method DepthBLIP-2 and make our code publicly available at: https://github.com/especiallyW/DepthBLIP-2.",https://openaccess.thecvf.com/content/ACCV2024/html/Chen_DepthBLIP-2_Leveraging_Language_to_Guide_BLIP-2_in_Understanding_Depth_Information_ACCV_2024_paper.html
Learning Neural Radiance Field from Quasi-Uniformly Sampled Spherical Image for Immersive Virtual Reality,"The neural radiance field (NeRF) is a prominent method of novel view synthesis that is widely applied to many tasks. Recently, the NeRF method, which was originally developed for perspective images, has been extended to 360-degree omnidirectional images, which may make it easier to perform immersive virtual reality tasks. However, in existing NeRF methods, omnidirectional images are typically represented as rectangular images. Equirectangular image representation is popular but is also known for distortion, especially as it approaches the poles. In this paper, we propose a method for learning a neural radiance field from geodesic dome division-based discrete spherical images. First, an input equirectangular image is mapped to a unit sphere. Then, the mapped spherical image is sampled on a unit sphere on the basis of geodesic dome division. Next, the obtained discrete spherical image is used to train an NeRF network. Finally, comparative experiments are conducted for the equirectangular image representation and geodesic dome division discrete spherical image representation. The experimental results demonstrate that the proposed method outperforms existing equirectangular image representation methods.",https://openaccess.thecvf.com/content/ACCV2024/html/Wang_Learning_Neural_Radiance_Field_from_Quasi-Uniformly_Sampled_Spherical_Image_for_ACCV_2024_paper.html
Strong but simple: A Baseline for Domain Generalized Dense Perception by CLIP-based Transfer Learning,"Domain generalization (DG) remains a significant challenge for perception based on deep neural networks (DNNs), where domain shifts occur due to synthetic data, lighting, weather, or location changes. Vision-language models (VLMs) marked a large step for the generalization capabilities and have been already applied to various tasks. Very recently, first approaches utilized VLMs for domain generalized segmentation and object detection and obtained strong generalization. However, all these approaches rely on complex modules, feature augmentation frameworks or additional models. Surprisingly and in contrast to that, we found that simple fine-tuning of vision-language pre-trained models yields competitive or even stronger generalization results while being extremely simple to apply. Moreover, we found that vision-language pre-training consistently provides better generalization than the previous standard of vision-only pre-training. This challenges the standard of using ImageNet-based transfer learning for domain generalization. Fully fine-tuning a vision-language pre-trained model is capable of reaching the domain generalization SOTA when training on the synthetic GTA5 dataset. Moreover, we confirm this observation for object detection on a novel synthetic-to-real benchmark. We further obtain superior generalization capabilities by reaching 77.9% mIoU on the popular Cityscapes-to-ACDC benchmark. We also found improved in-domain generalization, leading to an improved SOTA of 86.4% mIoU on the Cityscapes test set marking the first place on the leaderboard.",https://openaccess.thecvf.com/content/ACCV2024/html/Hummer_Strong_but_simple_A_Baseline_for_Domain_Generalized_Dense_Perception_ACCV_2024_paper.html
DFIMat: Decoupled Flexible Interactive Matting in Multi-Person Scenarios,"Interactive portrait matting refers to extracting the soft portrait from a given image that best meets the user's intent through their inputs. Existing methods often underperform in complex scenarios, mainly due to three factors. (1) Most works apply a tightly coupled network that directly predicts matting results, lacking interpretability and resulting in inadequate modeling. (2) Existing works are limited to a single type of user input, which is ineffective for intention understanding and also inefficient for user operation. (3) The multi-round characteristics have been under-explored, which is crucial for user interaction. To alleviate these limitations, we propose DFIMat, a decoupled framework that enables flexible interactive matting. Specifically, we first decouple the task into 2 sub-ones: localizing target instances by understanding scene semantics and the flexible user inputs, and conducting refinement for instance-level matting. We observe a clear performance gain from decoupling, as it makes sub-tasks easier to learn, and the flexible multi-type input further enhances both effectiveness and efficiency. DFIMat also considers the multi-round interaction property, where a contrastive reasoning module is designed to enhance cross-round refinement. Another limitation for multi-person matting task is the lack of training data. We address this by introducing a new synthetic data generation pipeline that can generate much more realistic samples than previous arts. A new large-scale dataset SMPMat is subsequently established. Experiments verify the significant superiority of DFIMat. With it, we also investigate the roles of different input types, providing valuable principles for users. Our code and dataset will be public.",https://openaccess.thecvf.com/content/ACCV2024/html/Jiao_DFIMat_Decoupled_Flexible_Interactive_Matting_in_Multi-Person_Scenarios_ACCV_2024_paper.html
Multiple active stereo systems calibration method based on Neural SDF using DSSS for wide area 3D reconstruction,"One of the practical methods for general 3D shape reconstruction is active stereo, which involves capturing images of scenes illuminated by structured light and reconstructing the scenes from the images. Active stereo has the advantage of being able to achieve high-density reconstructions even for texture-less scenes. However, it requires finding dense correspondences between the captured image and the projected pattern, making it difficult to capture the scene using multiple systems simultaneously where patterns are overlapped each other. A simple solution is to switch the patterns on and off synchronously; however, this naturally decreases exposure time in inverse proportion to the number of systems, resulting in a lower signal-to-noise ratio (SNR). In addition, retrieving relative poses between a projector and a camera of active stereo systems as well as relative poses between multiple systems is another challenge. To address these challenges, we propose a technique based neural signed distance field (Neural-SDF) using Direct Sequence Spread Spectrum (DSSS). DSSS is the latest technique widely used in the field of communications for multiplexing multiple signals and separating them. We propose a novel method to utilize DSSS to project multiple structured light patterns onto the object, where the patterns are overlapped and separated efficiently. To calibrate the relative poses between projectors and cameras in multiple sets of active stereo systems, a photometric loss-based calibration method using Neural-SDF is proposed. Such an approach has not yet been proposed for active stereo systems. In the experiments, it was proved that our technique worked successfully by both qualitative and quantitative evaluations using real sensors and objects.",https://openaccess.thecvf.com/content/ACCV2024/html/Nishihara_Multiple_active_stereo_systems_calibration_method_based_on_Neural_SDF_ACCV_2024_paper.html
S2Net: Skeleton-aware SlowFast Network for Efficient Sign Language Recognition,"Continuous Sign Language Recognition (CSLR) aims to interpret meaning from signers' postures and movements. Joint-wise correspondences between estimated skeleton data and sign videos provide complementary insights into appearance and motion. In this paper, we propose a Skeleton-aware SlowFast Network(S^2Net) to effectively capture the appearance and motion information in sign videos. S^2Net leverages skeleton data in the fast pathway and video data in the slow pathway, progressively integrating both streams of information. Initially, we project both skeleton and video data into a unified graph-structured space and employ a consistent GCN-based architecture for both pathways, then we propose a group-wise cross-attention module to fuse intermediate features between different pathways. Finally, a frame-wise fusion pathway is adopted to integrate the semantic information at the sequence level. Experimental results on three public datasets demonstrate the effectiveness and efficiency of the proposed method.",https://openaccess.thecvf.com/content/ACCV2024/html/Yang_S2Net_Skeleton-aware_SlowFast_Network_for_Efficient_Sign_Language_Recognition_ACCV_2024_paper.html
Rethinking Sampling for Music-Driven Long-Term Dance Generation,"Generating dance sequences that synchronize with music while maintaining naturalness and realism is a challenging task. Existing methods often suffer from freezing phenomena or abrupt transitions. In this work, we introduce DanceFusion, a conditional diffusion model designed to address the complexities of creating long-term dance sequences. Our method employs a past and future-conditioned diffusion model, leveraging the attention mechanism to learn the dependencies among music, past, and future motions. We also propose a novel sampling method that completes the transitional motions between two dance segments by treating previous and upcoming motions as conditions. Additionally, we address abruptness in dance sequences by incorporating inpainting strategies into a part of the sampling process, thereby improving the smoothness and naturalness of motion generation. Experimental results demonstrate that DanceFusion outperforms state-of-the-art methods in generating high-quality and diverse dance motions. User study results further validate the effectiveness of our approach in generating long dance sequences, with participants consistently rating DanceFusion higher across all key metrics. Code and model are available at https://github.com/trgvy23/DanceFusion.",https://openaccess.thecvf.com/content/ACCV2024/html/Truong-Thuy_Rethinking_Sampling_for_Music-Driven_Long-Term_Dance_Generation_ACCV_2024_paper.html
Continual Learning Improves Zero-Shot Action Recognition,"Zero-shot learning in action recognition requires a strong ability to generalize from pre-training and seen classes to novel unseen classes. Similarly, the area of continual learning addresses the problem of catastrophic forgetting and also aims to create models with generalization power that can learn new tasks without forgetting previous ones. While these two areas completely aligned goals their technologies have never been combined. In this paper we propose a novel generative model which acts as glue to build on two stepping stones: a feature generation network for zero-shot learning and memory replay for continual learning. This model, which we call   Generative Iterative Learning  (GIL) creates a memory of synthesized features of past classes as well as real novel ones. This memory is used to retrain the classification model, ensuring a balanced exposure to the old and the new. Experiments reveal that   GIL  alleviates catastrophic forgetting and improves generalization in unseen classes, which improves zero-shot recognition across multiple benchmarks and settings.",https://openaccess.thecvf.com/content/ACCV2024/html/Gowda_Continual_Learning_Improves_Zero-Shot_Action_Recognition_ACCV_2024_paper.html
Semantic Visual-inertial SLAM for Automated Valet Parking,"Enhancing localization and mapping accuracy in constrained environments like parking lots is critical for autonomous driving. This paper introduces a novel visual-inertial Simultaneous Localization and Mapping (SLAM) approach tailored for automated valet parking (AVP). By incorporating semantic information such as various objects and markings found in parking lots, our method significantly enhances the robustness and precision of the localization process. These semantic features provide essential information for the automated parking system to understand the structure and rules of the parking environment, enabling more accurate navigation and decision-making. We developed a hybrid algorithm that integrates traditional key-point feature-based localization with semantic feature-based localization. The  evaluations conducted in the CARLA simulator demonstrated a 54% reduction in position error compared to state-of-the-art methods, achieving an average trajectory error of 0.19 meters. These advancements are vital for improving AVP system and facilitating the broader adoption of autonomous parking solutions. Future research will focus on scaling the approach to various urban environments and addressing challenges presented by dynamic conditions.",https://openaccess.thecvf.com/content/ACCV2024/html/Oh_Semantic_Visual-inertial_SLAM_for_Automated_Valet_Parking_ACCV_2024_paper.html
Content-Adaptive Style Transfer:  A Training-Free Approach with VQ Autoencoders,"We introduce Content-Adaptive Style Transfer (CAST), a novel training-free approach for arbitrary style transfer that enhances visual fidelity using vector quantized-based pretrained autoencoder. Our method systematically applies coherent stylization to corresponding content regions. It starts by capturing the global structure of images through vector quantization, then refines local details using our style-injected decoder. CAST consists of three main components: a content-consistent style injection module, which tailors stylization to unique image regions; an adaptive style refinement module, which fine-tunes stylization intensity; and a content refinement module, which ensures content integrity through interpolation and feature distribution maintenance. Experimental results indicate that CAST outperforms existing generative-based and traditional style transfer models in both quantitative and qualitative measures.",https://openaccess.thecvf.com/content/ACCV2024/html/Gim_Content-Adaptive_Style_Transfer__A_Training-Free_Approach_with_VQ_Autoencoders_ACCV_2024_paper.html
TGCM: Cross-Domain Few-Shot Semantic Segmentation via one-shot Target Guided CutMix,"The goal of few-shot semantic segmentation is to build a model using a small amount of annotated data to generalize to a new object class. When there are significant differences between the target domain and the source domain, segmentation performance usually deteriorates significantly. Existing cross-domain few-shot image segmentation methods mainly rely on annotated data from the source domain to achieve domain-independent feature extraction through feature transformation. However, bridging the huge domain gap without guidance from the target domain data remains highly challenging.  In order to effectively address the domain shift problem in cross-domain few-shot semantic segmentation (CD-FSS), this paper introduces a new method called Target-guided Cross-domain Few-shot Semantic Segmentation (TGCM), which introduces a labeled sample from the target domain to guide model learning during training. Speciffcally, TGCM utilizes one-shot image and mask from the target domain as auxiliary data, employs the CutMix method for data augmentation on the source domain training data. Subsequently, the task-adaptive feature transformer module (TAFT) and domain channel alignment (DCA) module are introduced to translate the features of fused images into the feature space related to the target domain, reducing the domain drift effect caused by cross-domain discrepancies. Finally,  we present a Dynamic Prediction (DP) strategy to help the model gradually improve segmentation performance. Experimental results show that our model achieves significant improvement in CD-FSS, with average accuracies higher by 4.71% and 2.95% on 1-shot and 5-shot, respectively, compared to the baseline methods in CD-FSS. Code will be made available.",https://openaccess.thecvf.com/content/ACCV2024/html/Wei_TGCM_Cross-Domain_Few-Shot_Semantic_Segmentation_via_one-shot_Target_Guided_CutMix_ACCV_2024_paper.html
Latency Attack Resilience in Object Detectors: Insights from Computing Architecture,"Image-based object detectors are increasingly being used in surveillance and autonomous driving systems in real-time. However, those systems are threatened by latency attacks which inflate the elapsed time of each query, such that the system cannot respond properly within a reasonable time interval. In this paper, we find the root cause of the vulnerability of latency attacks on object detectors is caused by the occurrences of minor page faults. We propose a decision algorithm to mitigate this problem. The decision algorithm can automatically decide the optimal implementation to be executed based on the current status of the target system. To the best of our knowledge, this is the first paper to solve the latency issue from the point of view of computing architecture. Our studies provide a useful guideline for designing real-time applications on edge devices with more efficient and resilient responses.",https://openaccess.thecvf.com/content/ACCV2024/html/Chen_Latency_Attack_Resilience_in_Object_Detectors_Insights_from_Computing_Architecture_ACCV_2024_paper.html
Underground Mapping and Localization Based on Ground-Penetrating Radar,"3D object reconstruction based on deep neural networks has gained increasing attention in recent years. However, 3D reconstruction of underground objects to generate point cloud maps remains a challenge. Ground Penetrating Radar (GPR) is one of the most powerful and extensively used tools for detecting and locating underground objects such as plant root systems and pipelines, with its cost-effectiveness and continuously evolving technology. This paper introduces a parabolic signal detection network based on deep convolutional neural networks, utilizing B-scan images from GPR sensors. The detected keypoints can aid in accurately fitting parabolic curves used to interpret the original GPR B-scan images as cross-sections of the object model. Additionally, a multi-task point cloud network was designed to perform both point cloud segmentation and completion simultaneously, filling in sparse point cloud maps. For unknown locations, GPR A-scan data can be used to match corresponding A-scan data in the constructed map, pinpointing the position to verify the accuracy of the map construction by the model. Experimental results demonstrate the effectiveness of our method.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_Underground_Mapping_and_Localization_Based_on_Ground-Penetrating_Radar_ACCV_2024_paper.html
Learning 3D Point Cloud Registration as a Single Optimization Problem,"We present a simple yet effective technique to boost the performance of 3D point cloud registration. It only requires replacing the conventional hand-crafted differentiable matching module with an uncertainty-aware one. Conventional methods pass a distance (or similarity) matrix to the matching module as a deterministic input, ignoring any uncertainty in upstream distance calculation. In other words, existing works always consider the optimality of the feature extractor and matching module separately, which is sub-optimal. We propose to use a trainable matching network as the uncertainty-aware matching module and connect it with a feature extractor in a non-deterministic way. The matching network is trained with the feature extractor as a single probabilistic process in this way, yielding a joint-optimal solution. Experimental results have demonstrated that our strategy significantly boosts the performance of recent SOTA methods under versatile conditions, including rigid/non-rigid and whole/partial point cloud registration datasets.",https://openaccess.thecvf.com/content/ACCV2024/html/Yanagi_Learning_3D_Point_Cloud_Registration_as_a_Single_Optimization_Problem_ACCV_2024_paper.html
Amodal Instance Segmentation with Diffusion Shape Prior Estimation,"Amodal Instance Segmentation (AIS) presents an intriguing challenge, including the segmentation prediction of both visible and occluded parts of objects within images. Previous methods have often relied on shape prior information gleaned from training data to enhance amodal segmentation. However, these approaches are susceptible to overfitting and  disregard object category details. Recent advancements highlight the potential of conditioned diffusion models, pretrained on extensive datasets, to generate images from latent space. Drawing inspiration from this, we propose AISDiff with a Diffusion Shape Prior Estimation (DiffSP) module. AISDiff begins with the prediction of the visible segmentation mask and object category, alongside occlusion-aware processing through the prediction of occluding masks. Subsequently, these elements are inputted into our DiffSP module to infer the shape prior of the object. DiffSP utilizes conditioned diffusion models pretrained on extensive datasets to extract rich visual features for shape prior estimation. Additionally, we introduce the Shape Prior Amodal Predictor, which utilizes attention-based feature maps from the shape prior to refine amodal segmentation. Experiments across various AIS benchmarks demonstrate the effectiveness of our AISDiff.",https://openaccess.thecvf.com/content/ACCV2024/html/Tran_Amodal_Instance_Segmentation_with_Diffusion_Shape_Prior_Estimation_ACCV_2024_paper.html
Polyp-SES: Automatic Polyp Segmentation with Self-Enriched Semantic Model,"Automatic polyp segmentation is crucial for effective diagnosis and treatment in colonoscopy images. Traditional methods encounter significant challenges in accurately delineating polyps due to limitations in feature representation and the handling of variability in polyp appearance. Deep learning techniques, including CNN and Transformer-based methods, have been explored to improve polyp segmentation accuracy. However, existing approaches often neglect additional semantics, restricting their ability to acquire adequate contexts of polyps in colonoscopy images. In this paper, we propose an innovative method named ""Automatic Polyp Segmentation with Self-Enriched Semantic Model"" to address these limitations. First, we extract a sequence of features from an input image and decode high-level features to generate an initial segmentation mask. Using the proposed self-enriched semantic module, we query potential semantics and augment deep features with additional semantics, thereby aiding the model in understanding context more effectively. Extensive experiments show superior segmentation performance of the proposed method against state-of-the-art polyp segmentation baselines across five polyp benchmarks in both superior learning and generalization capabilities.",https://openaccess.thecvf.com/content/ACCV2024/html/Nguyen_Polyp-SES_Automatic_Polyp_Segmentation_with_Self-Enriched_Semantic_Model_ACCV_2024_paper.html
OneDiff: A Generalist Model for Image Difference Captioning,"In computer vision, Image Difference Captioning (IDC) is crucial for accurately describing variations between closely related images. Traditional IDC methods often rely on specialist models, which restrict their applicability across varied contexts. This paper introduces the OneDiff model, a novel generalist approach that utilizes a robust vision-language model architecture, integrating a siamese image encoder with a Visual Delta Module. This innovative configuration allows for the precise detection and articulation of fine-grained differences between image pairs. OneDiff is trained through a dual-phase strategy, encompassing Coupled Sample Training and multi-task learning across a diverse array of data types, supported by our newly developed DiffCap Dataset. This dataset merges real-world and synthetic data, enhancing the training process and bolstering the models robustness. Extensive testing on diverse IDC benchmarks, such as Spot-the-Diff, Image-Editing-Request, and Birds-to-Words, shows that OneDiff consistently outperforms existing state-of-the-art models in accuracy and adaptability, achieving improvements of up to 97% CIDEr points in average. By setting a new benchmark in IDC, OneDiff paves the way for more versatile and effective applications in detecting and describing visual differences. The code, models, and data will be made publicly available.",https://openaccess.thecvf.com/content/ACCV2024/html/Hu_OneDiff_A_Generalist_Model_for_Image_Difference_Captioning_ACCV_2024_paper.html
MV2MP: Segmentation Free Performance Capture of Humans in Direct Physical Contact from Sparse Multi-Cam Setups,"This paper introduces a novel and robust approach for the performance capture of multiple humans engaged in direct physical interactions with very sparse RGB camera setups. Unlike existing methods that only perform well under specific conditionssuch as when humans are relatively distant from each other, when a scene is surrounded by a large array of cameras, or when precise segmentation is availableour method operates without any of these requirements. We introduce a novel layered network architecture to represent the foreground and background together, as well as a tailored compositional volumetric rendering technique and objective functions, along with a new sampling method. These innovations enable the accurate reconstruction of humans engaged in direct physical interactions using only images and roughly estimated SMPL models. Our work demonstrates that our method is able not only to extract high-quality geometry of interacting people but also to provide segmentation and free viewpoint video, outperforming competitors that work in similar setups. Also we show the ability to improve the quality of the roughly estimated smpl models. We have conducted experiments on a variety of scenes using the HI4D and CMU Panoptic datasets. The code and examples are available in https://github.com/mv2mp/MV2MP.",https://openaccess.thecvf.com/content/ACCV2024/html/Eliseev_MV2MP_Segmentation_Free_Performance_Capture_of_Humans_in_Direct_Physical_ACCV_2024_paper.html
Exploiting Cross-modal Cost Volume for Multi-sensor Depth Estimation,"Single-modal depth estimation has shown steady improvement over the years. However, relying solely on a single imaging sensor such as RGB and near-infrared (NIR) cameras can result in unreliable and erroneous depth estimation, particularly in challenging lighting conditions such as low-light or sudden lighting change scenarios. Thereby, several approaches have leveraged multiple sensors for robust depth estimation. However, the effective fusion method that maximally utilizes multi-modal sensor information still requires further investigation. With this in mind, we propose a multi-modal cost volume fusion strategy with cross-modal attention, incorporating information from both cross-spectral and single-modality pairs. Our method initially constructs low-level cost volumes that consist of modality-specific (i.e., single modality) and modality-invariant (i.e., cross-spectral) volumes from multi-modal sensors. These cost volumes are then gradually fused using bidirectional cross-modal fusion and unidirectional LiDAR fusion to generate a multi-sensory cost volume. Furthermore, we introduce a straightforward domain gap reduction approach to learn modality-invariant features and depth refinement techniques through cost volume-guided propagation. Experimental results demonstrate that our method achieves SOTA (State-of-the-Art) performance under diverse environmental changes.",https://openaccess.thecvf.com/content/ACCV2024/html/Kim_Exploiting_Cross-modal_Cost_Volume_for_Multi-sensor_Depth_Estimation_ACCV_2024_paper.html
Parameter-Efficient Instance-Adaptive Neural Video Compression,"Learning-based Neural Video Codecs (NVCs) have emerged as a compelling alternative to standard video codecs, demonstrating promising performance, and simple and easily maintainable pipelines. However, NVCs often fall short of compression performance and occasionally exhibit poor generalization capability due to inference-only compression scheme and their dependence on training data. The instance-adaptive video compression techniques have recently been suggested as a viable solution, fine-tuning the encoder or decoder networks for a particular test instance video. However, fine-tuning all the model parameters incurs high computational costs, increases the bitrates, and often leads to unstable training. In this work, we propose a parameter-efficient instance-adaptive video compression framework. Inspired by the remarkable success of parameter-efficient fine-tuning on large-scale neural network models, we propose to use a lightweight adapter module that can be easily attached to the pretrained NVCs and fine-tuned for test video sequences. The resulting algorithm significantly improves compression performance and reduces the encoding time compared to the existing instant-adaptive video compression algorithms. Furthermore, the suggested fine-tuning method enhances the robustness of the training process, allowing for the proposed method to be widely used in many practical settings. We conducted extensive experiments on various standard benchmark datasets, including UVG, MCL-JVC, and HEVC sequences, and the experimental results have shown a significant improvement in rate-distortion (RD) curves (up to 5 dB PSNR improvements) and BD rates compared to the baselines NVC.",https://openaccess.thecvf.com/content/ACCV2024/html/Oh_Parameter-Efficient_Instance-Adaptive_Neural_Video_Compression_ACCV_2024_paper.html
Improving Image Clustering with Artifacts Attenuation via Inference-Time Attention Engineering,"The goal of this paper is to improve the performance of pretrained Vision Transformer (ViT) models, particularly DINOv2, in image clustering task without requiring re-training or fine-tuning. As model size increases, high-norm artifacts anomaly appears in the patches of multi-head attention. We observe that this anomaly leads to reduced accuracy in zero-shot image clustering. These artifacts are characterized by disproportionately large values in the attention map compared to other patch tokens. To address these artifacts, we propose an approach called Inference-Time Attention Engineering (ITAE), which manipulates attention function during inference. Specifically, we identify the artifacts by investigating one of the Query-Key-Value (QKV) patches in the multi-head attention and attenuate their corresponding attention values inside the pretrained models. ITAE shows improved clustering accuracy on multiple datasets by exhibiting more expressive features in latent space. Our findings highlight the potential of ITAE as a practical solution for reducing artifacts in pretrained ViT models and improving model performance in clustering tasks without the need for re-training or fine-tuning.",https://openaccess.thecvf.com/content/ACCV2024/html/Nakamura_Improving_Image_Clustering_with_Artifacts_Attenuation_via_Inference-Time_Attention_Engineering_ACCV_2024_paper.html
Enhanced Kalman with Adaptive Appearance Motion SORT for Grounded Generic Multiple Object Tracking,"Despite recent progress, Multi-Object Tracking (MOT) continues to face significant challenges, particularly its dependence on prior knowledge and predefined categories, complicating the tracking of unfamiliar objects. Generic Multiple Object Tracking (GMOT) emerges as a promising solution, requiring less prior information. Nevertheless, existing GMOT methods, primarily designed as OneShot-GMOT, rely heavily on initial bounding boxes and often struggle with variations in viewpoint, lighting, occlusion, and scale. To overcome the limitations inherent in both MOT and GMOT when it comes to tracking objects with specific generic attributes, we introduce Grounded-GMOT, an innovative tracking paradigm that enables users to track multiple generic objects in videos through natural language descriptors. Our contributions begin with the introduction of the G2MOT dataset, which includes a collection of videos featuring a wide variety of generic objects, each accompanied by detailed textual descriptions of their attributes. Following this, we propose a novel tracking method, KAM-SORT, which not only effectively integrates visual appearance with motion cues but also enhances the Kalman filter. KAM-SORT proves particularly advantageous when dealing with objects of high visual similarity from the same generic category in GMOT scenarios. Through comprehensive experiments, we demonstrate that Grounded-GMOT outperforms existing OneShot-GMOT approaches. Additionally, our extensive comparisons between various trackers highlight KAM-SORT's efficacy in GMOT, further establishing its significance in the field. The source code and dataset will be made publicly available.",https://openaccess.thecvf.com/content/ACCV2024/html/Le_Dinh_Anh_Enhanced_Kalman_with_Adaptive_Appearance_Motion_SORT_for_Grounded_Generic_ACCV_2024_paper.html
LoCo-MAD: Long-Range Context-Enhanced Model Towards Plot-Centric Movie Audio Description,"Movie Audio Description (MAD) aims to enable the visually impaired community to enjoy movies by transforming them into coherent and accurate audio descriptions. Due to the extended duration and complex plot natures of movies, MAD is in the early stages of research compared to other cross-modal text generation tasks. Current MAD methods fail to model long videos efficiently or integrate long-range context to generate plot-coherent descriptions. To address these challenges, we propose a Long-Range Context-Enhanced Movie Audio Description model (LoCo-MAD), which is trained in two stages. The first stage adapts an image-text pretrained model to a Pre-aligned Movie Encoder (PME), which utilizes learnable queries to obtain compact visual representations and is supervised by three multimodal objectives. The second stage builds LoCo-MAD with the pretrained PME, a Dynamic Selection Module (DSM), and a large language model. We project visual representations from PME into soft visual prompts and utilize DSM to select the most relevant descriptions and subtitles from a long range as contextual prompts. Then, a large language model integrates these multimodal prompts and generates plot-related movie descriptions. The proposed method is extensively evaluated on MAD-v2 and LSMDC datasets, where we achieve 23.7 and 20.0 CIDEr score, respectively. Our code will be released at https://github.com/blindwang/LoCo-MAD.",https://openaccess.thecvf.com/content/ACCV2024/html/Wang_LoCo-MAD_Long-Range_Context-Enhanced_Model_Towards_Plot-Centric_Movie_Audio_Description_ACCV_2024_paper.html
Dual-path Multimodal Optimal Transport for Composed Image Retrieval,"Unlike cross-modal retrieval tasks like text-to-image and image-to-text, which focus on one-way feature alignment, composed image retrieval emphasizes bidirectional feature alignment to differentiate between features that need to be preserved or modified. Existing methods usually map text and image modal data directly into a shared space for fusion, overlooking the issue of mismatched feature distributions between the source and target domains, resulting in limited retrieval performance. This paper presents the Dual-path Multimodal Optimal Transport (DMOT) model for composed image retrieval. It aligns features independently from both the text-to-image and image-to-text paths. During the fusion process, it explicitly calculates the preserved and modified features. Specifically, the pre-trained vision-language model BLIP is used to extract deep semantic features of both images and text. Then, we utilize two optimal transport modules to iteratively optimize and solve mapping matrices, aligning reference image features and modified text features in their respective spaces. Finally, considering the characteristics of the composed image retrieval task, we design the feature modifier module and the feature preserver module to handle the fusion of multimodal features. Extensive experiments on two public datasets, FashionIQ and CIRR, demonstrate DMOTs superior performance over state-of-the-art methods in retrieval accuracy, achieving an average improvement of 6.81% and 16.09%, respectively. The source code is available at https://github.com/AnoAuth/DMOT",https://openaccess.thecvf.com/content/ACCV2024/html/Yan_Dual-path_Multimodal_Optimal_Transport_for_Composed_Image_Retrieval_ACCV_2024_paper.html
EDeRF: Updating Local Scenes and Editing Across Fields for Real-Time Dynamic Reconstruction of Road Scene,"Neural Radiance Field(NeRF) offers high reconstruction precision but is time-consuming when training dynamic scenes. Editable NeRF achieves dynamic by editing static scenes, which reduces retraining and has been successfully applied in autonomous driving simulations. However, real-time dynamic road scene reconstruction is challenging due to the high dynamics of real scenes, the lack of depth cameras, and the difficulty in obtaining precise vehicle pose. The primary challenges include the fast and accurate reconstruction of new vehicles entering the scene and their trajectories. We propose EDeRF, a method for real-time dynamic road scene reconstruction from fixed cameras such as traffic surveillance through collaboration of sub-NeRFs and cross-field editing. We decompose the scene space and select key areas to update new vehicles by sharing parameters and local training with sub-fields. These vehicles are then integrated into the complete scene and achieve dynamic motion by warping the sampling rays across different fields, where vehicles' six degrees of freedom(6-DOF) is estimated based on inter-frame displacement and rigid body contact constraints. We have built simulated traffic monitoring scenarios with toll booths in real world and conducted experiments to demonstrate the effectiveness of our method. The results show that EDeRF achieves remarkable results and outperforms comparative methods in terms of real-time capability and accuracy in reconstructing the appearance and movement of newly entered vehicles.",https://openaccess.thecvf.com/content/ACCV2024/html/Liang_EDeRF_Updating_Local_Scenes_and_Editing_Across_Fields_for_Real-Time_ACCV_2024_paper.html
Accelerated Deep Nonlinear Dictionary Learning,"Most of the existing dictionary learning models are based on linearly learned dictionaries, which have weak performance in nonlinear signal representation, thus driving a research boom in nonlinear dictionary learning (NLDL). In this paper, we propose a deep nonlinear dictionary learning model for dictionaries and coefficients with full-layer sparse regularization. It possesses the capability to acquire deep latent information and applying l_1 regularization enhances the efficiency of hierarchically extracting key features. Initially, we investigate the proposed algorithm using the proximal operator machine, followed by the introduction of Nesterov acceleration to expedite convergence, termed as \text Accelerated DNLDL _l_1. We validate the feasibility of the proposed algorithm through numerical experiments, demonstrating that the acceleration scheme enhances the algorithm's performance. We applied our proposed algorithm to practical image classification and denoising tasks to demonstrate its generality across various nonlinear functions. Additionally, experimental results show that applying regularization to both the dictionaries and coefficients simultaneously facilitates parameter tuning and yields superior denoising performance.",https://openaccess.thecvf.com/content/ACCV2024/html/Tan_Accelerated_Deep_Nonlinear_Dictionary_Learning_ACCV_2024_paper.html
PrimeDepth: Efficient Monocular Depth Estimation with a Stable Diffusion Preimage,"This work addresses the task of zero-shot monocular depth estimation. A recent advance in this field has been the idea of utilising Text-to-Image foundation models, such as Stable Diffusion. Foundation models provide a rich and generic image representation, and therefore, little training data is required to reformulate them as a depth estimation model that predicts highly-detailed depth maps and has good generalisation capabilities. However, the realisation of this idea has so far led to approaches which are, unfortunately, highly inefficient at test-time due to the underlying iterative denoising process. In this work, we propose a different realisation of this idea and present PrimeDepth, a method that is highly efficient at test time while keeping, or even enhancing, the positive aspects of diffusion-based approaches. Our key idea is to extract from Stable Diffusion a rich, but frozen, image representation by running a single denoising step. This representation, we term preimage, is then fed into a refiner network with an architectural inductive bias, before entering the downstream task. We validate experimentally that PrimeDepth is two orders of magnitude faster than the leading diffusion-based method, Marigold, while being more robust for challenging scenarios and quantitatively marginally superior. Thereby, we reduce the gap to the currently leading data-driven approach, Depth Anything, which is still quantitatively superior, but predicts less detailed depth maps and requires 20 times more labelled data. Due to the complementary nature of our approach, even a simple averaging between PrimeDepth and Depth Anything predictions can improve upon both methods and sets a new state-of-the-art in zero-shot monocular depth estimation. In future, data-driven approaches may also benefit from integrating our preimage.",https://openaccess.thecvf.com/content/ACCV2024/html/Zavadski_PrimeDepth_Efficient_Monocular_Depth_Estimation_with_a_Stable_Diffusion_Preimage_ACCV_2024_paper.html
GPNF:A Point Cloud Registration Framework Using Sharp Global Linear Attention Prior and Neighborhood Filtering Strategy,"Robust point features are essential when registering point cloud scenes with numerous instances. To enhance the point features, we propose KPConvFormer module. It leverages the advantages of attention mechanisms to focus on important features, considers the feature and position differences among points in point convolution simultaneously, and pre-weights the neighborhood points in the convolution region. The pre-weight process filters out irrelevant points from other instances near the boundaries and noisy points within the convolution region, correcting the point convolution factors in each neighborhood to help aggregate more accurate point features. Addressing the incorrect registration caused by the similar structure of point clouds, we designed a Shareped-Linear-Self-Attention module. It learns a sharp global prior, efficiently capturing fine-grained global structural information. This module distinguishes similarity structures in the point clouds to be registered from a larger receptive field, providing a global prior for subsequent convolution operations. Compared to existing state-of-the-art methods, our approach achieves superior performance on most registration metrics across the 3DMatch, 3DLoMatch, and KITTI datasets.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhu_GPNFA_Point_Cloud_Registration_Framework_Using_Sharp_Global_Linear_Attention_ACCV_2024_paper.html
Enhancing Anchor-based Weakly Supervised Referring Expression Comprehension with Cross-Modality Attention,"Weakly supervised Referring Expression Comprehension (REC) tackles the challenge of identifying specific regions in an image based on textual descriptions without predefined mappings between the text and target objects during training. The primary obstacle lies in the misalignment between visual and textual features, often resulting in inaccurate bounding box predictions. To address this, we propose a novel cross-modality attention module (CMA) module that enhances the discriminative power of grid features and improves localization accuracy by harmonizing textual and visual features. To handle the noise from incorrect labels common in weak supervision, we also introduce a false negative suppression mechanism that uses intra-modal similarities as soft supervision signals. Extensive experiments conducted on four REC benchmark datasets: RefCOCO, RefCOCO+, RefCOCOg, and ReferItGame. Our results show that our model consistently outperforms state-of-the-art methods in accuracy and generalizability.",https://openaccess.thecvf.com/content/ACCV2024/html/Chu_Enhancing_Anchor-based_Weakly_Supervised_Referring_Expression_Comprehension_with_Cross-Modality_Attention_ACCV_2024_paper.html
Deformable Shape-aware Point Generation for 3D Object Detection,"As a fundamental task of the perception system for autonomous driving, LiDAR-based 3D object detection often suffers from the absence of incomplete object shapes under long distances and occlusion. 3D object detectors, based on point cloud completion methods, significantly improve detection performance by generating pseudo-point clouds. However, due to the scarcity of guidance provided by the geometric shape and orientation information of objects, it is more challenging to recover the accurate surface shape of objects. Motivated by this, we propose Deformable Shape-aware Point Generation(DSaPG) for 3D object detection. Specifically, we design a Geometry-guided Region Proposal Network (GgRPN) including a heatmap-guided shape branch and an orientation alignment supervision, which contributes to the generation of high-quality proposals. Moreover, we design a Density-aware Deformable Point Generation(DDPG) module, which generates points by encoding probability density at each grid ball query and deformation learning to accurately recover the shape of objects. The deformable-augmented generated points can more effectively represent the shape features of objects for 3D bounding box predictions. Experiments demonstrate that the proposed DSaPG achieves competitive performance on the KITTI dataset and the Waymo Open Dataset. The code will be available at https://github.com/Wkk121/DSaPG.",https://openaccess.thecvf.com/content/ACCV2024/html/Wang_Deformable_Shape-aware_Point_Generation_for_3D_Object_Detection_ACCV_2024_paper.html
VideoPatchCore: An Effective Method to Memorize Normality for Video Anomaly Detection,"Video anomaly detection (VAD) is a crucial task in video analysis and surveillance within computer vision. Currently, VAD is gaining attention with memory techniques that store the features of normal frames. The stored features are utilized for frame reconstruction, identifying an abnormality when a significant difference exists between the reconstructed and input frames. However, this approach faces several challenges due to the simultaneous optimization required for both the memory and encoder-decoder model. These challenges include increased optimization difficulty, complexity of implementation, and performance variability depending on the memory size. To address these challenges, we propose an effective memory method for VAD, called VideoPatchCore. Inspired by PatchCore, our approach introduces a structure that prioritizes memory optimization and configures three types of memory tailored to the characteristics of video data. This method effectively addresses the limitations of existing memory-based methods, achieving good performance comparable to state-of-the-art methods. Furthermore, our method requires no training and is straightforward to implement, making VAD  tasks more accessible. The code will be made publicly available upon the publication of this study.",https://openaccess.thecvf.com/content/ACCV2024/html/Ahn_VideoPatchCore_An_Effective_Method_to_Memorize_Normality_for_Video_Anomaly_ACCV_2024_paper.html
CNG-SFDA: Clean-and-Noisy Region Guided Online-Offline Source-Free Domain Adaptation,"Domain shift occurs when training (source) and test (target) data diverge in their distribution. Source-Free Domain Adaptation (SFDA) addresses this domain shift problem, aiming to adopt a trained model on the source domain to the target domain in a scenario where only a well-trained source model and unlabeled target data are available. In this scenario, handling false labels in the target domain is crucial because they negatively impact the model performance.  To deal with this problem, we propose to update cluster prototypes (i.e., centroid of each sample cluster) and their structure in the target domain formulated by the source model in online manners. In the feature space, samples in different regions have different pseudo-label distribution characteristics affected by the cluster prototypes, and we adopt distinct training strategies for these samples by defining clean and noisy regions: we selectively train the target with clean pseudo-labels in the clean region, whereas we introduce mix-up inputs representing intermediate features between clean and noisy regions to increase the compactness of the cluster. We conducted extensive experiments on multiple datasets in online/offline SFDA settings, whose results demonstrate that our method, CNG-SFDA, achieves state-of-the-art for most cases. Code is available at https://github.com/hyeonwoocho7/CNG-SFDA.",https://openaccess.thecvf.com/content/ACCV2024/html/Cho_CNG-SFDA_Clean-and-Noisy_Region_Guided_Online-Offline_Source-Free_Domain_Adaptation_ACCV_2024_paper.html
HT-SSPG:Hierarchical Transformers for Semantic Surface Point Generation in 3D Object Detection,"Currently, the incomplete point cloud structure in LiDAR point clouds has become the primary challenge for improving detector performance. Point cloud completion methods address this issue by adding more points to regions of interest, however, due to imprecise proposals and coarse feature extraction methods, these approaches often generate numerous low-quality points, which limits detection performance. To tackle this issue, we propose a hierarchical transformers for semantic surface point generation in 3D object detection (HT-SSPG), leveraging a voxel supervised network (VSN) and a hierarchical attention refinement (HAR) network to generate high-quality proposals and complete semantic surface points for precise detection. Specifically, the VSN enhances the backbone network's perception of spatial structures using 3D heatmaps, capturing complete structural and positional information of missing objects. The HAR module effectively integrates voxel and point cloud features using cross-attention transformers to accurately estimate the complete shape and position of objects, thus generating high-quality semantic surface points for precise detection. Extensive experiments demonstrate that our HT-SSPG achieves leading performance on the KITTI dataset. Compared to PG-RCNN, our method significantly improves detection accuracy for small objects such as pedestrians and cyclists. Specifically, it outperforms in pedestrian detection by 8.46% AP and 8.08% AP at moderate and hard levels, respectively.",https://openaccess.thecvf.com/content/ACCV2024/html/Kong_HT-SSPGHierarchical_Transformers_for_Semantic_Surface_Point_Generation_in_3D_Object_ACCV_2024_paper.html
O1O: Grouping of Known Classes to Identify Unknown Objects as Odd-One-Out,"Object detection methods trained on a fixed set of known classes struggle to detect objects of unknown classes in the open-world setting. Current fixes involve adding approximate supervision with pseudo-labels corresponding to candidate locations of objects, typically obtained in a class-agnostic manner. While previous approaches mainly rely on the appearance of objects, we find that geometric cues improve unknown recall. Although additional supervision from pseudo-labels helps to detect unknown objects, it also introduces confusion for known classes. We observed a notable decline in the model's performance for detecting known objects in the presence of noisy pseudo-labels. Drawing inspiration from studies on human cognition, we propose to group known classes into superclasses. By identifying similarities between classes within a superclass, we can identify unknown classes through an odd-one-out scoring mechanism. Our experiments on open-world detection benchmarks demonstrate significant improvements in unknown recall, consistently across all tasks. Crucially, we achieve this without compromising known performance, thanks to better partitioning of the feature space with superclasses. Project page: https://kuis-ai.github.io/O1O",https://openaccess.thecvf.com/content/ACCV2024/html/Yavuz_O1O_Grouping_of_Known_Classes_to_Identify_Unknown_Objects_as_ACCV_2024_paper.html
Real-SRGD: Enhancing Real-World Image Super-Resolution with Classifier-Free Guided Diffusion,"Real-world image super-resolution (RISR) aims to reconstruct high-resolution (HR) images from degraded low-resolution (LR) inputs, addressing challenges such as blurring, noise, and compression artifacts. Unlike conventional super-resolution (SR) approaches that typically generate LR images through synthetic downsampling, RISR confronts the complexity of real-world degradation. To effectively handle the intricate challenges of RISR, we adapt classifier-free guidance (CFG), a technique initially developed for multi-class image generation. Our proposed method, Real-SRGD (Real-world image Super-Resolution with classifier-free Guided Diffusion), decomposes RISR challenges into three distinct sub-tasks: Blind image restoration (BIR), conventional SR, and RISR itself. We then train class-conditional SR diffusion models tailored to these sub-tasks and use CFG to enhance the super-resolution performance in real-world settings. Our empirical results demonstrate that Real-SRGD surpasses existing state-of-the-art methods in both quantitative metrics and qualitative evaluations, as demonstrated by user studies. Moreover, our method demonstrates exceptional generalizability across a range of conventional SR benchmark datasets. The code can be found at https://github.com/yahoojapan/srgd.",https://openaccess.thecvf.com/content/ACCV2024/html/Doi_Real-SRGD_Enhancing_Real-World_Image_Super-Resolution_with_Classifier-Free_Guided_Diffusion_ACCV_2024_paper.html
HDNeXt: Hybrid Dynamic MedNeXt with Level Set Regularization for Medical Image Segmentation,"Deep learning has been extensively employed in the field of medical image segmentation, demonstrating its robustness and efficacy. However, the pursuit of consistent segmentation performance across diverse instrumental conditions and the challenge of achieving precise boundary delineation in segmented images remain significant hurdles. In this paper, we aim to develop a model capable of achieving consistent, high-quality segmentation of identical regions of interest across varying instrumental conditions, with precise boundary delineation. Toward this end, we introduce our Hybrid Dynamic MedNeXt (HDNeXt) model, an advanced framework capable of dynamically generating weights across diverse medical images to maintain consistently high segmentation performance. HDNeXt builds on the robust segmentation framework of MedNeXt by incorporating dynamic convolution techniques, which endow the model with the capability for dynamic weight adjustment, significantly enhancing its segmentation performance. To tackle the second challenge, we devised a novel loss function, L_ CR , formulated on the Curvature of the segmentation boundary and Region-Fitting energy derived from level set methods, which significantly enhances boundary precision during training and optimizes overall segmentation performance. Experiments were conducted on the abdominal CT datasets Synapse and the cardiac MRI datasets ACDC to demonstrate the efficiency and effectiveness of our method. Our method achieved an average Dice coefficient of 84.38 on the Synapse datasets and 93.59 on the ACDC datasets, surpassing other 2D state-of-the-art segmentation models and achieving optimal performance for 2D medical image segmentation. Codes are available at https://github.com/HaoyuCao/HDNeXt",https://openaccess.thecvf.com/content/ACCV2024/html/Cao_HDNeXt_Hybrid_Dynamic_MedNeXt_with_Level_Set_Regularization_for_Medical_ACCV_2024_paper.html
VIP: Versatile Image Outpainting Empowered by Multimodal Large Language Model,"In this paper, we focus on resolving the problem of image outpainting, which aims to extrapolate the surrounding parts given the center contents of an image. Although recent works have achieved promising performance, the lack of versatility and customization hinders their practical applications in broader scenarios. Therefore, this work presents a novel image outpainting framework that is capable of customizing the results according to the requirements of users. First of all, we take advantage of a Multimodal Large Language Model (MLLM) that automatically extracts and organizes the corresponding textual descriptions of the masked and unmasked part of a given image. Accordingly, the obtained text prompts are introduced to endow our model with the capacity to customize the outpainting results. In addition, a special Center-Total-Surrounding (C-T-S) decoupled control mechanism is elaborately designed to boost text-driven generation by enhancing the interaction between specific spatial regions of the image and corresponding parts of the text prompts. Note that unlike most existing methods, our approach is very resource-efficient since it is just slightly fine-tuned on the off-the-shelf stable diffusion (SD) model rather than being trained from scratch. Finally, the experimental results on three commonly used datasets, i.e. Scenery, Building, and WikiArt, demonstrate our model significantly surpasses the SoTA methods. Moreover, versatile outpainting results are listed to show its customized ability.",https://openaccess.thecvf.com/content/ACCV2024/html/Yang_VIP_Versatile_Image_Outpainting_Empowered_by_Multimodal_Large_Language_Model_ACCV_2024_paper.html
Capture Concept through Comparison: Vision-and-Language Representation Learning with Intrinsic Information Mining,"Achieving alignment between vision and language semantics poses a critical challenge. Prior works have sought to enhance alignment by incorporating additional supervision, such as tags or object bounding boxes, as anchors between modalities. However, these methods predominantly concentrate on aligning tangible entities, disregarding other crucial abstract concepts that elude perception, such as side by side. To overcome this limitation, we propose a novel approach to Capture various Concepts through data Comparison (C3) for learning cross-modal representations. Specifically, we devise a data mining procedure to uncover intrinsic information within the database, avoiding the need for external annotations. Furthermore, we distinctly frame model inputs as triplets to better elucidate abstract semantics in images. Building upon this formulation, we propose two concept-centric pre-training objectives to signify concept learning. Extensive experiments show that models trained within the C3 framework consistently achieve significant enhancements across a wide range of comprehension and reasoning benchmarks, whether starting from scratch or fine-tuning from an existing model.",https://openaccess.thecvf.com/content/ACCV2024/html/Song_Capture_Concept_through_Comparison_Vision-and-Language_Representation_Learning_with_Intrinsic_Information_ACCV_2024_paper.html
Image Deraining with Frequency-Enhanced State Space Model,"Removing rain degradations in images is recognized as a significant issue. In this field, deep learning-based approaches, such as Convolutional Neural Networks (CNNs) and Transformers, have succeeded. Recently, State Space Models (SSMs) have exhibited superior performance across various tasks in both natural language processing and image processing due to their ability to model long-range dependencies. This study introduces SSM to image deraining and proposes a Deraining Frequency-Enhanced State Space Model (DFSSM). To effectively remove rain streaks, which produce high-intensity frequency components in specific directions, we employ frequency domain processing concurrently with SSM. Additionally, we develop a novel mixed-scale gated-convolutional block, which uses convolutions with multiple kernel sizes to capture various scale degradations effectively and integrates a gating mechanism to manage the flow of information. Finally, experiments on synthetic and real-world rainy image datasets show that our method surpasses state-of-the-art methods. Code is available at https://github.com/ShugoYamashita/DFSSM.",https://openaccess.thecvf.com/content/ACCV2024/html/Yamashita_Image_Deraining_with_Frequency-Enhanced_State_Space_Model_ACCV_2024_paper.html
3D-Aware Instance Segmentation and Tracking in Egocentric Videos,"Egocentric videos present unique challenges for 3D scene understanding due to rapid camera motion, frequent object occlusions, and limited object visibility. This paper introduces a novel approach to instance segmentation and tracking in first-person video that leverages 3D awareness to overcome these obstacles. Our method integrates scene geometry, 3D object centroid tracking, and instance segmentation to create a robust framework for analyzing dynamic egocentric scenes. By incorporating spatial and temporal cues, we achieve superior performance compared to state-of-the-art 2D approaches. Extensive evaluations on the challenging EPIC-Fields dataset demonstrate significant improvements across a range of tracking and segmentation consistency metrics. Specifically, our method outperforms the second-best performing approach by 7 points in Association Accuracy (AssA) and 4.5 points in IDF1 score, while reducing the number of ID switches by 73% to 80% across various object categories. Leveraging our tracked instance segmentations, we showcase downstream applications in 3D object reconstruction and amodal video object segmentation in these egocentric settings.",https://openaccess.thecvf.com/content/ACCV2024/html/Bhalgat_3D-Aware_Instance_Segmentation_and_Tracking_in_Egocentric_Videos_ACCV_2024_paper.html
FedRepOpt: Gradient Re-parametrized Optimizers in Federated Learning,"Federated Learning (FL) has emerged as a privacy-preserving method for training machine learning models in a distributed manner on edge devices. However, on-device models face inherent computational power and memory limitations, potentially resulting in constrained gradient updates. As the model's size increases, the frequency of gradient updates on edge devices decreases, ultimately leading to suboptimal training outcomes during any particular FL round. This limits the feasibility of deploying advanced and large-scale models on edge devices, hindering the potential for performance enhancements. To address this issue, we propose FedRepOpt, a gradient re-parameterized optimizer for FL. The gradient re-parameterized method allows training a simple local model with a similar performance as a complex model by modifying the optimizer's gradients according to a set of model-specific hyperparameters obtained from the complex models. In this work, we focus on VGG-style and Ghost-style models in the FL environment. Extensive experiments demonstrate that models using FedRepOpt obtain a significant boost in performance of 16.7% and 11.4% compared to the RepGhost-style and RepVGG-style networks, while also demonstrating a faster convergence time of 11.7% and 57.4% compared to their complex structure. Codes are available at https://github.com/StevenLauHKHK/FedRepOpt.",https://openaccess.thecvf.com/content/ACCV2024/html/Lau_FedRepOpt_Gradient_Re-parametrized_Optimizers_in_Federated_Learning_ACCV_2024_paper.html
Facing Asymmetry - Uncovering the Causal Link between Facial Symmetry and Expression Classifiers using Synthetic Interventions,"Understanding expressions is vital for deciphering human behavior, and nowadays, end-to-end trained black box models achieve high performance. Due to the black-box nature of these models, it is unclear how they behave when applied out-of-distribution. Specifically, these models show decreased performance for unilateral facial palsy patients. We hypothesize that one crucial factor guiding the internal decision rules is facial symmetry. In this work, we use insights from causal reasoning to investigate the hypothesis. After deriving a structural causal model, we develop a synthetic interventional framework. This approach allows us to analyze how facial symmetry impacts a network's output behavior while keeping other factors fixed. All 17 investigated expression classifiers significantly lower their output activations for reduced symmetry. This result is congruent with observed behavior on real-world data from healthy subjects and facial palsy patients. As such, our investigation serves as a case study for identifying causal factors that influence the behavior of black-box models.",https://openaccess.thecvf.com/content/ACCV2024/html/Buchner_Facing_Asymmetry_-_Uncovering_the_Causal_Link_between_Facial_Symmetry_ACCV_2024_paper.html
Boosting Few-Shot Detection with Large Language Models and Layout-to-Image Synthesis,"Recent advancements in diffusion models have enabled a wide range of works exploiting their ability to generate high-volume, high-quality data for use in various downstream tasks. One subclass of such models, dubbed Layout-to-Image Synthesis (LIS), learns to generate images conditioned on a spatial layout (bounding boxes, masks, poses, etc.) and has shown a promising ability to generate realistic images, albeit with limited layout-adherence. Moreover, the question of how to effectively transfer those models for scalable augmentation of few-shot detection data remains unanswered. Thus, we propose a collaborative framework employing a Large Language Model (LLM) and an LIS model for enhancing few-shot detection beyond state-of-the-art generative augmentation approaches. We leverage LLMs reasoning ability to extrapolate the spatial prior of the annotation space by generating new bounding boxes given only a few example annotations. Additionally, we introduce our novel layout-aware CLIP score for sample ranking, enabling tight coupling between generated layouts and images. Significant improvements on COCO few-shot benchmarks are observed. With our approach, a YOLOX-S baseline is boosted by more than 140%, 50%, 35% in mAP on the COCO 5-,10-, and 30-shot settings, respectively.",https://openaccess.thecvf.com/content/ACCV2024/html/Abdullah_Boosting_Few-Shot_Detection_with_Large_Language_Models_and_Layout-to-Image_Synthesis_ACCV_2024_paper.html
Attention4Align: Align Multi-View Parts Via Part2Part Hierarchical Attention Maps for Fine-Grained 3D Object Classification,"Multi-view methods offer an effective approach to address 3D object classification, and there is a growing interest in handling more practical scenarios, such as fine-grained distinctions and arbitrary viewpoints. Fine-grained object distinctions often appearance around local parts, inspiring various part-based methods. However, the parts generated by these methods are typically unordered, significantly impacting the aggregation operation across different views and consequently diminishing overall performance. To address this issue, we propose a Part2Part Hierarchical Attention, facilitating the flow of information among parts in different viewpoints through attention mechanisms within and across views. Subsequently, the Part2Part similarity matrix generated during the attention process is utilized to measure the distance between multi-view parts, aiding in their alignment. We also employ multi-scale feature fusion to enhance the quality of parts generated by weakly supervised learning. Experimental results indicate that, under the same settings, our approach achieves state-of-the-art performance on the FG3D and MVP-N datasets.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_Attention4Align_Align_Multi-View_Parts_Via_Part2Part_Hierarchical_Attention_Maps_for_ACCV_2024_paper.html
Relative pose from cylinder silhouettes,"In this paper we propose minimal solvers for relative pose estimation for two views of the projected silhouettes of two 3D cylinders. Using such line features instead of the standard point feature correspondences means more stable information (ie more stable to lighting condition, seasons, changes in environment etc.). Such features also lead to more compact and semantically interpretable representations in 3D as opposed to standard  3D point feature clouds. In this paper we show how it is possible to transform the problem into a simple parameterization where we can represent this problem as a set of six polynomials and provide solvers for their solutions. Through tests in synthetic and real settings we show that the solver is accurate and stable in the presence  of added and inherent noise.",https://openaccess.thecvf.com/content/ACCV2024/html/Gummeson_Relative_pose_from_cylinder_silhouettes_ACCV_2024_paper.html
UAGE: A Supervised Contrastive Method for Unconstrained Adaptive Gaze Estimation,"Gaze estimation, which involves perceiving human gaze directions, is the foundation of gaze analysis. It provides crucial clues for understanding human attention and intention. However, most existing methods are designed for constrained environments, which leads to a significant performance drop in unconstrained practical applications. In this work, we propose a supervised contrastive method for Unconstrained Adaptive Gaze Estimation (UAGE), which consists of an unconstrained gaze estimation method and a Gaze-guided Contrastive Domain Adaptation (GCDA) framework. Our method leverages the entire human body states and the uncertainty of gaze behaviors to robustly estimate gazes in unconstrained environments, rather than solely relying on head states. Additionally, we employ the GCDA framework to adapt the model to new domains, thereby improving its generalization ability. Experiment results show that our UAGE method has achieved state-of-the-art within-domain performance on the unconstrained GAFA dataset and has reduced the angular error by 14% compared to the baseline in cross-domain gaze estimation, with GAFA as the source domain and Gaze360 as the target domain. The code is available at https://github.com/youthhfor/UAGE.git.",https://openaccess.thecvf.com/content/ACCV2024/html/Lan_UAGE_A_Supervised_Contrastive_Method_for_Unconstrained_Adaptive_Gaze_Estimation_ACCV_2024_paper.html
ReLUifying Smooth Functions: Low-Cost Knowledge Distillation to Obtain High-Performance ReLU Networks,"Smooth activation functions like Swish, GeLU, and Mish have gained popularity due to their potentially better generalization performance than ReLU. However, there is still high demand for ReLU networks due to their simplicity, yielding higher execution efficiency and broader device coverage. To meet such practical demand, there has been research on producing ReLU networks from pretrained smooth function networks within a limited training timea process termed ReLUification. Specifically, knowledge distillation (KD) has been key tool for this endeavor. While KD-based ReLUification shows effectiveness to certain extent, the previous approach fails to fully leverage the potential of KD, resulting in suboptimal outcomes. Through in-depth empirical analysis, we uncover that employing a high learning rate synergizes effectively with KD, leading to a substantial improvement in KD-based ReLUification. Additionally, we introduce a novel approach of selectively excluding a portion of the network from ReLUification, significantly enhancing accuracy with negligible additional latency compared to the use of all ReLU networks. Thus, our proposed method produces ReLU networks substantially surpassing the quality of independently trained ReLU networks with an order of magnitude smaller training time.",https://openaccess.thecvf.com/content/ACCV2024/html/Kim_ReLUifying_Smooth_Functions_Low-Cost_Knowledge_Distillation_to_Obtain_High-Performance_ReLU_ACCV_2024_paper.html
Frequency Learning Network with Dual-Guidance Calibration for Camouflaged Object Detection,"Camouflaged object detection (COD), which aims to accurately identify objects that visually blend into surroundings, has attracted increasing interest recently. Existing models usually seek a breakthrough in the RGB domain. However, it is difficult to distinguish the target objects that are visually consistent to the backgrounds in some challenging scenarios. Considering that the frequency components can more effectively capture the details and structures of the image, we rethink the COD task from the perspective of the frequency domain. To this end, we propose a frequency learning network to mine boundary and position cues for prediction. Specifically, we design the frequency feature aggregation module to merge cross-level frequency features, which are then grouped to generate details and position cues by the frequency feature learning module. Subsequently, we propose the frequency-assisted object-boundary calibration module and the dual-guidance feature reasoning module to progressively optimize the dual-guidance cues to help calibrate the camouflaged object feature for high-quality prediction. Quantitative and qualitative experimental results demonstrate that our network outperforms the state-of-the-art COD methods.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhao_Frequency_Learning_Network_with_Dual-Guidance_Calibration_for_Camouflaged_Object_Detection_ACCV_2024_paper.html
Telling Stories for Common Sense Zero-shot Action Recognition,"Video understanding has long suffered from reliance on large labeled datasets, motivating research into zero-shot learning. Recent progress in language modeling presents opportunities to advance zero-shot video analysis, but constructing an effective semantic space relating action classes remains challenging. We address this by introducing a novel dataset, Stories, which contains rich textual descriptions for diverse action classes extracted from WikiHow articles. For each class, we extract multi-sentence narratives detailing the necessary steps, scenes, objects, and verbs that characterize the action. This contextual data enables modeling of nuanced relationships between actions, paving the way for zero-shot transfer. We also propose an approach that harnesses Stories to improve feature generation for training zero-shot classification. Without any target dataset fine-tuning, our method achieves new state-of-the-art on multiple benchmarks, improving top-1 accuracy by up to 6.1%. We believe Stories provides a valuable resource that can catalyze progress in zero-shot action recognition. The textual narratives forge connections between seen and unseen classes, overcoming the bottleneck of labeled data that has long impeded advancements in this exciting domain.",https://openaccess.thecvf.com/content/ACCV2024/html/Gowda_Telling_Stories_for_Common_Sense_Zero-shot_Action_Recognition_ACCV_2024_paper.html
Federated Class Incremental Learning: A Pseudo Feature Based Approach Without Exemplars,"Federated learning often assumes that data is fixed in advance, which is unrealistic in many real-world scenarios where new data continuously arrives, causing catastrophic forgetting. To address this challenge, we propose FCLPF (Federated Class Incremental Learning with Pseudo Features), a method that uses pseudo features generated from prototypes to mitigate catastrophic forgetting. Our approach reduces communication costs and improves efficiency by eliminating the need for past data and avoiding computationally heavy models like GANs. Experimental results on CIFAR-100 show that FCLPF achieves an average accuracy of 51.87 % and an average forgetting of 9.62 %, significantly outperforming existing baselines with an average accuracy of 47.72 % and forgetting of 20.46 %. On TinyImageNet, FCLPF achieves 37.56 % accuracy and 3.14 % forgetting, compared to the baselines' 27.69 % accuracy and 24.46 % forgetting, demonstrating the superior performance of FCLPF.",https://openaccess.thecvf.com/content/ACCV2024/html/Yoo_Federated_Class_Incremental_Learning_A_Pseudo_Feature_Based_Approach_Without_ACCV_2024_paper.html
Progressive Target Refinement by Self-Distillation for Human Pose Estimation,"The handcrafted heatmap target can be improved and one way is knowledge distillation, which takes the predicted heatmaps from another model as auxiliary supervision. However, previous pose distillation methods are training inefficient, requiring either an extra training stage or complex network architecture modification. In this paper, we propose a novel Self-Distillation for Human Pose Estimation (SDP) method for better distillation efficiency. Specifically, a student pose estimator distills the soft targets from itself with the backup information of a previous batch, where the targets are progressively refined through model updating. The main advantage of our method is that we achieve efficient training and simple implementation simultaneously. Existing pose estimation networks can benefit from the proposed method effortlessly. A stepping strategy, that widens the distillation distance with the decaying of the learning rate, is further proposed. It ensures the difference between teacher and student in a low learning rate condition. Experimental results on two widely-used benchmark datasets, MPII and COCO, illustrate the effectiveness of the proposed approach.",https://openaccess.thecvf.com/content/ACCV2024/html/Li_Progressive_Target_Refinement_by_Self-Distillation_for_Human_Pose_Estimation_ACCV_2024_paper.html
IDDiffuse: Dual-Conditional Diffusion Model for Enhanced Facial Image Anonymization,"The increasing prevalence of computer vision applications in public spaces has raised substantial privacy concerns regarding facial image data. Traditional anonymization methods, despite their potential, often suffer from drawbacks such as limited output variety, inadequate detail, distortions in extreme poses, and inconsistent temporal patterns. This study introduces an identity diffuser based on a dual-conditional diffusion model that efficiently anonymizes facial images while preserving task-relevant features for diverse applications. Our approach ensures a clear separation from the original identity by utilizing synthetic identities and an optimized identity feature space derived from three state-of-the-art models. It maintains consistency across frames for video anonymization. Unlike existing methods, our approach eliminates the need for task-relevant feature extractors, such as those for pose and expression. Instead, it employs a dual-condition diffusion model to integrate both identity and non-identity information, offering improved anonymization without compromising data usefulness. Our technique enables seamless transitions from real to synthetic identities by incorporating a time-step-dependent ID loss, providing controllable identity anonymization. Extensive studies demonstrate that our method achieves superior de-identification rates and consistency compared to state-of-the-art techniques, preserving non-identity features with a 20% improvement in emotion recognition, handling extreme poses with enhanced image quality, output diversity, and temporal consistency. This makes it a valuable tool for privacy-preserving computer vision applications.",https://openaccess.thecvf.com/content/ACCV2024/html/Shaheryar_IDDiffuse_Dual-Conditional_Diffusion_Model_for_Enhanced_Facial_Image_Anonymization_ACCV_2024_paper.html
CrossViT-ReID: Cross-Attention Vision Transformer for Occluded Cloth-Changing Person Re-Identification,"Real-world Person Re-Identification (Re-ID) presents severe challenges like occlusions and clothing changes, making traditional Re-ID methods fail. Existing occluded Re-ID methods struggle with cloth-changing scenarios, while current cloth-changing Re-ID methods do not explicitly address occlusions. To this end, we propose CrossViT-ReID, the first framework for the challenging yet practical Occluded Cloth-Changing Person Re-ID task. We perform occlusion synthesis to expose the model to real-world occlusion variations, and capture cloth-invariant body shape modality from silhouettes. The key to success of CrossViT-ReID lies in our novel cross-modality collaborative training strategy which is capable of mining the complementary relationship between appearance and shape adaptively under occlusions, clothing changes, or bad lighting conditions. Specifically, we devise two identical ViT-based branches. One branch takes in holistic appearance and occluded shape, aiming to focus on appearance when shape is noisy. Meanwhile, occluded appearance and holistic shape are inputs to the other branch, aiming to attend to shape when appearance is partly unobservable. Cross attention fusion then makes the two branches exchange beneficial information and complement each other. After being trained, our framework is able to amplify the most informative cues when facing ambiguity caused by in-the-wild Re-ID challenges, thus significantly enhancing Re-ID accuracy. Extensive experiments demonstrate the superiority of CrossViT-ReID on both cloth-changing Re-ID and occluded Re-ID datasets.",https://openaccess.thecvf.com/content/ACCV2024/html/Nguyen_CrossViT-ReID_Cross-Attention_Vision_Transformer_for_Occluded_Cloth-Changing_Person_Re-Identification_ACCV_2024_paper.html
Enhancing 3D Human Pose Estimation with Bone Length Adjustment,"Current approaches in 3D human pose estimation primarily focus on regressing 3D joint locations, often neglecting critical physical constraints such as bone length consistency and body symmetry. This work introduces a recurrent neural network architecture designed to capture holistic information across entire video sequences, enabling accurate prediction of bone lengths. To enhance training effectiveness, we propose a novel augmentation strategy using synthetic bone lengths that adhere to physical constraints. Moreover, we present a bone length adjustment method that preserves bone orientations while substituting bone lengths with predicted values. Our results demonstrate that existing 3D human pose estimation models can be significantly enhanced through this adjustment process. Furthermore, we fine-tune human pose estimation models using inferred bone lengths, observing notable improvements. Our bone length prediction model surpasses the previous best results, and our adjustment and fine-tuning method enhance performance across several metrics on the Human3.6M dataset.",https://openaccess.thecvf.com/content/ACCV2024/html/Hsu_Enhancing_3D_Human_Pose_Estimation_with_Bone_Length_Adjustment_ACCV_2024_paper.html
COCA: Classifier-Oriented Calibration via Textual Prototype for Source-Free Universal Domain Adaptation,"Universal domain adaptation (UniDA) aims to address domain and category shifts across data sources. Recently, due to more stringent data restrictions, researchers have introduced source-free UniDA (SF-UniDA). SF-UniDA methods eliminate the need for direct access to source samples when performing adaptation to the target domain. However, existing SF-UniDA methods still require an extensive quantity of labeled source samples to train a source model, resulting in significant labeling costs. To tackle this issue, we present a novel plug-and-play classifier-oriented calibration (COCA) method. COCA, which exploits textual prototypes, is designed for the source models based on few-shot learning with vision-language models (VLMs). It endows the VLM-powered few-shot learners, which are built for closed-set classification, with the unknown-aware ability to distinguish common and unknown classes in the SF-UniDA scenario. Crucially, COCA is a new paradigm to tackle SF-UniDA challenges based on VLMs, which focuses on classifier instead of image encoder optimization. Experiments show that COCA outperforms state-of-the-art UniDA and SF-UniDA models.",https://openaccess.thecvf.com/content/ACCV2024/html/Liu_COCA_Classifier-Oriented_Calibration_via_Textual_Prototype_for_Source-Free_Universal_Domain_ACCV_2024_paper.html
Enhancing Object Detection in Adverse Weather Conditions through Entropy and Guided Multimodal Fusion,"Integrating diverse representations from complementary sensing modalities is essential for robust scene interpretation in autonomous driving. Deep learning architectures that fuse vision and range data have advanced 2D and 3D object detection in recent years. However, these modalities often suffer degradation in adverse weather or lighting conditions, leading to decreased performance. While domain adaptation methods have been developed to bridge the gap between source and target domains, they typically fall short because of the inherent discrepancy between the source and target domains. This discrepancy can manifest in different distributions of data and different feature spaces. This paper introduces a comprehensive domain-adaptive object detection framework. Developed through deep transfer learning, the framework is designed to robustly generalize from labelled clear-weather data to unlabeled adverse weather conditions, enhancing the performance of deep learning-based object detection models. The innovative Patch Entropy Fusion Module (PEFM) is central to our approach, which dynamically integrates sensor data, emphasizing critical information and minimizing background distractions. This is further complemented by a novel Weighted Decision Module (WDM) that adjusts the contributions of different sensors based on their efficacy under specific environmental conditions, thereby optimizing detection accuracy. Additionally, we integrate a domain align loss during the transfer learning process to ensure effective domain adaptation by regularizing the feature map discrepancies between clear and adverse weather datasets. We evaluate our model on diverse datasets, including ExDark (unimodal), Cityscapes (unimodal), and Dense (multimodal), where it ranks 1^ st  in all datasets when we finished.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_Enhancing_Object_Detection_in_Adverse_Weather_Conditions_through_Entropy_and_ACCV_2024_paper.html
GReFEL: Geometry-Aware Reliable Facial Expression Learning under Bias and Imbalanced Data Distribution,"Reliable facial expression learning (FEL) involves the effective learning of distinctive facial expression characteristics for more reliable, unbiased and accurate predictions in real-life settings. However, current systems struggle with FEL tasks because of the variance in people's facial expressions due to their unique facial structures, movements, tones, and demographics. Biased and imbalanced datasets compound this challenge, leading to wrong and biased prediction labels. To tackle these, we introduce GReFEL, leveraging Vision Transformers and a facial geometry-aware anchor-based reliability balancing module to combat imbalanced data distributions, bias, and uncertainty in facial expression learning.  Integrating local and global data with anchors that learn different facial data points and structural features, our approach adjusts biased and mislabeled emotions caused by intra-class disparity, inter-class similarity, and scale sensitivity, resulting in comprehensive, accurate, and reliable facial expression predictions. Our model outperforms current state-of-the-art methodologies, as demonstrated by extensive experiments on various datasets.",https://openaccess.thecvf.com/content/ACCV2024/html/Wasi_GReFEL_Geometry-Aware_Reliable_Facial_Expression_Learning_under_Bias_and_Imbalanced_ACCV_2024_paper.html
Scene-Adaptive SVAD Based On Multi-modal Action-based Feature Extraction,"Due to the lack of anomalous data, most existing semi-supervised video anomaly detection (SVAD) methods rely on designing self-supervised tasks to reconstruct video frames for learning normal patterns from training data, thereby distinguishing anomalous events from normal ones according to the reconstruction quality. However, these methods heavily rely on the frequency of event occurring to judge its abnormality, which often misidentify rare normal events as anomalies. More importantly, they are usually trained to fit a particular scene leading to poor generalization to other scenes. Besides, for all existing methods, the normal/abnormal events are fixed once the training is finished, and cannot conduct test-time adjust without retraining the model. To resolve these problems, we propose a semi-supervised video anomaly detection method based on a multi-modal action-based feature extraction model. Our method exploits a vision-language model pre-trained with an action recognition task for action-based feature extraction, making it robust to scene variations irrelevant to anomalies. A clustering model with learnable prompts is employed for learning the normal patterns and anomaly detection, which does not rely on event frequency and can correctly identify rare normal events. Benefiting from the multi-modal model, our method can conveniently adjust the normal events during test time by text guidance without retraining. We conduct experiments on benchmark datasets and the results demonstrate that our method achieves the start-of-the-art performances. More importantly, our method exhibits obviously better performances in cross-scene experiment and test-time anomalies adjustment experiment.",https://openaccess.thecvf.com/content/ACCV2024/html/Gao_Scene-Adaptive_SVAD_Based_On_Multi-modal_Action-based_Feature_Extraction_ACCV_2024_paper.html
LoG-VMamba: Local-Global Vision Mamba for Medical Image Segmentation,"Mamba, a State Space Model (SSM), has recently shown competitive performance to Convolutional Neural Networks (CNNs) and Transformers in Natural Language Processing and general sequence modeling. Various attempts have been made to adapt Mamba to Computer Vision tasks, including medical image segmentation (MIS). Vision Mamba (VM)-based networks are particularly attractive due to their ability to achieve global receptive fields, similar to Vision Transformers, while also maintaining linear complexity in the number of tokens. However, the existing VM models still struggle to maintain both spatially local and global dependencies of tokens in high dimensional arrays due to their sequential nature. Employing multiple and/or complicated scanning strategies is computationally costly, which hinders applications of SSMs to high-dimensional 2D and 3D images that are common in MIS problems. In this work, we propose Local-Global Vision Mamba, LoG-VMamba, that explicitly enforces spatially adjacent tokens to remain nearby on the channel axis, and retains the global context in a compressed form. Our method allows the SSMs to access the local and global contexts even before reaching the last token while requiring only a simple scanning strategy. Our segmentation models are computationally efficient and substantially outperform both CNN and Transformers-based baselines on a diverse set of 2D and 3D MIS tasks. The implementation of LoG-VMamba is available at https://github.com/Oulu-IMEDS/LoG-VMamba.",https://openaccess.thecvf.com/content/ACCV2024/html/Dang_LoG-VMamba_Local-Global_Vision_Mamba_for_Medical_Image_Segmentation_ACCV_2024_paper.html
Domain Aware Multi-Task Pre-Training of 3D Swin Transformer for Brain MRI,"The scarcity of annotated medical images is a major bottleneck in developing learning models for medical image analysis. Hence, recent studies have focused on pretrained models with fewer annotation requirements that can be fine-tuned for various downstream tasks. However, existing approaches are mainly 3D adaptions of 2D approaches ill-suited for 3D medical imaging data. Motivated by this gap, we propose novel domain-aware multi-task learning tasks to pretrain a 3D Swin Transformer for brain magnetic resonance imaging (MRI). Our method considers the domain knowledge in brain MRI by incorporating brain anatomy and morphology as well as standard pretext tasks adapted for 3D imaging in a contrastive learning setting. We pretrain our model using large-scale brain MRI data of 13,687 samples spanning several large-scale databases. Our method outperforms existing supervised and self-supervised methods in three downstream tasks of Alzheimers disease classification, Parkinsons disease classification, and age prediction tasks. The ablation study of the proposed pretext tasks shows the effectiveness of our pretext tasks",https://openaccess.thecvf.com/content/ACCV2024/html/Kim_Domain_Aware_Multi-Task_Pre-Training_of_3D_Swin_Transformer_for_Brain_ACCV_2024_paper.html
CoVLM: Leveraging Consensus from Vision-Language Models for Semi-supervised Multimodal Fake News Detection,"In this work, we address the real-world, challenging task of out-of-context misinformation detection, where a real image is paired with an incorrect caption for creating fake news. Existing approaches for this task assume the availability of large amounts of labeled data, which is often impractical in real-world, since it requires extensive manual intervention and domain expertise. In contrast, since obtaining a large corpus of unlabeled image-text pairs is much easier, here, we propose a semi-supervised protocol, where the model has access to a limited number of labeled image-text pairs and a large corpus of unlabeled pairs. Additionally, the occurrence of fake news being much lesser compared to the real ones, the datasets tend to be highly imbalanced, thus making the task even more challenging. Towards this goal, we propose a novel framework, Consensus from Vision-Language Models (CoVLM), which generates robust pseudo-labels for unlabeled pairs using thresholds derived from the labeled data. This approach can automatically determine the right threshold parameters of the model for selecting the confident pseudo-labels. Experimental results on benchmark datasets across challenging conditions and comparisons with state-of-the-art approaches demonstrate the effectiveness of our framework.",https://openaccess.thecvf.com/content/ACCV2024/html/Devank_CoVLM_Leveraging_Consensus_from_Vision-Language_Models_for_Semi-supervised_Multimodal_Fake_ACCV_2024_paper.html
Graph Cut-guided Maximal Coding Rate Reduction for Learning Image Embedding and Clustering,"In the era of pretrained models, image clustering task is usually addressed by two relevant stages: a) to produce features from pretrained vision models; and b) to find clusters from the pre-traiend features. However, these two stages are often considered separately or learned by different paradigms, leading to suboptimal clustering performance. In this paper, we propose a unified framework for jointly learning structured embeddings and clustering, termed graph Cut-guided Maximal Coding Rate Reduction (CgMCR^2), in which the learning of clustering results effectively facilitates the learning of embeddings toward forming a union-of-orthogonal-subspaces. To be specific, in CgMCR^2, we integrate a flexible and principled clustering module into the framework of maximal coding rate reduction, in which the clustering module provides partition information to help the cluster-wise compression for the embeddings and the learned embeddings in turn help to yield more accurate clustering results. We conduct extensive experiments on both standard and out-of-domain image datasets and experimental results validate the effectiveness of our approach.",https://openaccess.thecvf.com/content/ACCV2024/html/He_Graph_Cut-guided_Maximal_Coding_Rate_Reduction_for_Learning_Image_Embedding_ACCV_2024_paper.html
Tails Tell Tales: Chapter-wide Manga Transcriptions with Character Names,"Enabling engagement of manga by visually impaired individuals presents a significant challenge due to its inherently visual nature. With the goal of fostering accessibility, this paper aims to generate a dialogue transcript of a complete manga chapter, entirely automatically, with a particular emphasis on ensuring narrative consistency. This entails identifying (i)   what  is being said, i.e., detecting the texts on each page and classifying them into essential vs non-essential, and (ii)   who  is saying it, i.e., attributing each dialogue to its speaker, while ensuring the same characters are named consistently throughout the chapter. To this end, we introduce: (i)  Magiv2, a model that is capable of generating high-quality chapter-wide manga transcripts with named characters and significantly higher precision in speaker diarisation over prior works; (ii) an extension of the PopManga evaluation dataset, which now includes annotations for speech-bubble tail boxes, associations of text to corresponding tails, classifications of text as essential or non-essential, and the identity for each character box; and (iii) a new character bank dataset, which comprises over 2.2K principal characters from 64 manga series, featuring an average of 6.8 exemplar images per character, as well as a list of chapters in which they appear. The code, trained model, and both datasets will be made publicly available.",https://openaccess.thecvf.com/content/ACCV2024/html/Sachdeva_Tails_Tell_Tales_Chapter-wide_Manga_Transcriptions_with_Character_Names_ACCV_2024_paper.html
Interaction-Guided Two-Branch Image Dehazing Network,"Image dehazing aims to restore clean images from hazy ones. Convolutional Neural Networks (CNNs) and Transformers have demonstrated exceptional performance in local and global feature extraction, respectively, and currently represent the two mainstream frameworks in image dehazing. In this paper, we propose a novel dual-branch image dehazing framework that guides CNN and Transformer components interactively. We reconsider the complementary characteristics of CNNs and Transformers by leveraging the differential relationships between global and local features for interactive guidance. This approach enables the capture of local feature positions through global attention maps, allowing the CNN to focus solely on feature information at effective positions. The single-branch Transformer design ensures the network's global information recovery capability. Extensive experiments demonstrate that our proposed method yields competitive qualitative and quantitative evaluation performance on both synthetic and real public datasets. Codes are available at https://github.com/Feecuin/Two-Branch-Dehazing",https://openaccess.thecvf.com/content/ACCV2024/html/Liu_Interaction-Guided_Two-Branch_Image_Dehazing_Network_ACCV_2024_paper.html
Auxiliary Domain-guided Adaptive Detection in Adverse Weather Conditions,"To enhance detection accuracy in adverse weather conditions, domain adaptation methods that extract domain-invariant features from both the source and target domains have been proposed for one-stage detectors. However, the use of pseudo-labels in the instance-level domain adaptation inevitably introduces noise. To tackle this challenge, we propose an auxiliary domain-guided adaptive one-stage detection method. Firstly, a generative network is used to transform source domain images into the auxiliary domain. To form a suitable auxiliary domain that can provide reliable guidance for instance-level adaptation of the detector, the generated images are required to possess a similar style to that of the target domain, while also being restricted to maintaining the same object categories and location information as the source domain images. Secondly, for instance-level adaptation, we treat the same object from different domains as positive samples and different objects as negative samples, and utilize contrastive learning to ensure that only the domain shift, rather than other differences in data distributions between distinct instances, is reduced during adaptation. Experimental results demonstrate that the proposed algorithm achieves a significant improvement over state-of-the-art (SOTA) algorithms on real datasets captured under adverse weather conditions.",https://openaccess.thecvf.com/content/ACCV2024/html/Fu_Auxiliary_Domain-guided_Adaptive_Detection_in_Adverse_Weather_Conditions_ACCV_2024_paper.html
OccFusion: Depth Estimation Free Multi-sensor Fusion for 3D Occupancy Prediction,"3D occupancy prediction based on multi-sensor fusion, crucial for a reliable autonomous driving system, enables fine-grained understanding of 3D scenes. Previous fusion-based 3D occupancy predictions relied on depth estimation for processing 2D image features. However, depth estimation is an ill-posed problem, hindering the accuracy and robustness of these methods. Furthermore, fine-grained occupancy prediction demands extensive computational resources. To address these issues, we propose OccFusion, a depth estimation free multi-modal fusion framework. Additionally, we introduce a generalizable active training method and an active decoder that can be applied to any occupancy prediction model, with the potential to enhance their performance. Experiments conducted on nuScenes-Occupancy and nuScenes-Occ3D demonstrate our frameworks superior performance. Detailed ablation studies highlight the effectiveness of each proposed method.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_OccFusion_Depth_Estimation_Free_Multi-sensor_Fusion_for_3D_Occupancy_Prediction_ACCV_2024_paper.html
Diffusion-based Multimodal Video Captioning,"Diffusion-based models have recently demonstrated notable success in various generative tasks involving continuous signals, such as image, video, and audio synthesis. However, their applicability to video captioning has not yet received widespread attention, primarily due to the discrete nature of captions and the complexities of conditional generation across multiple modalities. This paper delves into diffusion-based video captioning and experiments with various modality fusion methods and different modality combinations to assess their impact on the quality of generated captions. The novelty of our proposed MM-Diff-Net is in the use of diffusion models in multimodal video captioning and in the introduction of a number of mid-fusion techniques for that purpose. Additionally, we propose a new input modality: generated description, which is attended to enhance caption quality. Experiments are conducted on four well-established benchmark datasets, YouCook2, MSR-VTT, VATEX, and VALOR-32K, to evaluate the proposed model and fusion methods. The findings indicate that combining all modalities yields the best captions, but the effect of fusion methods varies across datasets. The performance of our proposed model shows the potential of diffusion-based models in video captioning, paving the way for further exploration and future research in the area.",https://openaccess.thecvf.com/content/ACCV2024/html/Kainulainen_Diffusion-based_Multimodal_Video_Captioning_ACCV_2024_paper.html
Estimating Soil Organic Carbon from Multispectral Images using Physics-Informed Neural Networks,"Understanding the amount of Soil Organic Carbon (SOC) at farm and field scale is a necessary precursor to effective management, important for both agricultural productivity and to reduce CO2 emissions.  To avoid the prohibitive cost of measurement, SOC can be estimated by using multispectral images.  In this study, we propose a novel Physics-Informed Convolutional Neural Network (CNN) to model well-known but noisy relationship between a soil index and SOC using the networks loss function.  This study is also conducted by resampling the European Land Use/Classification Area Survey (LUCAS) dataset to Sentinel-2 bands.   Our experimental results show that our proposed network converges more quickly, has a lower root mean squared error (RMSE) and is more robust (as measured by the standard deviation of RMSE over multiple trials) than a compatible standard CNN.  The operation of the novel Physics-Informed CNN is explained in terms of the components of the loss function.",https://openaccess.thecvf.com/content/ACCV2024/html/Sargeant_Estimating_Soil_Organic_Carbon_from_Multispectral_Images_using_Physics-Informed_Neural_ACCV_2024_paper.html
RNA: Video Editing with ROI-based Neural Atlas,"With the recent growth of video-based Social Network Service (SNS) platforms, the demand for video editing among common users has increased. However, video editing can be challenging due to the temporally-varying factors such as camera movement and moving objects. While modern atlas-based video editing methods have addressed these issues, they often fail to edit videos including complex motion or multiple moving objects, and demand excessive computational cost, even for very simple edits. In this paper, we propose a novel region-of-interest (ROI)-based video editing framework: ROI-based Neural Atlas (RNA). Unlike prior work, RNA allows users to specify editing regions, simplifying the editing process by removing the need for foreground separation and atlas modeling for foreground objects. However, this simplification presents a unique challenge: acquiring a mask that effectively handles occlusions in the edited area caused by moving objects, without relying on an additional segmentation model. To tackle this, we propose a novel mask refinement approach designed for this specific challenge. Moreover, we introduce a soft neural atlas model for video reconstruction to ensure high-quality editing results. Extensive experiments show that RNA offers a more practical and efficient editing solution, applicable to a wider range of videos with superior quality compared to prior methods.",https://openaccess.thecvf.com/content/ACCV2024/html/Lee_RNA_Video_Editing_with_ROI-based_Neural_Atlas_ACCV_2024_paper.html
Masking Cascaded Self-Attentions for Few-Shot Font-Generation Transformer,"Few-shot Font Generation (FFG) is a practical technology widely used in designing artistic characters, handwriting imitation, and identification, etc., which aims to generate realistic font images with a few reference samples. Typically, convolutional neural networks (CNNs) are employed to learn the representations of style and content from the font images. However, owing to the locality of convolutional operations, CNNs are not good at capturing the global structure of fonts, which leads to the generated images with blurry components and distorted space layouts. To address this problem, we consider cascading self-attention modules to exploit long-range dependencies for font generation and propose a transformer-based approach called FGTr. Following the style-content disentanglement paradigm, FGTr contains two different transformer encoders to extract the style and content sequences. A multi-layer transformer decoder is adopted to merge the two sequences and generate target images. In order to smoothen the transition of patch edges, we utilize a Local Self-Attention Mask (LSAM) to restrict the attention scope of each patch to a fixed-size sliding window, which plugs into the Transformer with no extra parameters. We also propose an Auxiliary Generation Module (AGM) in favor of generating glyphs closer to the real. Extensive experiments demonstrate the effectiveness and superiority of our method compared with state-of-the-art CNN-based models.",https://openaccess.thecvf.com/content/ACCV2024/html/Ma_Masking_Cascaded_Self-Attentions_for_Few-Shot_Font-Generation_Transformer_ACCV_2024_paper.html
PixMamba: Leveraging State Space Models in a Dual-Level Architecture for Underwater Image Enhancement,"Underwater Image Enhancement (UIE) is critical for marine research and exploration but hindered by complex color distortions and severe blurring. Recent deep learning-based methods have achieved remarkable results, yet these methods struggle with high computational costs and insufficient global modeling, resulting in locally under- or over-adjusted regions. We present PixMamba, a novel architecture, designed to overcome these challenges by leveraging State Space Models (SSMs) for efficient global dependency modeling. Unlike convolutional neural networks (CNNs) with limited receptive fields and transformer networks with high computational costs, PixMamba efficiently captures global contextual information while maintaining computational efficiency. Our dual-level strategy features the patch-level Efficient Mamba Net (EMNet) for reconstructing enhanced image feature and the pixel-level PixMamba Net (PixNet) to ensure fine-grained feature capturing and global consistency of enhanced image that were previously difficult to obtain.  PixMamba achieves state-of-the-art performance across various underwater image datasets and delivers visually superior results. Code is available at https://github.com/weitunglin/pixmamba.",https://openaccess.thecvf.com/content/ACCV2024/html/Lin_PixMamba_Leveraging_State_Space_Models_in_a_Dual-Level_Architecture_for_ACCV_2024_paper.html
OneBEV: Using One Panoramic Image for Bird's-Eye-View Semantic Mapping,"In the field of autonomous driving, Bird's-Eye-View (BEV) perception has attracted increasing attention in the community since it provides more comprehensive information compared with pinhole front-view images and panoramas. Traditional BEV methods, which rely on multiple narrow-field cameras and complex pose estimations, often face calibration and synchronization issues. To break the wall of the aforementioned challenges, in this work, we introduce OneBEV, a novel BEV semantic mapping approach using merely a single panoramic image as input, simplifying the mapping process and reducing computational complexities. A distortion-aware module termed Mamba View Transformation (MVT) is specifically designed to handle the spatial distortions in panoramas, transforming front-view features into BEV features without leveraging traditional attention mechanisms. Apart from the efficient framework, we contribute two datasets, i.e., nuScenes-360 and DeepAccident-360, tailored for the OneBEV task. Experimental results showcase that OneBEV achieves state-of-the-art performance with 51.1% and 36.1% mIoU on nuScenes-360 and DeepAccident-360, respectively. This work advances BEV semantic mapping in autonomous driving, paving the way for more advanced and reliable autonomous systems.",https://openaccess.thecvf.com/content/ACCV2024/html/Wei_OneBEV_Using_One_Panoramic_Image_for_BirdAos-Eye-View_Semantic_Mapping_ACCV_2024_paper.html
Seamless-Through-Breaking: Rethinking Image Stitching for Optimal Alignment,"In this paper, we introduce a novel concept called seamless-through-breaking to tackle the challenges that arise in image stitching. Conventional methods attempt to maintain warping continuity while stitching two images together to avoid visible breaks in the final output. However, we propose that content alignment and warping continuity are mutually exclusive, especially when a significant depth gap exists between the foreground and the background. To solve this issue, we use optical flow to warp the source image into the target image's domain, which allows the creation of holes in the source image. Considering that optical flow estimators are trained on synthetic data, we fine-tune the estimator using real-world data to improve its accuracy in practical applications. Once the images are aligned within the same domain, we fill these holes with content from the target image. Additionally, as no optical flow estimators are perfect, directly copying pixels from the target image to fill the holes may create visual artifacts. To avoid this issue, we apply an image inpainting technique around the edges of the holes to smooth out alignment discrepancies, ensuring that the stitched image looks as natural as if it were captured in one shot.",https://openaccess.thecvf.com/content/ACCV2024/html/Chen_Seamless-Through-Breaking_Rethinking_Image_Stitching_for_Optimal_Alignment_ACCV_2024_paper.html
Spatiotemporal Pooling on Appropriate Topological Maps Represented as Two-Dimensional Images for EEG Classification,"Motor imagery classification based on electroencephalography (EEG) signals is one of the most important brain-computer interface applications, although it needs further improvement. Several methods have attempted to obtain useful information from EEG signals by using recent deep learning techniques such as transformers. To improve the classification accuracy, this study proposes a novel EEG-based motor imagery classification method with three key features: generation of a topological map represented as a two-dimensional image from EEG signals with coordinate transformation based on t-SNE, use of the InternImage to extract spatial features, and use of spatiotemporal pooling inspired by PoolFormer to exploit spatiotemporal information concealed in a sequence of EEG images. Experimental results using the PhysioNet EEG Motor Movement/Imagery dataset showed that the proposed method achieved the best classification accuracy of 88.57%, 80.69%, and 70.20% on two-, three-, and four-class motor imagery tasks in cross-individual validation.",https://openaccess.thecvf.com/content/ACCV2024/html/Fukushima_Spatiotemporal_Pooling_on_Appropriate_Topological_Maps_Represented_as_Two-Dimensional_Images_ACCV_2024_paper.html
TexDC: Text-Driven Disease-Aware 4D Cardiac Cine MRI Images Generation,"Generating disease-aware cardiac cine magnetic resonance imaging (cine MRI) images has immense potential in medical research, with recent advancements in text-driven image generation technology offering a viable solution. However, establishing clear correlations between textual descriptions and subtle disease regions, especially in capturing their dynamic complexities within cardiac contexts, remains a challenge. To tackle this, our approach emphasizes pre-aligning textual and cardiac cine MRI image features to highlight critical disease areas, establishing interactive relationships between disease text features and spatiotemporal image features during generation. We propose a text-driven framework for synthesizing disease-aware cardiac cine MRI images. Initially, knowledge is transferred from large language models, refining input semantics by updating learnable contexts. By introducing disease-aware pre-alignment, we emphasize and align key disease features across textual and spatiotemporal dimensions, effectively guiding image generation while maintaining spatiotemporal coherence. To our knowledge, this represents the first application of text-driven medical image generation in 4D modalities. We evaluate the superiority of our method on multi-center cardiac cine MRI datasets. Our code is available at: xxx.",https://openaccess.thecvf.com/content/ACCV2024/html/Liu_TexDC_Text-Driven_Disease-Aware_4D_Cardiac_Cine_MRI_Images_Generation_ACCV_2024_paper.html
4DPV: 4D Pet from Videos by Coarse-to-Fine Non-Rigid Radiance Fields,"We present a coarse-to-fine neural deformation model to simultaneously recover the camera pose and the 4D reconstruction of an unknown object from multiple RGB sequences in the wild. To that end, our approach does not consider any pre-built 3D template nor 3D training data as well as controlled illumination conditions, and can sort out the problem in a self-supervised manner. Our model exploits canonical and image-variant spaces where both coarse and fine components are considered. We introduce a neural local quadratic model with spatio-temporal consistency to encode fine details that is combined with canonical embeddings in order to establish correspondences across sequences. We thoroughly validate the method on challenging scenarios with complex and real-world deformations, providing both quantitative and qualitative evaluations, an ablation study and a comparison with respect to competing approaches.",https://openaccess.thecvf.com/content/ACCV2024/html/de_Paco_4DPV_4D_Pet_from_Videos_by_Coarse-to-Fine_Non-Rigid_Radiance_Fields_ACCV_2024_paper.html
A computational model for color assimilation illusions and color constancy,"Our visual system unconsciously estimates the objects reflectance in the scene. Even under different illumination conditions, it can discount the effects of the illuminant to recognize the true colors of the objects. Yet, under some circumstances, the perceived color can differ from the actual reflectance. Color illusions can be given as an example of such circumstances. While computer vision studies aim at estimating the scenes illuminant, computational biology studies mostly aim at reproducing our sensation on color illusions. However, as stated in many studies, an algorithm mimicking our perception should be deceived by color illusions, while estimating the reflectance under varying illumination conditions. Yet, to the best of our knowledge, there is no study that mimics our sensation on color illusions and also investigates computational color constancy in detail by using a single method. Based on this motivation, we develop a single method that mimics our behavior on color assimilation illusions and performs color constancy. In particular, we propose a multiresolution color constancy strategy that operates in scale-space. In our approach, we make use of a variant of the local space average color method which we further modify by considering the gradient changes of the scene. According to the experimental results, our algorithm mimics our sensation on color illusions, and it presents competitive results on 4 different color constancy benchmarks.",https://openaccess.thecvf.com/content/ACCV2024/html/Ulucan_A_computational_model_for_color_assimilation_illusions_and_color_constancy_ACCV_2024_paper.html
TaE: Task-aware Expandable Representation for Long Tail Class Incremental Learning,"Class-incremental learning is dedicated to the development of deep learning models that are capable of acquiring new knowledge while retaining previously learned information. Most methods focus on balanced data distribution for each task, overlooking real-world long-tailed distributions. Therefore, Long-Tailed Class-Incremental Learning has been introduced, which trains on data where head classes have more samples than tail classes. Existing methods mainly focus on preserving representative samples from previous classes to combat catastrophic forgetting. Recently, dynamic network algorithms freeze old network structures and expand new ones, achieving significant performance. However,  with the introduction of the long-tail problem, merely extending Determined blocks can lead to miscalibrated predictions, while expanding the entire backbone results in an explosion of memory size. To address these issues, we introduce a novel Task-aware Expandable (TaE) framework, dynamically allocating and updating task-specific trainable parameters to learn diverse representations from each incremental task while resisting forgetting through the majority of frozen model parameters. To further encourage the class-specific feature representation, we develop a Centroid-Enhanced (CEd) method to guide the update of these task-aware parameters. This approach is designed to adaptively allocate feature space for every class by adjusting the distance between intra- and inter-class features, which can extend to all ""training from sketch"" algorithms. Extensive experiments demonstrate that TaE achieves state-of-the-art performance.",https://openaccess.thecvf.com/content/ACCV2024/html/Li_TaE_Task-aware_Expandable_Representation_for_Long_Tail_Class_Incremental_Learning_ACCV_2024_paper.html
Feature Generator for Few-Shot Learning,"Few-shot learning (FSL) aims to enable models to recognize novel objects or classes with limited labeled data. Feature generators, which synthesize new data points to augment limited datasets, have emerged as a promising solution to this challenge. This paper investigates the effectiveness of feature generators in enhancing the embedding process for FSL tasks. To address the issue of inaccurate embeddings due to the scarcity of images per class, we introduce a feature generator that creates visual features from class-level textual descriptions. By training the generator with a combination of classifier loss, discriminator loss, and distance loss between the generated features and true class embeddings, we ensure the generation of accurate same-class features and enhance the overall feature representation. Our results show a significant improvement in accuracy over baseline methods, with our approach outperforming the baseline model by 10% in 1-shot and around 5% in 5-shot approaches. Additionally, both visual-only and visual + textual generators have also been tested in this paper. The code is publicly available at https://github.com/heethanjan/Feature-Generator-for-FSL.",https://openaccess.thecvf.com/content/ACCV2024/html/Kanagalingam_Feature_Generator_for_Few-Shot_Learning_ACCV_2024_paper.html
"SeSame: Simple, Easy 3D Object Detection with Point-Wise Semantics","In autonomous driving, 3D object detection provides more precise information for downstream tasks, including path planning and motion estimation, compared to 2D object detection. In this paper, we propose SeSame: a method aimed at enhancing semantic information in existing LiDAR-only based 3D object detection. This addresses the limitation of existing 3D detectors, which primarily focus on object presence and classification, thus lacking in capturing relationships between elemental units that constitute the data, akin to semantic segmentation. Experiments demonstrate the effectiveness of our method with performance improvements on the KITTI object detection benchmark. Our code is available at https://github.com/HAMA-DL-dev/SeSame",https://openaccess.thecvf.com/content/ACCV2024/html/O_SeSame_Simple_Easy_3D_Object_Detection_with_Point-Wise_Semantics_ACCV_2024_paper.html
Act Like a Radiologist: Radiology Report Generation across Anatomical Regions,"Automating radiology report generation can ease the reporting workload for radiologists. However, existing works focus mainly on the chest area due to the limited availability of public datasets for other regions. Besides, they often rely on naive data-driven approaches, e.g., a basic encoder-decoder framework with captioning loss, which limits their ability to recognise complex patterns across diverse anatomical regions. To address these issues, we propose X-RGen, a radiologist-minded report generation framework across six anatomical regions. In X-RGen, we seek to mimic the behaviour of human radiologists, breaking them down into four principal phases: 1) initial observation, 2) cross-region analysis, 3) medical interpretation, and 4) report formation. Firstly, we adopt a Transformer-based image encoder for feature extraction, akin to a radiologist's preliminary review. Secondly, we enhance the recognition capacity of the image encoder by analysing images and reports across various regions, mimicking how radiologists gain their experience and improve their professional ability from past cases. Thirdly, just as radiologists apply their expertise to interpret radiology images, we introduce radiological knowledge of multiple anatomical regions to further analyse the features from a clinical perspective. Lastly, we generate reports based on the medical-aware features using a typical auto-regressive text decoder. Both natural language generation (NLG) and clinical efficacy metrics show the effectiveness of X-RGen on six X-ray datasets.",https://openaccess.thecvf.com/content/ACCV2024/html/Chen_Act_Like_a_Radiologist_Radiology_Report_Generation_across_Anatomical_Regions_ACCV_2024_paper.html
PlainUSR: Chasing Faster ConvNet for Efficient Super-Resolution,"Reducing latency is a roaring trend in recent super-resolution (SR) research. While recent progress exploits various convolutional blocks, attention modules, and backbones to unlock the full potentials of the convolutional neural network (ConvNet), achieving real-time performance remains a challenge. To this end, we present PlainUSR, a novel framework incorporating three pertinent modifications to expedite ConvNet for efficient SR. For the convolutional block, we squeeze the lighter but slower MobileNetv3 block into a heavier but faster vanilla convolution by reparameterization tricks to balance memory access and calculations. For the attention module, by modulating input with a regional importance map and gate, we introduce local importance-based attention to realize high-order information interaction within a 1-order attention latency. As to the backbone, we propose a plain U-Net that executes channel-wise discriminate splitting and concatenation. In the experimental phase, PlainUSR exhibits impressively low latency, great scalability, and competitive performance compared to both state-of-the-art latency-oriented and quality-oriented methods. In particular, compared to recent NGswin, the PlainUSR-L is 16.4x faster with competitive performance.",https://openaccess.thecvf.com/content/ACCV2024/html/Wang_PlainUSR_Chasing_Faster_ConvNet_for_Efficient_Super-Resolution_ACCV_2024_paper.html
VIFA: An Efficient Visible and Infrared Image Fusion Architecture for Multi-task Applications via Continual Learning,"Visible-infrared image fusion has attracted great attention in a range of computer vision applications. Aiming at improving task-specific performance, recent studies have employed a cascading approach, where the fusion network is trained using feedback from the specific downstream task network. However, this training strategy will result in the overfitting of the fusion network, and deploying a different fusion network for each downstream task is inefficient for multi-task scenarios. To address this challenge, we propose VIFA, a visible-infrared image fusion architecture for multi-task applications. This architecture effectively mitigates the catastrophic forgetting problem by partitioning the fusion network into a knowledge-sharing backbone and task-specific components. To facilitate knowledge sharing, we introduce a key channel-constrained distillation strategy, which identifies and retains informative features, while allowing non-critical channels to learn new knowledge. In addition, we propose a reference model-guided distillation to compress the task-specific components while maintaining model performance. Evaluations on multiple representative fusion networks show that VIFA can significantly improve task performance and speed.",https://openaccess.thecvf.com/content/ACCV2024/html/Shi_VIFA_An_Efficient_Visible_and_Infrared_Image_Fusion_Architecture_for_ACCV_2024_paper.html
TCL-Net: A Lightweight and Efficient Dehazing Network with Frequency-Domain Fusion and Multi-Angle Attention,"Mamba, a State Space Model (SSM), has recently shown competitive performance to Convolutional Neural Networks (CNNs) and Transformers in Natural Language Processing and general sequence modeling. Various attempts have been made to adapt Mamba to Computer Vision tasks, including medical image segmentation (MIS). Vision Mamba (VM)-based networks are particularly attractive due to their ability to achieve global receptive fields, similar to Vision Transformers, while also maintaining linear complexity in the number of tokens. However, the existing VM models still struggle to maintain both spatially local and global dependencies of tokens in high dimensional arrays due to their sequential nature. Employing multiple and/or complicated scanning strategies is computationally costly, which hinders applications of SSMs to high-dimensional 2D and 3D images that are common in MIS problems. In this work, we propose Local-Global Vision Mamba, LoG-VMamba, that explicitly enforces spatially adjacent tokens to remain nearby on the channel axis, and retains the global context in a compressed form. Our method allows the SSMs to access the local and global contexts even before reaching the last token while requiring only a simple scanning strategy. Our segmentation models are computationally efficient and substantially outperform both CNN and Transformers-based baselines on a diverse set of 2D and 3D MIS tasks. The implementation of LoG-VMamba is available at https://github.com/Oulu-IMEDS/LoG-VMamba.",https://openaccess.thecvf.com/content/ACCV2024/html/Tang_TCL-Net_A_Lightweight_and_Efficient_Dehazing_Network_with_Frequency-Domain_Fusion_ACCV_2024_paper.html
Blind Super Resolution with Reference Images and Implicit Degradation Representation,"Previous studies in blind super-resolution (BSR) have primarily concentrated on estimating degradation kernels directly from low-resolution (LR) inputs to enhance super-resolution. However, these degradation kernels, which model the transition from a high-resolution (HR) image to its LR version, should account for not only the degradation process but also the downscaling factor. Applying the same degradation kernel across varying super-resolution scales may be impractical. Our research acknowledges degradation kernels and scaling factors as pivotal elements for the BSR task and introduces a novel strategy that utilizes HR images as references to establish scale-aware degradation kernels. By employing content-irrelevant HR reference images alongside the target LR image, our model adaptively discerns the degradation process. It is then applied to generate additional LR-HR pairs through down-sampling the HR reference images, which are keys to improving the SR performance. Our reference-based training procedure is applicable to proficiently trained blind SR models and zero-shot blind SR methods, consistently outperforming previous methods in both scenarios. This dual consideration of blur kernels and scaling factors, coupled with the use of a reference image, contributes to the effectiveness of our approach in blind super-resolution tasks.",https://openaccess.thecvf.com/content/ACCV2024/html/Do_Blind_Super_Resolution_with_Reference_Images_and_Implicit_Degradation_Representation_ACCV_2024_paper.html
DA^2: Degree-Accumulated Data Augmentation on Point Clouds with Curriculum Dynamic Threshold Selection,"Conventional point cloud data augmentation methods typically employ offline transformations with predefined, randomly applied transformations. This randomness may lead to suboptimal training samples that are not suitable for the current training stage. Additionally, the predefined parameter range restricts the exploration space of augmentation, limiting the diversity of samples. This paper introduces Degree-Accumulated Data Augmentation (DA^2), a novel approach that accumulates augmentations to expand the exploration space beyond predefined limits. We utilize a teacher-guided auto-augmenter to prevent the generation of excessively distorted or unrecognizable samples. This method aims to generate challenging yet suitable samples, progressively increasing the difficulty to enhance the model's robustness. Additionally, according to a student model's ability, we propose Curriculum Dynamic Threshold Selection (CDTS) to filter overly challenging samples, allowing the model to start with high-quality objects and gradually handle more complex ones as model stability improves. Our experiments show that this framework significantly enhances accuracy across various 3D point cloud classifiers.",https://openaccess.thecvf.com/content/ACCV2024/html/Tai_DA2_Degree-Accumulated_Data_Augmentation_on_Point_Clouds_with_Curriculum_Dynamic_ACCV_2024_paper.html
FaRE: A Feature-aware Radical Encoding Strategy for Zero-shot Chinese Character Recognition,"Due to the complexity of glyphs and the vast vocabulary, zero-shot Chinese character recognition (ZSCCR) remains a prominent research topic. A mainstream approach involves radical-based character decomposition. However, existing methods typically employ random encoding for each radical post-decomposition, leading to potential topology distortions in the radical encoding and glyph spaces. To address these issues, we propose a novel Feature-aware Radical Encoding (FaRE) strategy that incorporates visual feature clues into radical encodings to generate feature-aware representations. Initially, we create radical images by rendering TTF files and then apply a pre-trained feature extractor to obtain the feature representation of each radical. Finally, projection and binarization operations are performed to produce compact and efficient radical encodings. Extensive experiments on the public benchmark ICDAR2013 demonstrate that the proposed FaRE significantly enhances the state-of-the-art ZSCCR performance. Additionally, abundant ablation studies are conducted to validate the effectiveness of the proposed FaRE.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhan_FaRE_A_Feature-aware_Radical_Encoding_Strategy_for_Zero-shot_Chinese_Character_ACCV_2024_paper.html
KEM: SGW-based Multi-Task Learning in Vision Tasks,"Multi-task-learning(MTL) is a multi-target optimization task. Neural networks try to realize each target using a shared interpretative space within MTL. However, as the scale of datasets expands and the complexity of tasks increases, knowledge sharing becomes increasingly challenging. In this paper, we first re-examine previous cross-attention MTL methods from the perspective of noise. We theoretically analyze this issue and identify it as a flaw in the cross-attention mechanism. To address this issue, we propose an information bottleneck knowledge extraction module (KEM). This module aims to reduce inter-task interference by constraining the flow of information, thereby reducing computational complexity. Furthermore, we have employed neural collapse to stabilize the knowledge-selection process. That is, before input to KEM, we projected the features into ETF space. This mapping makes our method more robust. We implemented and conducted comparative experiments with this method on multiple datasets. The results demonstrate that our approach significantly outperforms existing methods in multi-task learning. All code will be made publicly available upon the paper's acceptance.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_KEM_SGW-based_Multi-Task_Learning_in_Vision_Tasks_ACCV_2024_paper.html
It's Just Another Day: Unique Video Captioning by Discriminative Prompting,"Long videos contain many repeating actions, events and shots. These repetitions are frequently given identical captions, which makes it difficult to retrieve the exact desired clip using a text search. In this paper, we formulate the problem of unique captioning: Given multiple clips with the same caption, we generate a new caption for each clip that uniquely identifies it. We propose Captioning by Discriminative Prompting (CDP), which predicts a property that can separate identically captioned clips, and use it to generate unique captions. We introduce two benchmarks for unique captioning, based on egocentric footage and timeloop movies - where repeating actions are common. We demonstrate that captions generated by CDP improve text-to-video R@1 by 15% for egocentric videos and 10% in timeloop movies.",https://openaccess.thecvf.com/content/ACCV2024/html/Perrett_Its_Just_Another_Day_Unique_Video_Captioning_by_Discriminative_Prompting_ACCV_2024_paper.html
Learning Classwise Untangled Continuums for Conditional Normalizing Flows,"Normalizing flows (NFs) are invertible and bijective generative models, capable of performing exact density estimation of complex data by mapping them from highly nonlinear ambient spaces to simpler latent ones. These mappings hold many promises for images since capturing their true distribution could greatly enhance the performance of downstream tasks such as  image classification. In this paper, we devise a novel conditional normalizing flow model that achieves both conditional image generation and classification. The main contribution of our method consists in learning untangled continuums of gaussian distributions in the latent space that maximize the discrimination power of the learned NFs together with the quality, diversity and label reliability of the underlying generated images. This results into  highly effective NF classifiers as well as convolutional and transformer networks built on top of the generated images. Extensive experiments conducted on different challenging datasets, including CIFAR100 and ImageNet show the highly balanced discrimination and generative properties of our proposed NF models and their outperformance w.r.t. the closely related work.",https://openaccess.thecvf.com/content/ACCV2024/html/Enescu_Learning_Classwise_Untangled_Continuums_for_Conditional_Normalizing_Flows_ACCV_2024_paper.html
Full-body Human De-lighting with Semi-Supervised Learning,"Removing undesired shading from human images is crucial in supporting various real-world applications. While recent advancements in deep learning-based methods show promise in addressing this challenge, there persists a struggle to accurately separate texture from shading, which often results in unresolved shading artifacts and altered texture patterns. This issue is exacerbated by dataset limitations, such as the lack of diverse real-world clothing styles in realistic datasets and oversimplified assumptions about human reflectance and illumination environments. To address this problem, our paper introduces a novel semi-supervised deep learning method to effectively assemble both real and synthetic data for better disentanglement of texture and shading. We present a global sparsity constraint designed on both labeled and unlabeled data to minimize color variations in the inferred shading map, enhancing texture recovery. By applying this constraint, our method demonstrates improved handling of a broad range of fashion-related textures in the real-world test. Additionally, we address the disparity between real and synthetic data with a novel domain adaptation module to realize effective transfer from synthetic to real images. This module is designed based on the insights of gamma correction, and demonstrates improved shadow removal in real-world images. By integrating these methods, our approach achieves state-of-the-art results, reducing unwanted shading artifacts while maintaining the integrity of underlying textures in real-world scenarios.",https://openaccess.thecvf.com/content/ACCV2024/html/Weir_Full-body_Human_De-lighting_with_Semi-Supervised_Learning_ACCV_2024_paper.html
Parameter-Selective Continual Test-Time Adaptation,"Continual Test-Time Adaptation (CTTA) aims to adapt a pretrained model to ever-changing environments during the test time under continuous domain shifts. Most existing CTTA approaches are based on the Mean Teacher (MT) structure, which contains a student and a teacher model, where the student is updated using the pseudo-labels from the teacher model, and the teacher is then updated by exponential moving average strategy. However, these methods update the MT model indiscriminately on all parameters of the model. That is, some critical parameters involving sharing knowledge across different domains may be erased, intensifying error accumulation and catastrophic forgetting. In this paper, we introduce Parameter-Selective Mean Teacher (PSMT) method, which is capable of effectively updating the critical parameters within the MT network under domain shifts. First, we introduce a selective distillation mechanism in the student model, which utilizes past knowledge to regularize new knowledge, thereby mitigating the impact of error accumulation. Second, to avoid catastrophic forgetting, in the teacher model, we created a mask through Fisher information to selectively update parameters via exponential moving average, with preservation measures applied to crucial parameters. Extensive experimental results verify that PSMT outperforms state-of-the-art methods across multiple benchmark datasets. Our code is available at https://github.com/JiaxuTian/PSMT.",https://openaccess.thecvf.com/content/ACCV2024/html/Tian_Parameter-Selective_Continual_Test-Time_Adaptation_ACCV_2024_paper.html
EDAF: Early Detection of Atrial Fibrillation from Post-Stroke Brain MRI,"Atrial fibrillation (AF) is a common cause of ischemic stroke, accounting for up to one-third of all cases. Untreated AF can increase the risk of stroke by up to five times and make stroke recurrence more likely. Anticoagulation has proven beneficial in reducing stroke risk. However, AF is often paroxysmal and asymptomatic, remaining undetected and undiagnosed in up to 30% of cases. The current methods for AF detection are usually lengthy (cardiac monitoring), expensive (smart devices), or invasive (implantable cardiac monitors), limiting their routine use. We present a novel method to screen for AF by analyzing infarct patterns of stroke patients from brain magnetic resonance imaging (MRI) scans. We propose EDAF, a novel method based on the segment anything model (SAM) that leverages the power of a foundational deep learning model to efficiently analyze brain MRI and identify whether the underlying stroke etiology is AF. EDAF is trained and validated using a retrospectively acquired dataset of 235 post-stroke patients, achieving an area under the receiver operating characteristic (AUROC) of 83.08% Kui  2.96% in identifying the presence of AF. EDAF can achieve optimal solutions with minimal training, highlighting its potential for use in low-resource settings. As MRI is readily available in stroke centers and routinely performed on many patients after a stroke, either during their admission or as an outpatient, the proposed method can effectively identify patients for further AF investigation.",https://openaccess.thecvf.com/content/ACCV2024/html/Shokri_EDAF_Early_Detection_of_Atrial_Fibrillation_from_Post-Stroke_Brain_MRI_ACCV_2024_paper.html
Chinese Character Component Segmentation Based on Character Structure Masks,"To address the issue where rectangular anchor boxes in object detection-based Chinese character component segmentation cannot segment semi-enclosed Chinese characters, this paper proposes a method for segmenting Chinese character components based on Chinese character structure masks. This method utilizes a U-Net encoder with ResNet as the backbone network, transforming the segmentation of Chinese character components into the generation of Chinese character structure masks. First, this study proposes a Res-CBAM module, which leverages the structural features of Chinese characters by incorporating CBAM into the residual U-Net network, effectively solving the problem of incomplete segmentation of Chinese character components. Secondly, a vector-guided supervision mechanism is designed to guide the training process of the model by designing structure vectors of Chinese characters, effectively addressing the issue of component adhesion in Chinese characters. Experimental results demonstrate that compared to traditional object detection methods, this method can achieve fast and efficient segmentation in lightweight networks by training small datasets.",https://openaccess.thecvf.com/content/ACCV2024/html/Li_Chinese_Character_Component_Segmentation_Based_on_Character_Structure_Masks_ACCV_2024_paper.html
MeshGS: Adaptive Mesh-Aligned Gaussian Splatting for High-Quality Rendering,"Recently, 3D Gaussian splatting has gained attention for its capability to generate high-fidelity rendering results. At the same time, most applications such as games, animation, and AR/VR use mesh-based representations to represent and render 3D scenes. We propose a novel approach that integrates mesh representation with 3D Gaussian splats to perform high-quality rendering of reconstructed real-world scenes. In particular, we introduce a distance-based Gaussian splatting technique to align the Gaussian splats with the mesh surface and remove redundant Gaussian splats that do not contribute to the rendering. We consider the distance between each Gaussian splat and the mesh surface to distinguish between tightly-bound and loosely-bound Gaussian splats. The tightly-bound splats are flattened and aligned well with the mesh geometry. The loosely-bound Gaussian splats are used to account for the artifacts in reconstructed 3D meshes in terms of rendering. We present a training strategy of binding Gaussian splats to the mesh geometry, and take into account both types of splats. In this context, we introduce several regularization techniques aimed at precisely aligning tightly-bound Gaussian splats with the mesh surface during the training process. We validate the effectiveness of our method on large and unbounded scene from mip-NeRF 360 and Deep Blending datasets. Our method surpasses recent mesh-based neural rendering techniques by achieving a 2dB higher PSNR, and outperforms mesh-based Gaussian splatting methods by 1.3 dB PSNR, particularly on the outdoor mip-NeRF 360 dataset, demonstrating better rendering quality. We provide analyses for each type of Gaussian splat and achieve a reduction in the number of Gaussian splats by 30% compared to the original 3D Gaussian splatting.",https://openaccess.thecvf.com/content/ACCV2024/html/Choi_MeshGS_Adaptive_Mesh-Aligned_Gaussian_Splatting_for_High-Quality_Rendering_ACCV_2024_paper.html
Learning Interval-Aware Embedding for Macro- and Micro-expression Spotting,"Spotting the start and end frames of macro- and micro-expression in untrimmed long videos(i.e. Macro- and Micro-Expression Spotting, shorted by M^2ES) is extremely challenging due to the significant interval scale variations. Leading works borrowed the idea of ""anchor"" from temporal action localization into M^2ES, and achieved great improvements because of the finer proposal generation. However, covering diverse intervals is challenging for anchor-based methods due to latent domain shifts between macro- and micro-expression instances. Instead, we propose a purely anchor-free method for M^2ES, which eliminates the setting of redundant hyperparameters, and is both efficient and effective. In this work, we explore an Interval-aware Embedding Network (IAENet), which first exploits a basic two-stream network as the backbone to extract spatial and temporal feature embeddings from videos and optical flows, then a carefully designed temporal pyramid module is used to process interval-specific macro- and micro-expression instances in a parallel manner through a novel temporal attention mechanism and cross-scale feature fusion modules. We further design an interval-aware proposal generation scheme to specialize each spotting branch by sampling instances of proper intervals during training and inference. Extensive experiments demonstrate that our method beats all existing technologies, including interval-based and frame-based methods, with state-of-the-art results on the CAS(ME)^2 dataset and competitive results on the SAMM-LV dataset.",https://openaccess.thecvf.com/content/ACCV2024/html/Li_Learning_Interval-Aware_Embedding_for_Macro-_and_Micro-expression_Spotting_ACCV_2024_paper.html
Learning Complementary Maps for Light Field Salient Object Detection,"Light field imaging presents a promising avenue for advancing salient object detection (SOD). However, existing light field SOD (LFSOD) methods grapple with challenges related to effectively aggregating features from all-in-focus (AiF) images and focal slices. These methods often under-utilize the complementary nature of salient and non-saliency maps, leading to inaccurate predictions, particularly at fine boundaries. To tackle these limitations, in this paper, we introduce a novel method for LFSOD. Our method incorporates a Cross-Modality Aggregation (CMA) module at multiple levels, facilitating the efficient fusion of AiF image and focal slice features. This progressive aggregation capitalizes on global and local dependencies to harness implicit geometric information in an LF. Based on the observation that, salient regions and non-salient counterparts are complementary to each other, thus a better estimation on one side leads to an improved estimation on the other, and vice versa, we introduce the Complementary Saliency Map Generator (CSMG). The CSMG generates both saliency and non-saliency maps interactively to leverage the inherent complementary relationship between salient regions and their non-salient counterparts. Through extensive experiments conducted on benchmark datasets, we have demonstrated that our proposed method achieves superior performance in LFSOD.",https://openaccess.thecvf.com/content/ACCV2024/html/Xiao_Learning_Complementary_Maps_for_Light_Field_Salient_Object_Detection_ACCV_2024_paper.html
MS-UMLP: Medical Image Segmentation via  Multi-Scale U-shape MLP-Mixer,"With the emergence and rapid development of Transformers, medical image segmentation has also been revolutionized by Transformers due to their ability to encode long-range dependencies. Despite their advantages, Transformers also come with some drawbacks, such as larger models being built, resulting in more parameters being introduced. In some cases, several times the parameters may only result in marginal improvements. Additionally, medical segmentation images typically consist of multiple classes, with significant differences in size among classes and minimal differences within each class, which can be addressed via a multiple-scale model. In this paper, we proposed a novel Multi-Scale U-shape MLP-Mixer network named MS-UMLP, which aims to achieve multiple-scale receptive fields while using fewer parameters. Unlike the prevailing transformer-based trend of building models with more parameters, our MS-UMLP adopts dimension-wise multi-scale MLP-Mixer blocks via redesigning MLP-Mixer to reduce model parameters and computational complexity, retain the ability to exploit long-term dependencies, and provide the ability to capture the different scale information in each block. Extensive experiments show that our MS-UMLP not only has the least number of parameters (only 48% parameters of a pure convolutional network) but also outperforms existing methods on the popular ACDC and Synapse medical image segmentation datasets.",https://openaccess.thecvf.com/content/ACCV2024/html/Xie_MS-UMLP_Medical_Image_Segmentation_via__Multi-Scale_U-shape_MLP-Mixer_ACCV_2024_paper.html
A StyleCLIP-based  Facial Emotion Manipulation Method for Discrepant Emotion Transitions,"Leveraging StyleCLIP's expressivity and its disentangled latent codes, current methodologies enable facial emotion manipulation through textual inputs. Despite these advancements, significant challenges remain in manipulating target emotions that deviate markedly from the originals without introducing artifacts or errors. This paper introduces a novel approach for discrepant emotion transitions. Our network architecture integrates a StyleGAN2 generator with an Emotion Manipulation Mapper, a Dual Auxiliary Classifier, and a CLIP Text Encoder. By utilizing the inverse cumulative distribution function, we convert source emotion labels into conditional data, thus enhancing the models ability to accurately map and modify the emotional distribution across faces. We evaluate our method against established techniques using the Radboud Faces Database and the CelebA-HQ dataset, and introduced a new quantitative measure including seven metrics for assessing manipulation efficacy.",https://openaccess.thecvf.com/content/ACCV2024/html/Guo_A_StyleCLIP-based__Facial_Emotion_Manipulation_Method_for_Discrepant_Emotion_ACCV_2024_paper.html
Few Exemplar-Based General Medical Image Segmentation via Domain-Aware Selective Adaptation,"Medical image segmentation poses challenges due to domain gaps, data modality variations, and dependency on domain knowledge or experts, especially for low- and middle-income countries (LMICs). Whereas for humans, given a few exemplars (with corresponding labels), we are able to segment different medical images even without extensive domain-specific clinical training. In addition, current SAM-based medical segmentation models use fine-grained visual prompts, such as the maximum bounding rectangle generated from manually annotated lesion area segmentation masks, as bounding box prompt during the testing phase. However, in actual clinical diagnosis, no prior konwledge is avaiable for such fine-grained visual prompt. Our experimental results also reveal that previoums models nearly fail to predict when given coarser bbox prompts. Considering these drawbacks, we propose a domain-aware selective adaptation approach to adapt the general knowledge learned from a large model trained with natural images to the corresponding medical domains/modalities with access to only a few (e.g., less than 5) exemplars. Our method mitigates the aforementioned limitations, providing an efficient and LMICs-friendly solution. Extensive experimental analysis showcases the effectiveness of our approach, offering potential advancements in healthcare diagnostics and clinical applications in LMICs.",https://openaccess.thecvf.com/content/ACCV2024/html/Xu_Few_Exemplar-Based_General_Medical_Image_Segmentation_via_Domain-Aware_Selective_Adaptation_ACCV_2024_paper.html
Tracking Reflected Objects: A Benchmark,"Visual tracking has made great progress recently, thanks to large-scale training datasets. These datasets have enabled the development of highly accurate tracking algorithms. However, most research focuses on tracking general objects, with less attention on specialized challenges like tracking reflected objects. Reflections can distort objects' appearances, making them harder to track, which is a critical issue in areas like autonomous driving, security, smart homes, and industrial settings where tracking reflections in mirrors or glass is important. To fill this gap, we introduce TRO, a benchmark designed for Tracking Reflected Objects. TRO contains 200 sequences with about 70,000 frames, all annotated with bounding boxes, aiming to foster the development of methods tailored for this challenging task. We tested 20 state-of-the-art trackers and found they struggle with the complexities of reflections. To set a better baseline, we propose HIP-HaTrack, a new tracker that uses hierarchical features and outperforms existing algorithms. We hope our benchmark and HIP-HaTrack will inspire further research on tracking reflected objects. The TRO and code are available at https://github.com/Guoxiaoyuy/HIP-HaTrack.",https://openaccess.thecvf.com/content/ACCV2024/html/Guo_Tracking_Reflected_Objects_A_Benchmark_ACCV_2024_paper.html
CrossPAR: Enhancing Pedestrian Attribute Recognition with Vision-Language Fusion and Human-Centric Pre-training,"Pedestrian attribute recognition (PAR) is crucial in various applications like surveillance and urban planning. Accurately identifying attributes in diverse and intricate urban environments is challenging despite its significance. This paper introduces a novel network for PAR that integrates a human-centric encoder, trained on extensive human datasets, with a vision-language encoder, trained on substantial text-image pair datasets. We also develop a cross-attention mechanism utilizing a Mixture-of-Experts approach that combines the human-centric encoder's proficiency in local attribute detection with the vision-language encoder's ability to comprehend global content. CrossPAR showcases a comparable accuracy result compared to existing techniques across multiple benchmarks, using less training data. These results confirm our approach's effectiveness and suggest promising avenues for further research and practical applications in the domain of PAR and related fields.",https://openaccess.thecvf.com/content/ACCV2024/html/Ngo_CrossPAR_Enhancing_Pedestrian_Attribute_Recognition_with_Vision-Language_Fusion_and_Human-Centric_ACCV_2024_paper.html
GSMNet: Towards Long-term Trajectory Prediction by Integrating Multi-Scale Information,"Predicting the future trajectories of pedestrians is a vital task for many applications, such as autonomous driving and robot navigation. Most existing methods only predict short-term trajectories. In this paper, we challenge the problem of long-term trajectory prediction. Different from short-term prediction which focus most on the local information, long-term prediction needs to model future trajectory with multi-scale information hierarchically from the multimodal global destination, to mid-distance scene layout limitation, other agent movement and finally the local history motion pattern. The destination reflects pedestrian long-term multimodal goal, the scene layout along with interaction constrains the possible path choice, and history motion pattern guides the future movement. We propose GSMNet, which achieves effective long-term trajectory prediction by integrating multi-scale factors: multimodal goals, scene interaction and motion patterns. We design separate modules to extract different scale features. Multi-layer-perceptron extracts the local-scale feature from history motion pattern. U-Net with attention captures the mid-scale pedestrian-scene correlation feature and goal feature with scene layout at global-scale. Finally, combining multi-scale feature to predict future trajectories. Experiments on SDD dataset and ETH-UCY dataset show that proposed GSMNet outperforms the previous state-of-the-art for both long-term and short-term trajectory prediction task. Qualitative results show GSMNet generates more reasonable trajectories.",https://openaccess.thecvf.com/content/ACCV2024/html/Liu_GSMNet_Towards_Long-term_Trajectory_Prediction_by_Integrating_Multi-Scale_Information_ACCV_2024_paper.html
Structure-Centric Robust Monocular Depth Estimation via Knowledge Distillation,"Monocular depth estimation, enabled by self-supervised learning, is a key technique for 3D perception in computer vision. However, it faces significant challenges in real-world scenarios, which encompass adverse weather variations, motion blur, as well as scenes with poor lighting conditions at night. Our research reveals that we can divide monocular depth estimation into three sub-problems: depth structure consistency, local texture disambiguation, and semantic-structural correlation. Our approach tackles the non-robustness of existing self-supervised monocular depth estimation models to interference textures by adopting a structure-centered perspective and utilizing the scene structure characteristics demonstrated by semantics and illumination. We devise a novel approach to reduce over-reliance on local textures, enhancing robustness against missing or interfering patterns. Additionally, we incorporate a semantic expert model as the teacher and construct inter-model feature dependencies via learnable isomorphic graphs to enable aggregation of semantic structural knowledge. Our approach achieves state-of-the-art out-of-distribution monocular depth estimation performance across a range of public adverse scenario datasets. It demonstrates notable scalability and compatibility, without necessitating extensive model engineering. This showcases the potential for customizing models for diverse industrial applications.",https://openaccess.thecvf.com/content/ACCV2024/html/Chen_Structure-Centric_Robust_Monocular_Depth_Estimation_via_Knowledge_Distillation_ACCV_2024_paper.html
High-Quality Visually-Guided Sound Separation from Diverse Categories,"We propose DAVIS, a Diffusion-based Audio-VIusal Separation framework that solves the audio-visual sound source separation task through generative learning. Existing methods typically frame sound separation as a mask-based regression problem, achieving significant progress. However, they face limitations in capturing the complex data distribution required for high-quality separation of sounds from diverse categories. In contrast, DAVIS leverages a generative diffusion model and a Separation U-Net to synthesize separated sounds directly from Gaussian noise, conditioned on both the audio mixture and the visual information. With its generative objective, DAVIS is better suited to achieving the goal of high-quality sound separation across diverse sound categories. We compare DAVIS to existing state-of-the-art discriminative audio-visual separation methods on the AVE and MUSIC datasets, and results show that DAVIS outperforms other methods in separation quality, demonstrating the advantages of our framework for tackling the audio-visual source separation task. We will make our source code and pre-trained models publicly available.",https://openaccess.thecvf.com/content/ACCV2024/html/Huang_High-Quality_Visually-Guided_Sound_Separation_from_Diverse_Categories_ACCV_2024_paper.html
FSGait: Fine Grained Self-Supervised Gait Abnormality Detection,"Gait Abnormality Detection (GAD) plays an important role in diagnosing diseases associated with abnormal gait patterns. However, existing works are limited in generalization capability and granularity, which indicates they detect only the types of abnormalities seen during training and sequence-level gait anomalies. The reason is the restricted variety of abnormalities in datasets and the reliance on supervised learning algorithms. Therefore, we propose a Fine-grained Self-supervised Gait Abnormality Detection method (FSGait). We divide gait abnormality into two sub-problems: postural anomaly and temporal anomaly, which are solved by two designed modules, Gait Reconstruction Module (GRM) and Gait Prediction Module (GPM).  The two modules are trained in self-supervised way on normal gait data. In this way, they capture normal gait patterns to distinguish abnormalities, thereby enhancing the generalization capability. For fine-grained detection, three-level (Sequence, Frame and Joint) abnormal detections are achieved with the intermediate results of these two modules. FSGait has a high degree of granularity and holds significant potential for aiding medical diagnosis and automating disease detection. Experiments on two datasets show that FSGait achieves state-of-the-art performance in frame-level GAD, while maintaining high sequence-level GAD accuracy. The joint-level detection results are presented with visualization.",https://openaccess.thecvf.com/content/ACCV2024/html/Duan_FSGait_Fine_Grained_Self-Supervised_Gait_Abnormality_Detection_ACCV_2024_paper.html
Spotlight on Small-scale Ship Detection: Empowering YOLO with Advanced Techniques and a Novel Dataset,"In recent years, significant advancements have been made in deep learning-based ship detection methods on the ocean surface. However, publicly available maritime datasets that include categories for small-scale ships and network frameworks optimized explicitly for small-scale ship detection on the ocean surface are still limited in availability. To address the data scarcity in small-scale ship detection, bridge the gap between small-scale ship detection and general object detection, and mitigate the impact of small objects on maritime safety, we collect a multi-scale dataset with a particular emphasis on detecting small objects on the ocean surface, named the iShip-1. Leveraging this dataset, we train the S^3Det for small-scale ship detection, which remarkably detects small-scale ships on the ocean surface. Specifically, the iShip-1 comprises 17,236 images encompassing six categories captured from multiple perspectives and under various weather conditions. Notably, the Other Ship category focuses explicitly on small-scale ships. The S^3Det is optimized for detecting small-scale ships through improved backbone and neck architecture. It employs the NWD Loss instead of the traditional IoU Loss and utilizes the Feedback Cut&Paste technique for effective data augmentation. We evaluate the performance of the S^3Det on both the Seaships and iShip-1. For small-scale ship detection, S^3Det achieved a recall rate of 68.9%, a mAP50 of 73.9%, and a mAP50:90 of 39.4%. These results indicate improvements of 5.9%, 2%, and 1.2% compared to the original YOLOv8 model, respectively. Our code and dataset are available here https://github.com/li01233/S3Det.",https://openaccess.thecvf.com/content/ACCV2024/html/Li_Spotlight_on_Small-scale_Ship_Detection_Empowering_YOLO_with_Advanced_Techniques_ACCV_2024_paper.html
Foundation Model-Powered 3D Few-Shot Class Incremental Learning via Training-free Adaptor,"Recent advances in deep learning for processing point clouds hold increased interest in Few-Shot Class Incremental Learning (FSCIL) for 3D computer vision. This paper introduces a new method to tackle the Few-Shot Continual Incremental Learning (FSCIL) problem in 3D point cloud environments. We leverage a foundational 3D model trained extensively on point cloud data. Drawing from recent improvements in foundation models, known for their ability to work well across different tasks, we propose a novel strategy that does not require additional training to adapt to new tasks. Our approach uses a dual cache system: first, it uses previous test samples based on how confident the model was in its predictions to prevent forgetting, and second, it includes a small number of new task samples to prevent overfitting. This dynamic adaptation ensures strong performance across different learning tasks without needing lots of fine-tuning. We tested our approach on datasets like ModelNet, ShapeNet, ScanObjectNN, and CO3D, showing that it outperforms other FSCIL methods and demonstrating its effectiveness and versatility. The code is available at https://github.com/ahmadisahar/ACCV_FCIL3D.",https://openaccess.thecvf.com/content/ACCV2024/html/Ahmadi_Foundation_Model-Powered_3D_Few-Shot_Class_Incremental_Learning_via_Training-free_Adaptor_ACCV_2024_paper.html
Calibration Transfer via Knowledge Distillation,"Modern deep neural networks often suffer from miscalibration, leading to overly confident errors that undermine their reliability. Although Knowledge Distillation (KD) is known to improve student classifier accuracy, its impact on model calibration remains unclear. It is generally assumed that well-calibrated teachers produce well-calibrated students. However, previous findings indicate that teachers calibrated with label smoothing (LS) result in less accurate students. This paper explores the theoretical foundations of KD, revealing that prior results are artifacts of specific calibration methods rather than KD itself. Our study shows that calibrated teachers can effectively transfer calibration to their students, but not all training regimes are equally effective. Notably, teachers calibrated using dynamic label smoothing methods yield better-calibrated student classifiers through KD. We also show that transfer of calibration can be induced from lower capacity teachers to larger capacity students. The proposed KD based Calibration framework, named KD(C), leads to a state-of-the-art calibration results. More specifically, on CIFAR100 using WRN-40-1 feature extractor, we report an ECE of 0.98 compared to 7.61, 7.00, and 2.1 by the current SOTA calibration techniques, Adafocal, ACLS , and CPC respectively, and 11.16 by the baseline NLL loss (lower ECE is better).",https://openaccess.thecvf.com/content/ACCV2024/html/Hebbalaguppe_Calibration_Transfer_via_Knowledge_Distillation_ACCV_2024_paper.html
TuneVLSeg: Prompt Tuning Benchmark for Vision-Language Segmentation Models,"Vision-Language Models (VLMs) have shown impressive performance in vision tasks, but adapting them to new domains often requires expensive fine-tuning. Prompt tuning techniques, including textual, visual, and multimodal prompting, offer efficient alternatives by leveraging learnable prompts. However, their application to Vision-Language Segmentation Models (VLSMs) and evaluation under significant domain shifts remain unexplored. This work presents an open-source benchmarking framework, TuneVLSeg, to integrate various unimodal and multimodal prompt tuning techniques into VLSMs, making prompt tuning usable for downstream segmentation datasets with any number of classes. We test various prompt tuning on 8 diverse medical datasets, including 3 radiology datasets (breast tumor, echocardiograph, chest X-ray pathologies) and 5 non-radiology datasets (polyp, ulcer, skin cancer), and two natural domain segmentation datasets. Our study found that textual prompt tuning struggles under significant domain shifts, from natural-domain images to medical data. Furthermore, visual prompt tuning, with fewer hyperparameters than multimodal prompt tuning, often achieves performance competitive to multimodal approaches, making it a valuable first attempt. Our work advances the understanding and applicability of different prompt-tuning techniques for robust domain-specific segmentation. The source code is available at https://github.com/naamiinepal/tunevlseg.",https://openaccess.thecvf.com/content/ACCV2024/html/Adhikari_TuneVLSeg_Prompt_Tuning_Benchmark_for_Vision-Language_Segmentation_Models_ACCV_2024_paper.html
DENEB: A Hallucination-Robust Automatic Evaluation Metric for Image Captioning,"In this work, we address the challenge of developing automatic evaluation metrics for image captioning, with a particular focus on robustness against hallucinations. Existing metrics are often insufficient for handling hallucinations, primarily due to their limited ability to compare candidate captions with multifaceted reference captions. To address this shortcoming, we propose DENEB, a novel supervised automatic evaluation metric specifically robust against hallucinations. DENEB incorporates the Sim-Vec Transformer, a mechanism that processes multiple references simultaneously, thereby efficiently capturing the similarity between an image, a candidate caption, and reference captions. To train DENEB, we construct the diverse and balanced Nebula dataset comprising 32,978 images, paired with human judgments provided by 805 annotators. We demonstrated that DENEB achieves state-of-the-art performance on the FOIL, Composite, Flickr8K-Expert, Flickr8K-CF, Nebula, and PASCAL-50S datasets, validating its effectiveness and robustness against hallucinations.",https://openaccess.thecvf.com/content/ACCV2024/html/Matsuda_DENEB_A_Hallucination-Robust_Automatic_Evaluation_Metric_for_Image_Captioning_ACCV_2024_paper.html
EgoCoord: Self-calibrated Egocentric 3D Body Pose Estimation using Pixel-wise Coordinate Encoding,"The primary challenges for egocentric 3D human pose estimation techniques are the perspective and radial distortions introduced by fisheye lenses. Previous methods utilized camera calibration for undistortion or utilized neural networks to regress 3D human poses from distorted 2D poses. In this paper, we propose a novel approach that integrates a pixel-wise coordinate encoding technique for recognizing image distortion and utilizes the Vision Transformer (ViT) to extract distortion and pose tokens from the input image. The extracted tokens are used in a 3D volumetric heatmap-based egocentric pose estimator, which predicts the 3D human pose using pose tokens and performs pose correction using distortion tokens. The approach integrates CoordConv's positional encoding strategies, neural network-based camera calibration methods, and the volumetric heatmap-based 3D human pose estimation method. We evaluate the proposed model's performance using our new evaluation dataset and compare it with state-of-the-art models. Additionally, we perform an ablation study to demonstrate the individual effects of each module in the proposed model.",https://openaccess.thecvf.com/content/ACCV2024/html/Lee_EgoCoord_Self-calibrated_Egocentric_3D_Body_Pose_Estimation_using_Pixel-wise_Coordinate_ACCV_2024_paper.html
Match-free Inbetweening Assistant (MIBA): A Practical Animation Tool without User Stroke Correspondence,"In traditional 2D frame-by-frame animation, inbetweening (interpolating line drawings, abbr. ""IB"") is still a manual and labor-intensive task. Despite the abundance of literature and software offering automation and claiming speedups, animators and the industry as a whole have been hesitant to adopt these new tools. Upon inspection, we find prior work often unreasonably expects adoption of novel stroke-matching workflows, naively assumes access to adequate center-line vectorization, and lacks rigorous evaluation with professional users on real production data. Facing these challenges, we leverage optical flow estimation and differentiable vector graphics to design a ""Match-free Inbetweening Assistant"" (MIBA). Unrestricted by the need for user stroke correspondence, MIBA integrates into the existing IB workflow without introducing additional requirements, and makes the raster input case feasible thanks to its robustness to vectorization quality. MIBA's simplicity and effectiveness is demonstrated in our comprehensive user study, where users with professional IB experience achieved 4.2x average speedup and better chamfer distance scores on real-world production data, given only a 5-minute tutorial of new functionality.",https://openaccess.thecvf.com/content/ACCV2024/html/Chen_Match-free_Inbetweening_Assistant_MIBA_A_Practical_Animation_Tool_without_User_ACCV_2024_paper.html
LCM: Log Conformal Maps for Robust Representation Learning to Mitigate Perspective Distortion,"Perspective distortion (PD) leads to substantial alterations in the shape, size, orientation, angles, and spatial relationships of visual elements in images. Accurately determining camera intrinsic and extrinsic parameters is challenging, making it hard to synthesize perspective distortion effectively. The current distortion correction methods involve removing distortion and learning vision tasks, thus making it a multi-step process, often compromising performance. Recent work leverages the MYi bius transform for mitigating perspective distortions (MPD) to synthesize perspective distortions without estimating camera parameters. MYi bius transform requires tuning multiple interdependent and interrelated parameters and involving complex arithmetic operations, leading to substantial computational complexity. To address these challenges, we propose Log Conformal Maps (LCM), a method leveraging the logarithmic function to approximate perspective distortions with fewer parameters and reduced computational complexity. We provide a detailed foundation complemented with experiments to demonstrate that LCM with fewer parameters approximates the MPD. We show that LCM integrates well with supervised and self-supervised representation learning, outperform standard models, and matches the state-of-the-art performance in mitigating perspective distortion over multiple benchmarks, namely Imagenet-PD, Imagenet-E, and Imagenet-X. Further LCM demonstrate seamless integration with person re-identification and improved the performance. Source code is made publicly available at https://github.com/meenakshi23/Log-Conformal-Maps.",https://openaccess.thecvf.com/content/ACCV2024/html/Chippa_LCM_Log_Conformal_Maps_for_Robust_Representation_Learning_to_Mitigate_ACCV_2024_paper.html
Reference-Based Face Super-Resolution Using the Spatial Transformer,"Face super-resolution is the task of increasing the resolution of an image containing a face thereby adding finer detail. It is a ubiquitous task in many computer vision applications and quite often the user isnt even aware that it is being performed. However, doing it with high fidelity is challenging as it is an ill-posed problem. In this paper we present a reference-based solution for face super-resolution that uses higher resolution reference images to aid in the task. We show an alignment module based on the spatial transformer that is considerably more stable than the popular deformable convolutions. We also show an aggregation function that can take good quality information from the reference images when available or suppress the function when such information is unavailable. Finally, we show that our relatively smaller model can achieve state of the art results on multiple datasets. The source code is available at https://github.com/varun-jois/FSRST.",https://openaccess.thecvf.com/content/ACCV2024/html/Jois_Reference-Based_Face_Super-Resolution_Using_the_Spatial_Transformer_ACCV_2024_paper.html
Tracking Correction Method for Rapid and Random Protein Molecules Movement,"In recent years, there has been a increasing demand for tracking protein molecules with the focus on immune system researches. However, machine learning-based single-particle tracking (SPT) faces the challenges in accuracy due to the rapid and random movement of molecules as well as detection errors. To address these issues, we use frame interpolation to pseudo-decrease the speed of movement and perform two-stage matching to achieve stable tracking. We also use an optimization algorithm that concatenates short tracks. This approach has achieved higher performance on the CD47 dataset and the PTC dataset than conventional baselines.",https://openaccess.thecvf.com/content/ACCV2024/html/Kamiya_Tracking_Correction_Method_for_Rapid_and_Random_Protein_Molecules_Movement_ACCV_2024_paper.html
PSG-Adapter: Controllable Planning Scene Graph for Improving Text-to-Image Diffusion,"Significant progress in text-to-image generation has been driven by the application of diffusion models, highlighting their crucial role and exceptional impact on this field. However, diffusion models often fall short of comprehending spatial relationships within text. This limitation primarily stems from their challenge in constructing logical spatial relationships, such as distinguishing between foreground and background elements. Additionally, their limited text encoding capacity exacerbates inconsistencies in the generated images derived from textual prompts. In this paper, we introduce the Planning Scene Graph Adapter (PSG-Adapter). Our approach employs Planning Scene Graph (PSG) method to decompose the original text prompt into distinct sub-prompts containing spatial relationships. By leveraging the proposed Planning Scene Graph ControlNet (PSG-ControlNet), additional spatial information is infused into original text embeddings. By fully exploiting the implicit spatial relationships within the text, our method achieves fine-grained control over the composition of the generated images. This enhancement is particularly notable in scenarios involving the generation of multiple objects and complex spatial relationships. Extensive experiments have been conducted to verify the efficacy of PSG-Adapter in generating spatially coherent images and complex scenes with multiple objects and relationships.",https://openaccess.thecvf.com/content/ACCV2024/html/Gao_PSG-Adapter_Controllable_Planning_Scene_Graph_for_Improving_Text-to-Image_Diffusion_ACCV_2024_paper.html
BoT-FaceSORT: Bag-of-Tricks for Robust Multi-Face Tracking in Unconstrained Videos,"Multi-face tracking (MFT) is a subtask of multi-object tracking (MOT) that focuses on detecting and tracking multiple faces across video frames. Modern MOT trackers adopt the Kalman filter (KF), a linear model that estimates current motions based on previous observations. However, these KF-based trackers struggle to predict motions in unconstrained videos with frequent shot changes, occlusions, and appearance variations. To address these limitations, we propose BoT-FaceSORT, a novel MFT framework that integrates shot change detection, shared feature memory, and an adaptive cascade matching strategy for robust tracking. It detects shot changes by comparing the color histograms of adjacent frames and resets KF states to handle discontinuities. Additionally, we introduce MovieShot, a new benchmark of challenging movie clips to evaluate MFT performance in unconstrained scenarios. We also demonstrate the superior performance of our method compared to existing methods on three benchmarks, while an ablation study validates the effectiveness of each component in handling unconstrained videos.",https://openaccess.thecvf.com/content/ACCV2024/html/Kim_BoT-FaceSORT_Bag-of-Tricks_for_Robust_Multi-Face_Tracking_in_Unconstrained_Videos_ACCV_2024_paper.html
"ADSP: Advanced Dataset for Shadow Processing, enabling visible occluders via synthesizing strategy.","Shadows can lead to malfunctions in computer vision, making shadow removal an essential task for restoring underlying information. For a long time, researchers have proposed hand-crafted methods based on observing shadow formation models. Then, deep-learning-based solutions have further advanced performance in restoration quality. However, existing datasets have several limitations, such as lacking occluders, restricted camera angles, and inconsistency. In this paper, a novel benchmark called the Advanced Dataset for Shadow Processing (ADSP) is introduced. Through the synthesizing strategy, the ADSP becomes the first dataset containing outdoor images with occluders. Statistical analysis and experiments demonstrate that the ADSP has the advantages of less domain shifting, matching real-world scenarios, and sufficient generalizing capability. Moreover, as a reference for the removal task, we also propose the Segmented Refinement Removal Network (SRRN), which includes three subnets for shadow removal, color adjustment, and boundary smoothing, respectively. It achieves state-of-the-art performance and can be set as a reference for shadow removal.",https://openaccess.thecvf.com/content/ACCV2024/html/Hsieh_ADSP_Advanced_Dataset_for_Shadow_Processing_enabling_visible_occluders_via_ACCV_2024_paper.html
ObjectCompose: Evaluating Resilience of Vision-Based Models on Object-to-Background Compositional Changes,"Given the large-scale multi-modal training of recent vision-based models and their generalization capabilities, understanding the extent of their robustness is critical for their real-world deployment. In this work, our goal is to evaluate the resilience of current vision-based models against diverse object-to-background context variations. The majority of robustness evaluation methods have introduced synthetic datasets to induce changes to object characteristics (viewpoints, scale, color) or utilized image transformation techniques (adversarial changes, common corruptions) on real images to simulate shifts in distributions. Recent works have explored leveraging large language models and diffusion models to generate changes in the background. However, these methods either lack in offering control over the changes to be made or distort the object semantics, making them unsuitable for the task. Our method, on the other hand, can induce diverse object-to-background changes while preserving the original semantics and appearance of the object. To achieve this goal, we harness the generative capabilities of text-to-image, image-to-text, and image-to-segment models to automatically generate a broad spectrum of object-to-background changes. We induce both natural and adversarial background changes by either modifying the textual prompts or optimizing the latents and textual embedding of text-to-image models. This allows us to quantify the role of background context in understanding the robustness and generalization of deep neural networks. We produce various versions of standard vision datasets (ImageNet, COCO), incorporating either diverse and realistic backgrounds into the images or introducing color, texture, and adversarial changes in the background. We conduct thorough experimentation and provide an in-depth analysis of the robustness of vision-based models against object-to-background context variations across different tasks. Our code and evaluation benchmark will be available at https://github.com/Muhammad-Huzaifaa/ObjectCompose.",https://openaccess.thecvf.com/content/ACCV2024/html/Malik_ObjectCompose_Evaluating_Resilience_of_Vision-Based_Models_on_Object-to-Background_Compositional_Changes_ACCV_2024_paper.html
Feature Estimation of Global Language Processing in EEG Using Attention Maps,"Understanding the correlation between EEG features and cognitive tasks is crucial for elucidating brain function. Brain activity synchronizes during speaking and listening tasks. However, it is challenging to estimate task-dependent brain activity characteristics with methods with low spatial resolution but high temporal resolution, such as EEG, rather than methods with high spatial resolution, like fMRI. This study introduces a novel approach to EEG feature estimation that utilizes the weights of deep learning models to explore this association. We demonstrate that attention maps generated from Vision Transformers and EEGNet effectively identify features that align with findings from prior studies. EEGNet emerged as the most accurate model regarding subject independence and the classification of Listening and Speaking tasks. The application of Mel-Spectrogram with ViTs enhances the resolution of temporal and frequency-related EEG characteristics. Our findings reveal that the characteristics discerned through attention maps vary significantly based on the input data, allowing for tailored feature extraction from EEG signals. By estimating features, our study reinforces known attributes and predicts new ones, potentially offering fresh perspectives in utilizing EEG for medical purposes, such as early disease detection. These techniques will make substantial contributions to cognitive neuroscience.",https://openaccess.thecvf.com/content/ACCV2024/html/Shimizu_Feature_Estimation_of_Global_Language_Processing_in_EEG_Using_Attention_ACCV_2024_paper.html
Efficient Implicit SDF and Color Reconstruction via Shared Feature Field,"Recent advancements in neural implicit 3D representations have enabled simultaneous surface reconstruction and novel view synthesis using only 2D RGB images. However, these methods often struggle with textureless and minimally visible areas. In this study, we introduce a simple yet effective encoder-decoder framework that encodes positional and viewpoint coordinates into a shared feature field (SFF). This feature field is then decoded into an implicit signed distance field (SDF) and a color field. By employing a weight-sharing encoder, we enhance the joint optimization of the SDF and color field, enabling better utilization of the limited information in the scene. Additionally, we incorporate a periodic sine function as an activation function, eliminating the need for a positional encoding layer and significantly reducing rippling artifacts on surfaces. Empirical results demonstrate that our method more effectively reconstructs textureless and minimally visible surfaces, synthesizes higher-quality novel views, and achieves superior multi-view reconstruction with fewer input images.",https://openaccess.thecvf.com/content/ACCV2024/html/Fang_Efficient_Implicit_SDF_and_Color_Reconstruction_via_Shared_Feature_Field_ACCV_2024_paper.html
BAMG: Text-based Person Re-identification via Bottlenecks Attention and Masked Graph Modeling,"In the realm of computer vision, traditional person re-identifi-cation (ReID) methods have primarily focused on matching pedestrian identities across varied cameras and temporal instances. Text-based Person Re-identification (TBPReID) extends these efforts by utilizing textual descriptions alongside images to enhance retrieval applications, such as tracking suspects or locating missing children. A Text-based Person Re-identification framework based on bottleneck attention and masked graph modeling(BAMG) is introduced in this paper, which incorporates the prowess of CLIP's pre-trained models into an advanced architecture. BAMG features a bottleneck fusion module for optimized modal integration, a Masked Graph Modeling (MGM) component for enhanced feature extraction, and additional supportive modules that refine the processing of multimodal data. BAMG not only enhances the alignment and interaction between text and image data but also significantly boosts the accuracy and robustness of the identification process. Through evaluations on the CUHK-PEDES dataset, the BAMG model has achieved a rank-1 accuracy of 79% and a mean average precision (mAP) of 68%. These results establish BAMG as a leading framework, setting new benchmarks for performance and adaptability in the field of multimodal learning environments focused on text-based person re-identification.",https://openaccess.thecvf.com/content/ACCV2024/html/Cheng_BAMG_Text-based_Person_Re-identification_via_Bottlenecks_Attention_and_Masked_Graph_ACCV_2024_paper.html
Nash Meets Wertheimer: Using Good Continuation in Jigsaw Puzzles,"Jigsaw puzzle solving is a challenging task for computer vision since it requires high-level spatial and semantic reasoning. To solve the problem, existing approaches invariably use color and/or shape information but in many real-world scenarios, such as in archaeological fresco reconstruction, this kind of clues is often unreliable due to severe physical and pictorial deterioration of the individual fragments. This makes state-of-the-art approaches entirely unusable in practice. On the other hand, in such cases, simple geometrical patterns such as lines or curves offer a powerful yet unexplored clue. In an attempt to fill in this gap, in this paper we introduce a new challenging version of the puzzle solving problem in which one deliberately ignores conventional color and shape features and relies solely on the presence of linear geometrical patterns. The reconstruction process is then only driven by one of the most fundamental principles of Gestalt perceptual organization, namely Wertheimer's law of good continuation. In order to tackle this problem, we formulate the puzzle solving problem as the problem of finding a Nash equilibrium of a (noncooperative) multiplayer game and use classical multi-population replicator dynamics to solve it. The proposed approach is general and allows us to deal with pieces of arbitrary shape, size and orientation. We evaluate our approach on both synthetic and real-world data and compare it with state-of-the-art algorithms. The results show the intrinsic complexity of our purely line-based puzzle problem as well as the relative effectiveness of our game-theoretic formulation.",https://openaccess.thecvf.com/content/ACCV2024/html/Khoroshiltseva_Nash_Meets_Wertheimer_Using_Good_Continuation_in_Jigsaw_Puzzles_ACCV_2024_paper.html
A Simple Finetuning Strategy Based on Bias-Variance Ratios of Layer-Wise Gradients,"Finetuning is an effective method for adapting pretrained networks to downstream tasks. However, the success of finetuning depends heavily on the selection of layers to be tuned, as full finetuning can lead to overfitting, while tuning only the last layer may not capture the necessary task-specific features. This requires a balanced approach of automatic layer selection to achieve higher performance. In this context, we propose the Bias-Variance Guided Layer Selection (BVG-LS), a simple yet effective strategy that adaptively selects a layer to be tuned at each training iteration. More specifically, BVG-LS computes the bias-variance ratios of mini-batch gradients for each layer and updates the parameters of the layer with the largest ratio. This strategy reduces the risk of overfitting while maintaining the model's capacity to learn task-specific features. In our experiments, we demonstrate the effectiveness of the BVG-LS strategy on seven image classification tasks. We show that BVG-LS outperforms full finetuning on all tasks with the WideResNet-50-2 model and on six out of seven tasks with the ViT-S model.",https://openaccess.thecvf.com/content/ACCV2024/html/Tomita_A_Simple_Finetuning_Strategy_Based_on_Bias-Variance_Ratios_of_Layer-Wise_ACCV_2024_paper.html
Adapting Models to Scarce Target Data without Source Samples,"When significant discrepancies exist in data distributions between source and target domains, source-trained models often exhibit suboptimal performance in the target domain. Unsupervised domain adaptation (UDA) effectively addresses this issue without needing labels of target data. More recent source-free UDA methods handle the situations where source data is inaccessible. However, the performance of UDA is substantially compromised when the target domain data is scarce. Despite the challenges in obtaining and storing large target data, this aspect of UDA has not been extensively investigated. Our study introduces a new method to alleviate performance degradation in source-free UDA under  target data scarcity. The proposed method retains the architecture and pretrained parameters of the source model, thereby reducing the risk of overfitting. Instead, it incorporates less than 3.3% of trainable parameters that comprise a set of convolution layers with non-linearity and a spatial attention network. Empirical assessments reveal that our approach achieves up to a 5.4% performance improvement with limited target data on VisDA benchmark over existing UDA methods. Similar trends are also evident in Office-31 benchmark and multi-source UDA experiments with Office-Home benchmark across different target domains. Our method shows promising enhancement of the adapted model's generalization. These findings highlight the efficacy of our method in improving UDA across diverse domain adaptation scenarios.",https://openaccess.thecvf.com/content/ACCV2024/html/Lee_Adapting_Models_to_Scarce_Target_Data_without_Source_Samples_ACCV_2024_paper.html
SSTHyper: Sparse Spectral Transformer for Hyperspectral Image Reconstruction,"Transformer-based methods have improved the quality of hyperspectral images (HSIs) reconstructed from RGB by effectively capturing their remote relationships. The self-attention mechanisms in existing Transformer models have not fully considered the spatial sparsity and spectral continuity characteristics of HSIs and fail to effectively filter out significant features, resulting in lower-quality reconstruction results. This paper proposes a sparse spectral transformer model for HSI reconstruction (SSTHyper) to address this limitation, adaptively preserving crucial features. The network consists of an encoder-decoder structure capable of learning shallow and deep spatial-spectral priors, primarily composed of sparse spectral self-attention groups. Introducing a sparse spectral self-attention mechanism allows adaptive masking of non-significant details, enhancing reconstruction accuracy. Meanwhile, a lightweight cross-level fusion network is proposed to reduce model parameters and computational costs to enhance spatial-spectral feature extraction. Experimental results on two benchmark datasets demonstrate the outstanding performance of the proposed method. The code will be released at https://github.com/MingyingLin/SSTHyper.",https://openaccess.thecvf.com/content/ACCV2024/html/Xu_SSTHyper_Sparse_Spectral_Transformer_for_Hyperspectral_Image_Reconstruction_ACCV_2024_paper.html
Conditional Distribution Modelling for Few-Shot Image Synthesis with Diffusion Models,"Few-shot image synthesis entails generating diverse and realistic images of novel categories using only a few example images. While multiple recent efforts in this direction have achieved impressive results, the existing approaches are dependent only upon the few novel samples available at test time in order to generate new images, which restricts the diversity of the generated images. To overcome this limitation, we propose Conditional Distribution Modelling (CDM) -- a framework which effectively utilizes Diffusion models for few-shot image generation. By modelling the distribution of the latent space used to condition a Diffusion process, CDM leverages the learnt statistics of the training data to get a better approximation of the unseen class distribution, thereby removing the bias arising due to limited number of few shot samples. Simultaneously, we devise a novel inversion based optimization strategy that further improves the approximated unseen class distribution, and ensures the fidelity of the generated samples to the unseen class. The experimental results on four benchmark datasets demonstrate the effectiveness of our proposed CDM for few-shot generation.",https://openaccess.thecvf.com/content/ACCV2024/html/Gupta_Conditional_Distribution_Modelling_for_Few-Shot_Image_Synthesis_with_Diffusion_Models_ACCV_2024_paper.html
The Devil is in the Details: Simple Remedies for  Image-to-LiDAR Representation Learning,"LiDAR is a crucial sensor in autonomous driving, commonly used alongside cameras. By exploiting this camera-LiDAR setup and recent advances in image representation learning, prior studies have shown the promising potential of image-to-LiDAR distillation. These prior arts focus on the designs of their own losses to effectively distill the pre-trained 2D image representations into a 3D model. However, the other parts of the designs have been surprisingly unexplored. We find that fundamental design elements, e.g., the LiDAR coordinate system, quantization according to the existing input interface, and data utilization, are more critical than developing loss functions, which have been overlooked in prior works. In this work, we show that simple fixes to these designs notably outperform existing methods by 16% in 3D semantic segmentation on the nuScenes dataset and 13% in 3D object detection on the KITTI dataset in downstream task performance. We focus on overlooked design choices along the spatial and temporal axes. Spatially, prior work has used cylindrical coordinate and voxel sizes without considering their side effects yielded with a commonly deployed sparse convolution layer input interface, leading to spatial quantization errors in 3D models. Temporally, existing work has avoided cumbersome data curation by discarding unsynced data, limiting the use to only the small portion of data that is temporally synced across sensors. We analyze these effects and propose simple solutions for each overlooked aspect.",https://openaccess.thecvf.com/content/ACCV2024/html/Jo_The_Devil_is_in_the_Details_Simple_Remedies_for__ACCV_2024_paper.html
SCCA-Net: A Novel Network for Image Manipulation Localization Using Split-Channel Contextual Attention,"This paper introduces SCCA-Net, an advanced end-to-end network designed specifically for Image Manipulation Localization (IML). SCCA-Net comprises four critical modules: Split-Channel Contextual Attention (SCCA), Extractor, Encoder, and Decoder. The SCCA module fuses dynamic frequency contextual features and similarity features extracted from RGB feature pyramids of images, overcoming the common shortcomings of existing attention-based IML technologies that typically overlook the frequency adaptation of contextual information. The SCCA modules pivotal component, the Parallel Dynamic Frequency Aggregator (PDFA), integrates Parallel Low-pass (PL) and Similarity Attention (SA) blocks to merge contextual and similarity vectors. The Extractor produces an RGB feature pyramid, channeling varied frequency features into the SCCA. The Encoder, utilizing Transformer, establishes a robust global feature representation. To reconstruct the predicted mask, the Decoder employs uniquely designed cascaded upsampling-convolution (Up-Conv) blocks. Rigorous testing demonstrates that SCCA-Net surpasses conventional models, achieving F1 score improvements of +14.3% on Coverage and +11.8% on CASIA, matching top performances on NIST2016. SCCA-Net pushes the field's boundaries and redefines the benchmarks for assessing IML.",https://openaccess.thecvf.com/content/ACCV2024/html/Xiang_SCCA-Net_A_Novel_Network_for_Image_Manipulation_Localization_Using_Split-Channel_ACCV_2024_paper.html
Revealing Hidden Context in Camouflage Instance Segmentation,"Predicting the instance-level masks of objects hidden in complex contexts is the goal of Camouflage Instance Segmentation (CIS),  a task complicated by the striking similarities between camouflaged objects and their backgrounds. This challenge is further heightened by the diverse appearances of camouflage objects, including varying angles, partial visibilities, and ambiguous morphologies. Prior works considered classifying pixels in a high uncertainty area without considering their contextual semantics, leading to numerous false positives. We proposed a novel method called Mask2Camouflage, which simultaneously enhances the modeling of contextual features and refines instance-level predicted maps. Mask2Camouflage leverages multi-scale features to integrate the extracted features from the backbone. Then, a Global Refinement Cross-Attention Module (GCA) is introduced to complement the foreground mask and background mask each other to reduce the false positive. Furthermore, by simulating a global shift clustering process, we present the Global-Shift Multi-Head Self-Attention (GSA), which enables the object query to capture not only information from earlier features but also their structural concepts, thereby reducing intra-class issues in the camouflage object detection task when validated with evaluated data. Compared with 15 state-of-the-art approaches, our Mask2Camouflage significantly improves the performance of camouflage instance segmentation.",https://openaccess.thecvf.com/content/ACCV2024/html/Phung_Revealing_Hidden_Context_in_Camouflage_Instance_Segmentation_ACCV_2024_paper.html
Dessie: Disentanglement for Articulated 3D Horse Shape and Pose Estimation from Images,"In recent years, 3D parametric animal models have been developed to aid in estimating 3D shape and pose from images and video. While progress has been made for humans, it's more challenging for animals due to limited annotated data. To address this, we introduce the first method using synthetic data generation and disentanglement to learn to regress 3D shape and pose. Focusing on horses, we use text-based texture generation and a synthetic data pipeline to create varied shapes, poses, and appearances, learning disentangled spaces. Our method, Dessie, surpasses existing 3D horse reconstruction methods and generalizes to other large animals like zebras, cows, and deer.",https://openaccess.thecvf.com/content/ACCV2024/html/Li_Dessie_Disentanglement_for_Articulated_3D_Horse_Shape_and_Pose_Estimation_ACCV_2024_paper.html
Greit-HRNet: Grouped Lightweight High-Resolution Network for Human Pose Estimation,"As multi-scale features are necessary for human pose estimation tasks, high-resolution networks are widely applied. To improve efficiency, lightweight modules are proposed to replace costly point-wise convolutions in high-resolution networks, including channel weighting and spatial weighting methods. However, they fail to maintain the consistency of weights and capture global spatial information. To address these problems, we present a Grouped lightweight High-Resolution Network (Greit-HRNet), in which we propose a Greit block including a group method Grouped Channel Weighting (GCW) and a spatial weighting method Global Spatial Weighting (GSW). GCW modules group conditional channel weighting to make weights stable and maintain the high-resolution features with the deepening of the network, while GSW modules effectively extract global spatial information and exchange information across channels. In addition, we apply the Large Kernel Attention (LKA) method to improve the whole efficiency of our Greit-HRNet. Our experiments on both MS-COCO and MPII human pose estimation datasets demonstrate the superior performance of our Greit-HRNet, outperforming other state-of-the-art lightweight networks.",https://openaccess.thecvf.com/content/ACCV2024/html/Han_Greit-HRNet_Grouped_Lightweight_High-Resolution_Network_for_Human_Pose_Estimation_ACCV_2024_paper.html
Improve Model Robustness in Less Time Than It Takes to Drink A Cup of Coffee with Plug-and-Play Plugins.,"Self-supervised learning has become the primary method for pre-training large models due to its ability to train without labeled data and its excellent data feature representation capabilities. However, neural network models are vulnerable to adversarial attacks, which can lead to incorrect predictions. Previous work has attempted to enhance the robust representation capabilities of base models through self-supervised adversarial training (self-AT), which integrates adversarial training into the self-supervised learning pre-training process. However, self-supervised learning requires numerous training epoches, and adversarial training is computationally complex. Consequently, these methods need an additional 2.75 to 12 times the pre-training time of the model to obtain robust representations. Considering the resource consumption of training large models and the current high cost of computational resources, the cost of obtaining robustness for base models is excessively high and impractical. This paper proposes a novel Plug-and-Play model Robustness Plugin training framework called PPRP. PPRP is designed as a robustness plugin for self-supervised base models that have completed pre-training. Once the robust plugin is added, the base model gains robust representation capabilities. Essentially, PPRP is a teacher-student network that performs adversarial training on a plugin model with only a few parameters, reducing the time required to achieve model robustness to 5% of the pre-training time. The robust plugin can be seamlessly integrated into pre-trained models without additional inference latency. Experiments show that on multiple datasets, different base models with the PPRP-trained robust plugin achieve state-of-the-art robustness.",https://openaccess.thecvf.com/content/ACCV2024/html/Fang_Improve_Model_Robustness_in_Less_Time_Than_It_Takes_to_ACCV_2024_paper.html
Locate n' Rotate: Two-stage Openable Part Detection with Geometric Foundation Model Priors,"Detecting the openable parts of articulated objects is crucial for downstream applications in intelligent robotics, such as pulling a drawer. This task poses a multitasking challenge due to the necessity of understanding object categories and motion. Most existing methods are either category-specific or trained on specific datasets, lacking generalization to unseen environments and objects. In this paper, we propose a Transformer-based Openable Part Detection (OPD) framework named Multi-feature Openable Part Detection (MOPD) that incorporates perceptual grouping and geometric priors, outperforming previous methods in performance. In the first stage of the framework, we introduce a perceptual grouping feature model that provides perceptual grouping feature priors for openable part detection, enhancing detection results through a cross-attention mechanism. In the second stage, a geometric understanding feature model offers geometric feature priors for predicting motion parameters. Compared to existing methods, our proposed approach shows better performance in both detection and motion parameter prediction. Codes and models are publicly available at https://github.com/lisiqi-zju/MOPD.",https://openaccess.thecvf.com/content/ACCV2024/html/Li_Locate_n_Rotate_Two-stage_Openable_Part_Detection_with_Geometric_Foundation_ACCV_2024_paper.html
Character-aware audio-visual subtitling in context,"This paper presents an improved framework for character-aware audio-visual subtitling in TV shows.  Our approach integrates speech recognition, speaker diarisation, and character recognition, utilising both audio and visual cues.  This holistic solution addresses what is said, when it's said, and who is speaking, providing a more comprehensive and accurate character-aware subtitling for TV shows. Our approach brings improvements on two fronts: first, we show that audio-visual synchronisation can be used to pick out the talking face amongst others present in a video clip, and assign an identity to the corresponding speech segment. This audio-visual approach improves recognition accuracy and yield over current methods. Second, we show that the speaker of short segments can be determined by using the temporal context of the dialogue within a scene. We propose an approach using local voice embeddings of the audio, and large language model reasoning on the text transcription. This overcomes a limitation of existing methods that they are unable to accurately assign speakers to short temporal segments. We validate the method on a dataset with 12 TV shows, demonstrating superior performance in speaker diarisation and character recognition accuracy compared to existing approaches. Project page : https://www.robots.ox.ac.uk/ vgg/research/llr-context/",https://openaccess.thecvf.com/content/ACCV2024/html/Huh_Character-aware_audio-visual_subtitling_in_context_ACCV_2024_paper.html
A Generic Autoregressive Predictive Feedback Framework for Skeleton-Based Action Recognition,"Prior works in skeleton-based action recognition have struggled with overcoming sequence order constraints while achieving comprehensive global modeling of temporal dependencies. However, most focus on enhancing the model's fitting ability across different temporal scales, overlooking the temporal non-stationary characteristics inherent in motion sequences. In this paper, we explore the adaptation of state-space modeling (SSM), typically suited for stationary sequences, to motion sequences. Addressing the challenge posed by the trendiness of motion sequences and the stability requirement of SSM, we integrate SSM into a generalized Autoregressive Predictive Feedback (APF) framework. Our approach involves segmenting motion sequences into trend and stationary components. We introduce the Non-Independent Multi-channel Processing (NiMc-P) module to capture implicit relationships among 3D coordinates and propose the Independent Multi-joint SSM (IMj-S) module to model temporal dependencies within stationary components. Throughout this process, state space matrices drive the feedback mechanism. Experiments conducted on the NTU-RGB+D 60 and NTU-RGB+D 120 datasets demonstrate the efficiency and versatility of APF.",https://openaccess.thecvf.com/content/ACCV2024/html/Yin_A_Generic_Autoregressive_Predictive_Feedback_Framework_for_Skeleton-Based_Action_Recognition_ACCV_2024_paper.html
Exploring Limits of Diffusion-Synthetic Training with Weakly Supervised Semantic Segmentation,"The advance of generative models for images has inspired various training techniques for image recognition utilizing synthetic images. In semantic segmentation, one promising approach is extracting pseudo-masks from attention maps in text-to-image diffusion models, which enables real-image-and-annotation-free training. However, the pioneering training methods using the diffusion-synthetic images and pseudo-masks, e.g., DiffuMask have limitations in terms of mask quality, scalability, and ranges of applicable domains. To address these limitations, we propose a new framework to view diffusion-synthetic semantic segmentation training as a weakly supervised learning problem, where we utilize potentially inaccurate attentive information within the generative model as supervision. Motivated by this perspective, we first introduce reliability-aware robust training, originally used as a classifier-based WSSS method, with modification to handle generative attentions. Additionally, we propose techniques to boost the weakly supervised synthetic training: We introduce prompt augmentation by synonym-and-hyponym replacement, which is data augmentation to the prompt text set to scale up and diversify training images with limited text resources. Finally, LoRA-based adaptation of Stable Diffusion enables the transfer to a distant domain, e.g., auto-driving images. Experiments in PASCAL VOC, ImageNet-S, and Cityscapes show that our method effectively closes gap between real and synthetic training in semantic segmentation.",https://openaccess.thecvf.com/content/ACCV2024/html/Yoshihashi_Exploring_Limits_of_Diffusion-Synthetic_Training_with_Weakly_Supervised_Semantic_Segmentation_ACCV_2024_paper.html
ESM-YOLO: Enhanced Small Target Detection Based on Visible and Infrared Multi-modal Fusion,"Detecting small targets in remote sensing imagery is frequently impeded by target faintness and complex background, resulting in reduced accuracy. This work introduces an Enhanced Small Target Detection method, termed ESM-YOLO, which leverages multi-modal fusion of visible and infrared data to enhance inter-modality correlation and thereby augments performance. Firstly, we devise a pixel-level Bilateral Excitation Fusion (BEF) module to extract both shared and unique features from distinct modalities symmetrically and efficiently. Subsequently, an Improved Atrous Spatial Pyramid Pooling (IASPP) unit and a Compact BottleneckCSP (CBCSP) unit are incorporated into the detection architecture. These components are meticulously tailored to enhance the detection of minute object features, while ensuring a balance between computational efficiency and feature representation capability. Experimental results show that ESM-YOLO achieves 82.42% accuracy on the widely used Vehicle Detection in Aerial Imagery (VEDAI) dataset. The effectiveness and superiority of our proposed method are demonstrated through extensive experiments.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_ESM-YOLO_Enhanced_Small_Target_Detection_Based_on_Visible_and_Infrared_ACCV_2024_paper.html
Diffusing Background Dictionary  for Hyperspectral Anomaly Detection,"The diffusion model (DM) has achieved remarkable results in image generation and has been used in hyperspectral image (HSI) processing. However, DM has not been directly applied in the HSI anomaly detection (HAD) task. In this paper, based on the characteristics of  HSI and HAD tasks, we combine the advantages of model-driven and data-driven and propose the diffusion background dictionary method (DBD). DBD intrinsically combines the DM with the low-rank representation (LRR) model, using DM to get the crucial background dictionary tensor in the tensor LRR, so that it can accurately detect the anomalies. We also diffuse the multivariate normal distribution that approximates the HSI background based on the idea of the RX algorithm in the HAD, making it more suitable for suppressing the background. DBD combines the advantages of the three main groups of HAD methods, and the experimental results on real datasets prove its effectiveness. DBD can outperform several existing state-of-the-art methods in terms of detection accuracy, which proves the DM's potential in HAD.",https://openaccess.thecvf.com/content/ACCV2024/html/Wu_Diffusing_Background_Dictionary__for_Hyperspectral_Anomaly_Detection_ACCV_2024_paper.html
Robust Single-view 3D Human Digitization via  Explicit Geometric Field with Semantic Guidance,"We propose a novel single-view human body 3D reconstruction framework guided by the semantic field. We argue that full visibility of 2D human shape and the alignment between geometric and semantic features are vital for a robust 3D human reconstruction. Due to the single view setting, existing methods fail to predict robust and completed 2D human shapes, which leads to vulnerability against invisibility. The usage of a parametric model could inform the model with a human shape, but the shape prior is too general, resulting in converging to a general body shape, and poor generalization to different types of garments.  In response to the aforementioned challenges, we propose a novel framework to reconstruct a photo-realistic 3D human mesh by estimating the overall 2D human shape from the given view via a Human shape predictor and predicting view-dependent explicit geometric field with an estimated semantic field via a novel Semantic-aware explicit geometric field. Then we ensure the queriablity of our view-dependent explicit geometric field via Semantic-aware Geometric Field Integration. Finally, we first propose a cascaded query strategy, termed Semantic-Guided Query Strategy to further combine the 2D and 3D features. Through extensive experiments, our framework surpasses all selected SoTA by a considerable gap in Chamfer and P2S distances on Thuman2.0, CAPE-NFP, and CAPE-FP datasets.",https://openaccess.thecvf.com/content/ACCV2024/html/Liu_Robust_Single-view_3D_Human_Digitization_via__Explicit_Geometric_Field_ACCV_2024_paper.html
Learning Dual Hierarchical Representation for 3D Surface Reconstruction,"Neural implicit functions have proved successful in representing 3D shapes or surfaces at arbitrary resolutions and with high fidelity. Unfortunately, between the various forms of reconstruction tasks neural implicit representation methods target, reconstructing from discrete voxels remains limited because of the computational complexity involved. We address this problem by introducing Dual Hierarchical Representation (DHR), which allows for faithful reconstructions under constrained computation by hierarchical encoding, decoding, and training procedures. A hierarchical latent feature code set is produced by first encoding the sparse voxelized shape into multi-scale feature grids and then grid-sampling each feature with a query point. The proposed transformer decoder then incorporates individual latent codes in hierarchical order, directing feature-to-3D projection and modeling the interaction of latent features with occupancies via cross-attention. At the training phase, representations derived from all feature hierarchies are integrated with varying contributions for another global-to-local learning technique. Experiments verify that DHR gains representation power by outperforming various baselines by voxel reconstruction tasks. It also shows robustness against different shape categories and gains the potential for being useful in the wild thanks to the generalization ability the transformer carries. Our code is available at https://github.com/JYeShin/DualHierarchicalRepresentation.",https://openaccess.thecvf.com/content/ACCV2024/html/Shin_Learning_Dual_Hierarchical_Representation_for_3D_Surface_Reconstruction_ACCV_2024_paper.html
Joint Image Super-resolution and Low-light Enhancement in the Dark,"Existing super-resolution (SR) models are typically trained on datasets captured under normal-light conditions.However, when dealing with images captured under low-light conditions, the degradation becomes more complex, and the difference in lighting conditions often leads to low-quality results when using standard SR models. Due to error accumulation, simply cascading low-light enhancement (LE) and SR algorithms may not result in satisfactory results. Therefore, in this paper, we tackle this issue by jointly considering SR and LE. We first propose a new dataset called DarkSR, which contains low-resolution (LR) RAW and sRGB images captured under the low-light conditions, along with the corresponding high-resolution (HR) sRGB images captured under normal-light conditions. Noticing the linear relationship between pixel values and scene radiance, as well as the high bit depth of RAW images, and considering the presence of ISP pipeline-related information in sRGB images, we introduce JSLNet, a dual-input network that effectively explores the complementary information from the low-light LR RAW and sRGB images. Extensive experiments demonstrate that compared to other state-of-the-art (SOTA) methods, our method achieves the best quality of results, while maintaining a relatively low computational burden. The code and dataset are available at https://github.com/flyhu2/DarkSR",https://openaccess.thecvf.com/content/ACCV2024/html/Zhou_Joint_Image_Super-resolution_and_Low-light_Enhancement_in_the_Dark_ACCV_2024_paper.html
Beyond Coarse-Grained Matching in Video-Text Retrieval,"Text-to-video retrieval has seen significant advancements, yet the ability of models to discern subtle differences in captions still requires verification. In this paper, we introduce a new metric for fine-grained evaluation that can be applied to existing datasets by automatically generating hard negative test captions with subtle single-word variations across noun, verb, adjective, adverb, and preposition. We perform comprehensive experiments using four state-of-the-art models across two standard benchmarks (MSR-VTT and VATEX) and two specially curated datasets enriched with detailed descriptions (VLN-UVO and VLN-OOPS), resulting in a number of novel findings and insights: 1) our analyses show that the current evaluation benchmarks fall short in detecting a model's ability to perceive subtle single-word differences, 2) our fine-grained evaluation highlights the difficulty models face in distinguishing such subtle variations. To enhance fine-grained understanding, we propose a new baseline that can be easily combined with current methods. Results from experiments on this fine-grained evaluation demonstrate that our approach clearly enhances a models' ability to understand fine-grained differences.",https://openaccess.thecvf.com/content/ACCV2024/html/Chen_Beyond_Coarse-Grained_Matching_in_Video-Text_Retrieval_ACCV_2024_paper.html
UNet--: Memory-Efficient and Feature-Enhanced Network Architecture based on U-Net with Reduced Skip-Connections,"U-Net models with encoder, decoder, and skip-connections components have demonstrated effectiveness in a variety of vision tasks. The skip-connections transmit fine-grained information from the encoder to the decoder. It is necessary to maintain the  feature maps used by the skip-connections in memory before the decoding stage. Therefore, they are not friendly to devices with limited resource. In this paper, we propose a universal method and architecture to reduce the memory consumption and meanwhile generate enhanced feature maps to improve network performance. To this end, we design a simple but effective Multi-Scale Information Aggregation Module (MSIAM) in the encoder and an Information Enhancement Module (IEM) in the decoder. The MSIAM aggregates multi-scale feature maps into single-scale with less memory. After that, the aggregated feature maps can be expanded and enhanced to multi-scale feature maps by the IEM. By applying the proposed method on NAFNet, a SOTA model in the field of image restoration, we design a memory-efficient and feature-enhanced network architecture, UNet--. The memory demand by the skip-connections in the UNet-- is reduced by 93.3%, while the performance is improved compared to NAFNet. Furthermore, we show that our proposed method can be generalized to multiple visual tasks, with consistent improvements in both memory consumption and network accuracy compared to the existing efficient architectures.",https://openaccess.thecvf.com/content/ACCV2024/html/Yin_UNet--_Memory-Efficient_and_Feature-Enhanced_Network_Architecture_based_on_U-Net_with_ACCV_2024_paper.html
Enhancing Multimedia Applications by Removing Dynamic Objects in Neural Radiance Fields,"Neural Radiance Fields (NeRF) are at the forefront of view synthesis technology, renowned for their versatility and ease of implementation across various applications. However, their integration into multimedia environments faces challenges: objects occlude background information during motion, which usually compromises the quality of reconstructions. In this paper, we present a novel framework to exclude dynamic interference from NeRF scenes, enhancing the practicability in multimedia applications. Our method leverages perceptual optimization, informed by image quality assessment (IQA), and employs text-guided 2D image inpainting to address view synthesis inaccuracies. Furthermore, we propose a new and challenging dataset of real-world scenes to address the lack of evaluation ground truth for dynamic object inpainting in scenes. Experimental results show that our method significantly outperforms existing methods in terms of appearance metrics for the task of removing dynamic objects from scenes.",https://openaccess.thecvf.com/content/ACCV2024/html/Yang_Enhancing_Multimedia_Applications_by_Removing_Dynamic_Objects_in_Neural_Radiance_ACCV_2024_paper.html
Revisiting sample weights based method for noisy-label detection and classification,"The remarkable success of Convolutional Neural Networks (CNNs) in image classification can be attributed large clean training datasets. However, real-world data is often far from noise-free, impacting the performance of resulting deep neural network (DNN) models. Existing literature focuses on noisy label detection, often drawing a clear line between noisy and clean label samples. Nevertheless, each sample contributes differently to the final model performance; some noisy-label samples may still be valuable to a certain level, while certain clean-label samples might not significantly enhance the model. In this work, assuming that a small clean-label dataset may be available, we aim to learn a sample weight for each training sample. This weight is gradually updated as the model is training to indicate the usefulness of a particular sample in minimizing loss with respect to the clean-label dataset. Consequently, our method prioritizes high-quality data samples, minimizing the impact of harmful or unhelpful ones by assigning close-to-zero weights in a weighted loss function. We empirically demonstrate that our method is not dependent on noise type and can work well for both real-world and synthetic noise. Our method can achieve state-of-the-art performance in terms of the classification accuracy on clean test sets.",https://openaccess.thecvf.com/content/ACCV2024/html/Hoang_Revisiting_sample_weights_based_method_for_noisy-label_detection_and_classification_ACCV_2024_paper.html
Enhancing Robustness to Noise Corruption for Point Cloud Recognition via Spatial Sorting and Set-Mixing Aggregation Module,"Current models for point cloud recognition demonstrate promising performance on synthetic datasets. However, real-world point cloud data inevitably contains noise, impacting model robustness. While recent efforts focus on enhancing robustness through various strategies, there still remains a gap in comprehensive analyzes from the standpoint of network architecture design. Unlike traditional methods that rely on generic techniques, our approach optimizes model robustness to noise corruption through network architecture design. Inspired by the token-mixing technique applied in 2D images, we propose Set-Mixer, a noise-robust aggregation module which facilitates communication among all points to extract geometric shape information and mitigating the influence of individual noise points. A sorting strategy is designed to enable our module to be invariant to point permutation, which also tackles the unordered structure of point cloud and introduces consistent relative spatial information. Experiments conducted on ModelNet40-C indicate that Set-Mixer significantly enhances the model performance on noisy point clouds, underscoring its potential to advance real-world applicability in 3D recognition and perception tasks.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_Enhancing_Robustness_to_Noise_Corruption_for_Point_Cloud_Recognition_via_ACCV_2024_paper.html
Hybrid and Non-minimal Planar Motion Estimation from Point Correspondences,"We address the problem of relative camera pose estimation in the context of planar motion, where the rotation axis and translation vectors are orthogonal to each other. For such scenarios, it is common to assume a known motion plane to leverage the reduced algebraic structure and geometric parameterization of the ensuing epipolar constraints. In this work, we focus on the general prior-free case, in which no assumptions about the plane of motion are made. While current solvers estimate planar motion from homogeneous (i.e. 2D-2D or 3D-3D) point correspondences, leveraging hybrid (i.e. combinations of 2D-2D, 2D-3D, and 3D-3D) point correspondences remains an open problem. We explore the solution space for the general planar motion problem and propose three novel minimal solvers from hybrid point correspondences, as well as a triplet of new non-minimal solvers from 2D-2D point correspondences bridging the theoretical gap from minimal to linear solutions. Experiments on both synthetic data and standard benchmark sequences of real-world imagery demonstrate that our proposed solvers can provide better pose estimates than homogeneous planar motion solvers (with or without motion plane prior), while achieving competitive run times.",https://openaccess.thecvf.com/content/ACCV2024/html/Simental_Hybrid_and_Non-minimal_Planar_Motion_Estimation_from_Point_Correspondences_ACCV_2024_paper.html
Text Query to Web Image to Video: A Comprehensive Ad-hoc Video Search,"In this study, we propose a novel approach for Ad-hoc Video Search that leverages the power of image search engines to synthesize query images for corresponding textual sentence query. Existing methods primarily rely on pre-trained language-image models to extract features from textual queries and video keyframes of video segments. While recent approaches using generative models to generate visual representations based on text descriptions show promise, they are limited by diversity, authenticity, speed, and hardware requirements. In contrast, our proposed method leverages the vast and diverse image database available on the Internet through image search engines to directly synthesize query images based on input text descriptions. Moreover, to enhance computational efficiency, each video segment is represented by only single keyframe. Specifically, we use only two general-purpose multimodal models for extracting feature embeddings for textual queries, query images, and keyframes. To return a list of relevant video segments for each query, we compute the weighted average similarity between each keyframe and both the textual query and query images. Experiments conducted on the TRECVID dataset (V3C2) and main set of textual queries from 2022 and 2023 demonstrate the efficiency of our method.",https://openaccess.thecvf.com/content/ACCV2024/html/Nguyen_Text_Query_to_Web_Image_to_Video_A_Comprehensive_Ad-hoc_ACCV_2024_paper.html
GeoRefineNet: A Multistage Framework for Enhanced Cephalometric Landmark Detection in CBCT Images Using 3D Geometric Information,"The precise detection of cephalometric landmarks on two-dimensional (2D) radiographs or three-dimensional (3D) computed tomography (CT) images is a fundamental step in various medical fields, especially in research on orthodontics and maxillofacial surgery. Deep learning-based detectors have demonstrated remarkable accuracy in 2D cephalometric analysis, whereas conventional single-view approaches are limited by their reliance on information from a single perspective. This study proposes GeoRefineNet, a novel multistage framework that leverages information from multiple CT scans acquired at various angles. By incorporating geometric knowledge through a 3D heatmap reconstruction process, GeoRefineNet improves robustness, accuracy, and adaptability to various cephalometric configurations. The proposed framework predicts 3D landmark positions on CT images, effectively addressing challenges associated with high-dimensional input data and limited training examples. GeoRefineNet surpasses the existing state-of-the-art models in the 2D and 3D domains, as demonstrated by its superior performance on numerical and clinical datasets. These findings indicate that GeoRefineNet offers a promising avenue for improving the accuracy and reliability of cephalometric landmark detection fostering further advances in clinical diagnosis and treatment planning. Our code is available at https://anonymous.4open.science/r/GeoRefineNet-7E01/.",https://openaccess.thecvf.com/content/ACCV2024/html/Viriyasaranon_GeoRefineNet_A_Multistage_Framework_for_Enhanced_Cephalometric_Landmark_Detection_in_ACCV_2024_paper.html
AutoAD-Zero: A Training-Free Framework for Zero-Shot Audio Description,"Our objective is to generate Audio Descriptions (ADs) for both movies and TV series in a training-free manner. We use the power of off-the-shelf Video Language Models (VLMs) and Large Language Models (LLMs), and develop visual and text prompting strategies for this task. Our contributions are three-fold: (i) We demonstrate that a VLM can successfully name and refer to characters if directly prompted with character information through visual indications without requiring any fine-tuning; (ii) A two-stage process is developed to generate ADs, with the first stage asking the VLM to comprehensively describe the video, followed by a second stage utilising a LLM to summarise dense textual information into one succinct AD sentence; (iii) A new dataset for TV audio description is formulated. Our approach, named AutoAD-Zero, demonstrates outstanding performance (even competitive with some models fine-tuned on ground-truth ADs) in AD generation for both movies and TV series, achieving state-of-the-art CRITIC scores.",https://openaccess.thecvf.com/content/ACCV2024/html/Xie_AutoAD-Zero_A_Training-Free_Framework_for_Zero-Shot_Audio_Description_ACCV_2024_paper.html
LocoMotion: Learning Motion-Focused Video-Language Representations,"This paper strives for motion-focused video-language representations.  Existing methods to learn video-language representations use spatial-focused data, where identifying the objects and scene is often enough to distinguish the relevant caption. We instead propose LocoMotion to learn from motion-focused captions that describe the movement and temporal progression of local object motions. We achieve this by adding synthetic motions to videos and using the parameters of these motions to generate corresponding captions. Furthermore, we propose verb-variation paraphrasing to increase the caption variety and learn the link between primitive motions and high-level verbs. With this, we are able to learn a motion-focused video-language representation.  Experiments demonstrate our approach is effective for a variety of downstream tasks, particularly when limited data is available for fine-tuning. Code is available: https://hazeldoughty.github.io/Papers/LocoMotion/",https://openaccess.thecvf.com/content/ACCV2024/html/Doughty_LocoMotion_Learning_Motion-Focused_Video-Language_Representations_ACCV_2024_paper.html
D'OH: Decoder-Only Random Hypernetworks for Implicit Neural Representations,"Deep implicit functions have been found to be an effective tool for efficiently encoding all manner of natural signals. Their attractiveness stems from their ability to compactly represent signals with little to no offline training data. Instead, they leverage the implicit bias of deep networks to decouple hidden redundancies within the signal. In this paper, we explore the hypothesis that additional compression can be achieved by leveraging redundancies that exist between layers. We propose to use a novel runtime decoder-only hypernetwork - that uses no offline training data - to better exploit cross-layer parameter redundancy. Previous applications of hypernetworks with deep implicit functions have employed feed-forward encoder/decoder frameworks that rely on large offline datasets that do not generalize beyond the signals they were trained on. We instead present a strategy for the optimization of runtime deep implicit functions for single-instance signals through a Decoder-Only randomly projected Hypernetwork (D'OH). By directly changing the latent code dimension, we provide a natural way to vary the memory footprint of neural representations without the costly need for neural architecture search on a space of alternative low-rate structures.",https://openaccess.thecvf.com/content/ACCV2024/html/Gordon_DOH_Decoder-Only_Random_Hypernetworks_for_Implicit_Neural_Representations_ACCV_2024_paper.html
Cross Feature Fusion of Fundus Image and Generated Lesion Map for Referable Diabetic Retinopathy Classification,"Diabetic Retinopathy (DR) is a primary cause of blindness, necessitating early detection and diagnosis. This paper focuses on referable DR classification to enhance the applicability of the proposed method in clinical practice. We develop an advanced cross-learning DR classification method leveraging transfer learning and cross-attention mechanisms. The proposed method employs the Swin U-Net architecture to segment lesion maps from DR fundus images. The Swin U-Net segmentation model, enriched with DR lesion insights, is transferred to generate a lesion map. Both the fundus image and its segmented lesion map are used as complementary inputs for the classification model. A cross-attention mechanism is deployed to improve the model's ability to capture fine-grained details from the input pairs. Our experiments, utilizing two public datasets, FGADR and EyePACS, demonstrate a superior accuracy of 94.6%, surpassing current state-of-the-art methods by 4.4%. To this end, we aim for the proposed method to be seamlessly integrated into clinical workflows, enhancing accuracy and efficiency in identifying referable DR.",https://openaccess.thecvf.com/content/ACCV2024/html/Mok_Cross_Feature_Fusion_of_Fundus_Image_and_Generated_Lesion_Map_ACCV_2024_paper.html
Randomized Channel-pass Mask for Channel-wise Explanation of Black-box Models,"In recent years, there has been active research on interpreting the classification results of deep models. Among these methods, MC-RISE enables pixel-color-wise interpretation based on the models output for images where pixels have been randomly replaced with a predetermined color. However, this approach requires manually preparing appropriate color candidates. This study proposes a pixel-channel-wise interpretation method, using a Randomized Channel-pass Mask (RaCM), which directly evaluates the importance of the original RGB values of an image through randomly generated masks that pass or exclude color channels of each pixel. Experiments are conducted using the German Traffic Sign Recognition Benchmark and ImageNet datasets. The effectiveness of the proposed method is demonstrated through evaluation metrics such as Insertion, Deletion, and Average DCC.",https://openaccess.thecvf.com/content/ACCV2024/html/Hachiya_Randomized_Channel-pass_Mask_for_Channel-wise_Explanation_of_Black-box_Models_ACCV_2024_paper.html
DPL: Cross-quality DeepFake Detection via Dual Progressive Learning,"Real-world DeepFake videos often undergo various compression operations, resulting in a range of video qualities. These varying qualities diversify the pattern of forgery traces, significantly increasing the difficulty of DeepFake detection. To address this challenge, we introduce a new Dual Progressive Learning (DPL) framework for cross-quality DeepFake detection. Specifically, we liken this task to progressively drilling for underground water, where low-quality videos require more effort than high-quality ones. To achieve this, we develop two sequential-based branches to ""drill waters"" with different efforts. The first branch progressively excavates the forgery traces according to the levels of video quality, i.e., time steps, determined by a dedicated CLIP-based indicator. In this branch, a Feature Selection Module is designed to adaptively assign appropriate features to the corresponding time steps. Considering that different techniques may introduce varying forgery traces within the same video quality, we design a second branch targeting forgery identifiability as complementary. This branch operates similarly and shares the feature selection module with the first branch. Our design elegantly achieves progressive learning, as the sequential model is inherently dynamic with weight-sharing units across different time steps and can memorize the previous progress, reducing memory costs while retaining effectiveness. Extensive experiments demonstrate the superiority of our method for cross-quality DeepFake detection.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_DPL_Cross-quality_DeepFake_Detection_via_Dual_Progressive_Learning_ACCV_2024_paper.html
Diffusion Model Compression for Image-to-Image Translation,"As recent advances in large-scale Text-to-Image (T2I) diffusion models have yielded remarkable high-quality image generation, diverse downstream Image-to-Image (I2I) applications have emerged. Despite the impressive results achieved by these I2I models, their practical utility is hampered by their large model size and the computational burden of the iterative denoising process. In this paper, we propose a novel compression method tailored for diffusion-based I2I models. Based on the observations that the image conditions of I2I models already provide rich information on image structures, and that the time steps with a larger impact tend to be biased, we develop surprisingly simple yet effective approaches for reducing the model size and latency. We validate the effectiveness of our method on three representative I2I tasks: InstructPix2Pix for image editing, StableSR for image restoration, and ControlNet for image-conditional image generation. Our approach achieves satisfactory output quality with 39.2%, 56.4% and 39.2% reduction in model footprint, as well as 81.4%, 68.7% and 31.1% decrease in latency to InstructPix2Pix, StableSR and ControlNet, respectively.",https://openaccess.thecvf.com/content/ACCV2024/html/Kim_Diffusion_Model_Compression_for_Image-to-Image_Translation_ACCV_2024_paper.html
Contrastive Max-correlation for Multi-view Clustering,"Multi-view clustering exhibits advantages over single-view clustering due to its ability to fully utilize complementary information between multiple views. However, most mainstream methods have the following two drawbacks: 1) Ignoring structural conflicts between views leads to a deterioration in clustering performance, because merging a certain view actually worsens the clustering results; 2) Rather than globally extracting the maximum correlation between views, their approaches center on individual instances, consequently making models more susceptible to interference from local noise points. To address these issues, this paper proposes a novel framework, entitled Contrastive Max-correlation for Multi-view Clustering (CMMC) for robust multi-view clustering. In particular, the network framework incorporates two effective methods. The first method, maximum structure correlation learning, enhances the downstream task representations by incorporating complementary structural information. Additionally, the framework achieves simultaneous mining of view correlations and alignment of views through the global max-correlation contrastive learning method. As the above methods operate globally, CMMC can effectively reduce the impact of noise information. Experiments on various types of multi-view datasets demonstrate that CMMC outperforms existing methods in terms of clustering accuracy and robustness.",https://openaccess.thecvf.com/content/ACCV2024/html/Deng_Contrastive_Max-correlation_for_Multi-view_Clustering_ACCV_2024_paper.html
Robust Visual Reinforcement Learning by Prompt Tuning,"Training an agent based solely on observational data in a single environment, which then performs well in a zero-shot manner in unseen contexts, presents a significant challenge in the field of Reinforcement Learning. Given that environmental signals are limited to pixel-based inputs, the development of a generalized visual encoder is crucial for enhancing the agent's robustness. While pre-trained image encoders provide a straightforward and effective means of obtaining universal representations, the inability to perform end-to-end retraining on off-the-shelf models limits them from acquiring essential in-domain knowledge. This paper explores the promising potential of  Visual Prompt Tuning to construct a more resilient image encoder for the agent. Extensive empirical evaluations are conducted on multiple benchmarks derived from the DeepMind Control Suite. The findings indicate notable improvements in both episode rewards and sample efficiency.",https://openaccess.thecvf.com/content/ACCV2024/html/Tran_Robust_Visual_Reinforcement_Learning_by_Prompt_Tuning_ACCV_2024_paper.html
LoLI-Street: Benchmarking Low-light Image Enhancement and Beyond,"Low-light image enhancement (LLIE) is essential for numerous computer vision tasks, including object detection, tracking, segmentation, and scene understanding. Despite substantial research on improving low-quality images captured in underexposed conditions, clear vision remains critical for autonomous vehicles, which often struggle with low-light scenarios, signifying the need for continuous research. However, LLIE models and LLIE-paired datasets are scarce, particularly for street scenes, limiting the development of robust LLIE methods. Despite using advanced transformers and/or diffusion-based models, current LLIE methods struggle in real-world low-light conditions and lack training on street-scene datasets, limiting their effectiveness for autonomous vehicles. To bridge these gaps, we introduce a new large-scale dataset ""LoLI-Street"" (Low-Light Images of Streets) with 33k paired low-light and well-exposed images from street scenes in developed cities, covering 19k object classes for object detection, including Person, Bicycle, Car, Bus, Motorcycle, and Traffic Light, etc. LoLI-Street dataset also features 1,000 real low-light test images, providing a benchmark for evaluating models under real-world conditions. Furthermore, we propose a transformer and diffusion-based LLIE model named ""TriFuse"". Leveraging the LoLI-Street dataset, we train and evaluate our TriFuse and other SOTA models to benchmark our dataset. Comparing various models, the feasibility of our dataset for generalization is evident in testing across different mainstream datasets by significantly enhancing low-quality images and object detection for practical applications in autonomous driving and surveillance systems. The benchmark dataset and the evaluation code will be released to ensure reproducibility.",https://openaccess.thecvf.com/content/ACCV2024/html/Islam_LoLI-Street_Benchmarking_Low-light_Image_Enhancement_and_Beyond_ACCV_2024_paper.html
Fine-tuning Large Language Models for Automatic Font Skeleton Generation: Exploration and Analysis,"Despite the pivotal role that font skeletons could play in typeface research and font design, the availability of font skeleton data is sparse and limited. This research explores the possibility of using Large Language Models (LLMs) to generate font skeleton data based on font outline data. Our method represents font skeletons and font outlines as sequences of text tokens derived from SVG commands, and formulates the font skeleton task as a language modeling task predicting the token sequence for a font skeleton given the token sequence of a font outline. As a first attempt, we fine-tuned GPT-3.5 on a dataset of 8,213 Japanese font outlines and corresponding skeletons. Both quantitative and qualitative evaluations show the effectiveness of the approach in terms of rasterized pixel distance, Chamfer distance, and visual analysis. Our proposed method achieved average results of 14.678 for rasterized pixel distance and 1.713 for Chamfer distance, both better than the baseline method (PolyVectorization). In visual analysis, we found better generation results for complex shapes which logograms such as Chinese characters tend to have, than for the simple shapes of syllabograms such as Japanese kana, phonograms such as Latin alphabets, and symbols. Although our fine-tuned model has limitations in generating the skeletons of other font styles, this research establishes a foundation for the automatic generation of font skeletons using LLMs, setting the stage for future work on automatic skeleton generation and the wider application of font skeletons in typography.",https://openaccess.thecvf.com/content/ACCV2024/html/Liu_Fine-tuning_Large_Language_Models_for_Automatic_Font_Skeleton_Generation_Exploration_ACCV_2024_paper.html
Pluggable Style Representation Learning for Multi-Style Transfer,"Due to the high diversity of image styles, the scalability to various styles plays a critical role in real-world applications. To accommodate a large amount of styles, previous multi-style transfer approaches rely on enlarging the model size while arbitrary-style transfer methods utilize heavy backbones. However, the additional computational cost introduced by more model parameters hinders these methods to be deployed on resource-limited devices. To address this challenge, in this paper, we develop a style transfer framework by decoupling the style modeling and transferring. Specifically, for style modeling, we propose a style representation learning scheme to encode the style information into a compact representation. Then, for style transferring, we develop a style-aware multi-style transfer network (SaMST) to adapt to diverse styles using pluggable style representations. In this way, our framework is able to accommodate diverse image styles in the learned style representations without introducing additional overhead during inference, thereby maintaining efficiency. Experiments show that our style representation can extract accurate style information. Moreover, qualitative and quantitative results demonstrate that our method achieves state-of-the-art performance in terms of both accuracy and efficiency. The codes are available in https://github.com/The-Learning-And-Vision-Atelier-LAVA/SaMST.",https://openaccess.thecvf.com/content/ACCV2024/html/Liu_Pluggable_Style_Representation_Learning_for_Multi-Style_Transfer_ACCV_2024_paper.html
CNN Mixture-of-Depths,"We introduce Mixture-of-Depths (MoD) for Convolutional Neural Networks (CNNs), a novel approach that enhances the computational efficiency of CNNs by selectively processing channels based on their relevance to the current prediction. This method optimizes computational resources by dynamically selecting key channels in feature maps for focused processing within the convolutional blocks (Conv-Blocks), while skipping less relevant channels. Unlike conditional computation methods that require dynamic computation graphs, CNN MoD uses a static computation graph with fixed tensor sizes which improve hardware efficiency. It speeds up the training and inference processes without the need for customized CUDA kernels, unique loss functions, or fine-tuning. CNN MoD either matches the performance of traditional CNNs with reduced inference times, GMACs, and parameters, or exceeds their performance while maintaining similar inference times, GMACs, and parameters. For example, on ImageNet, ResNet86-MoD exceeds the performance of the standard ResNet50 by 0.45% with a 6% speedup on CPU and 5% on GPU. Moreover, ResNet75-MoD achieves the same performance as ResNet50 with a 25% speedup on CPU and 15% on GPU.",https://openaccess.thecvf.com/content/ACCV2024/html/Cakaj_CNN_Mixture-of-Depths_ACCV_2024_paper.html
Optimized Breast Lesion Segmentation in Ultrasound Videos Across Varied Resource-Scant Environments,"Medical video segmentation plays a crucial role in clinical diagnosis and therapeutic procedures by enabling dynamic tracking of breast lesions across frames in ultrasound videos, thereby improving segmentation accuracy. However, the existing methods struggle to strike a balance between segmentation accuracy and inference speed, which impedes their real-time deployment in resource-limited medical environments. To overcome these challenges, we introduce a rapid breast lesion segmentation framework named RbS. RbS employs the Stem module and RbSBlock to enhance representations through intra-frame analysis of ultrasound videos. Moreover, we have developed two versions of RbS: RbS-S boasts enhanced segmentation accuracy, while RbS-L ensures faster inference speeds. Experimental evidence indicates that RbS surpasses current leading models in both segmentation efficiency and prediction accuracy, particularly on resource-limited devices. Our contribution significantly propels the progress of developing efficient medical video segmentation frameworks suitable for various medical platforms.",https://openaccess.thecvf.com/content/ACCV2024/html/Li_Optimized_Breast_Lesion_Segmentation_in_Ultrasound_Videos_Across_Varied_Resource-Scant_ACCV_2024_paper.html
Wavelet-based Mamba with Fourier Adjustment for Low-light Image Enhancement,"Frequency information (e.g., Discrete Wavelet Transform and Fast Fourier Transform) has been widely applied to solve the issue of Low-Light Image Enhancement (LLIE). However, existing frequency-based models primarily operate in the simple wavelet or Fourier space of images, which lacks utilization of valid global and local information in each space. We found that wavelet frequency information is more sensitive to global brightness due to its low-frequency component while Fourier frequency information is more sensitive to local details due to its phase component. In order to achieve superior preliminary brightness enhancement by optimally integrating spatial channel information with low-frequency components in the wavelet transform, we introduce channel-wise Mamba, which compensates for the long-range dependencies of CNNs and has lower complexity compared to Diffusion and Transformer models. So in this work, we propose a novel Wavelet-based Mamba with Fourier Adjustment model called WalMaFa, consisting of a Wavelet-based Mamba Block (WMB) and a Fast Fourier Adjustment Block (FFAB). We employ an Encoder-Latent-Decoder structure to accomplish the end-to-end transformation. Specifically, WMB is adopted in the Encoder and Decoder to enhance global brightness while FFAB is adopted in the Latent to fine-tune local texture details and alleviate ambiguity. Extensive experiments demonstrate that our proposed WalMaFa achieves state-of-the-art performance with fewer computational resources and faster speed. Code is now available at: https://github.com/mcpaulgeorge/WalMaFa.",https://openaccess.thecvf.com/content/ACCV2024/html/Tan_Wavelet-based_Mamba_with_Fourier_Adjustment_for_Low-light_Image_Enhancement_ACCV_2024_paper.html
Spectral Modality-Aware Interactive Fusion Network for HSI Super-Resolution,"Due to the limitations of current spectral imaging equipment in acquiring high-resolution hyperspectral images (HR-HSIs), a common approach is to fuse low-resolution hyperspectral images (LR-HSIs) with high-resolution multispectral images (HR-MSIs). However, most existing methods have not fully taken into account the correlation and discrepancy in modality between HSIs and MSIs. To address this limitation, we propose an innovative spectral modality-aware interactive fusion network (SMIF-NET) for comprehensive extraction of spectral information and seamless feature fusion. First, we introduce the spectral modality-aware transformer (SMAT) with a dual-attention mechanism to compute spectral self-similarity and cross-spectral correlation. Second, we apply the interactive spatial-spectral feature fusion (IS2F2) to fuse the acquired high-level spectral and spatial features. This fusion technique combines spatial-wise and channel-wise squeeze and excitation to achieve seamless integration of spatial-spectral information. Finally, the extensive experiments on three datasets demonstrate the superior performance of SMIF-NET in both visual and quantitative assessments compared to eight state-of-the-art (SOTA) fusion-based methods.",https://openaccess.thecvf.com/content/ACCV2024/html/Xu_Spectral_Modality-Aware_Interactive_Fusion_Network_for_HSI_Super-Resolution_ACCV_2024_paper.html
MonoDSSMs: Efficient Monocular 3D Object Detection with Depth-Aware State Space Models,"Monocular 3D object detection has been an important part of autonomous driving support systems. In recent years, we have seen enormous improvement in both detection quality and runtime performance. This work presents MonoDSSM, the first to utilize the Mamba architecture to push the performance further while maintaining the detection quality. In short, our contributions are: (1) introduce Mamba-based encoder-decoder architecture to extract 3D features, and (2) propose a novel Cross-Mamba module to fuse the depth-aware features and context-aware features using the State-Space-Models (SSMs). In addition, we employ the multi-scale feature prediction strategy to enhance the predicted depth map quality. Our experiments demonstrate that the proposed architecture yields competitive performance on the KITTI dataset while significantly improving the model's effectiveness in both model size and computational cost. Our MonoDSSM achieves a comparable detection quality to the baseline, with 2.2x fewer parameters and a 1.28x faster computation time.",https://openaccess.thecvf.com/content/ACCV2024/html/Vu_MonoDSSMs_Efficient_Monocular_3D_Object_Detection_with_Depth-Aware_State_Space_ACCV_2024_paper.html
BootsTAP: Bootstrapped Training for Tracking-Any-Point,"To endow models with greater understanding of physics and motion, it is useful to enable them to perceive how solid surfaces move and deform in real scenes. This can be formalized as Tracking-Any-Point (TAP), which requires the algorithm to track any point on solid surfaces in a video, potentially densely in space and time. Large-scale ground-truth training data for TAP is only available in simulation, which currently has a limited variety of objects and motion. In this work, we demonstrate how large-scale, unlabeled, uncurated real-world data can improve a TAP model with minimal architectural changes, using a self-supervised student-teacher setup. We demonstrate state-of-the-art performance on the TAP-Vid benchmark surpassing previous results by a wide margin: for example, TAP-Vid-DAVIS performance improves from 61.3% to 67.4%, and TAP-Vid-Kinetics from 57.2% to 62.5%.  For visualizations, see our project webpage at https://bootstap.github.io",https://openaccess.thecvf.com/content/ACCV2024/html/Doersch_BootsTAP_Bootstrapped_Training_for_Tracking-Any-Point_ACCV_2024_paper.html
Match me if you can: Semi-Supervised Semantic Correspondence Learning with Unpaired Images,"Semantic correspondence methods have advanced to obtaining high-quality correspondences employing complicated networks, aiming to maximize the model capacity. However, despite the performance improvements, they may remain constrained by the scarcity of training keypoint pairs, a consequence of the limited training images and the sparsity of keypoints. This paper builds on the hypothesis that there is an inherent data-hungry matter in learning semantic correspondences and uncovers the models can be more trained by employing densified training pairs. We demonstrate a simple machine annotator reliably enriches paired key points via machine supervision, requiring neither extra labeled key points nor trainable modules from unlabeled images. Consequently, our models surpass current state-of-the-art models on semantic correspondence learning benchmarks like SPair-71k, PF-PASCAL, and PF-WILLOW and enjoy further robustness on corruption benchmarks.",https://openaccess.thecvf.com/content/ACCV2024/html/Kim_Match_me_if_you_can_Semi-Supervised_Semantic_Correspondence_Learning_with_ACCV_2024_paper.html
HMGS: Hybrid Model of Gaussian Splatting for Enhancing 3D Reconstruction with Reflections,"The advent of 3D Gaussian Splatting (3D-GS) marks a significant breakthrough in the field of 3D reconstruction, leveraging GPU rasterization technology to achieve real-time rendering with state-of-the-art quality. However, 3D-GS is limited by the capacity of low-order spherical harmonics to represent high-frequency reflective attributes, often resulting in the loss of critical information in scenes with highlights and reflections. To address this limitation, we propose HMGS, a hybrid model that enhances the original 3D-GS's ability to capture reflective colors. Our approach employs a neural network to learn color components from both the camera viewing direction and the reflected light direction, which are then jointly trained with the original 3D-GS model. Furthermore, we introduce a smoothing loss for the reflective color component, effectively decoupling the two color components. Our method significantly improves the reconstruction performance of 3D-GS on datasets featuring metallic sheen, light reflections, and shadows, while also enhancing reconstruction quality on general datasets.",https://openaccess.thecvf.com/content/ACCV2024/html/Zhang_HMGS_Hybrid_Model_of_Gaussian_Splatting_for_Enhancing_3D_Reconstruction_ACCV_2024_paper.html
Do they Share the Same Tail? Learning Individual Compositional Attribute Prototype for Generalized Zero-Shot Learning,"Attributes are considered fundamental in zero-shot learning. By incorporating the correspondences between classes and attributes as prior knowledge, the model is able to approximate a class prototype for numerous classes without the need for any visual samples of these classes. In the majority of prior research, attributes are considered primitives and are not subjected to further subdivision. While the only distinction between shared attributes across classes is the absolute magnitude of their values, this does not adequately reflect the more significant visual differences between these classes in natural images. To address this issue, we propose learning the Individual Compositional Attribute Prototype (InCAP). Specifically, InCAP does not treat attributes as the sole primitives but uses attribute semantics as objects in compositions, while class semantics are introduced as a special kind of state description within these compositions. This approach allows attributes and classes to form the structure of the composition. To avoid domain shift when encountering unseen classes, these compositional attributes are not used for direct contrusting class prototypes. Instead, they serve as spatial composition bottlenecks to suppress potential overfitting caused by attribute-visual mismatches during training and provide advanced location guidance information during testing. Experiments demonstrate that InCAP achieves leading results on mainstream datasets, validating the full potential of this strategy.",https://openaccess.thecvf.com/content/ACCV2024/html/Shi_Do_they_Share_the_Same_Tail_Learning_Individual_Compositional_Attribute_ACCV_2024_paper.html
Multiview Detection with Cardboard Human Modeling,"Multiview detection uses multiple calibrated cameras with overlapping fields of views to locate occluded pedestrians. In this field, existing methods typically adopt a ""human modeling - aggregation"" strategy. To find robust pedestrian representations, some intuitively incorporate 2D perception results from each frame, while others use entire frame features projected to the ground plane. However, the former does not consider the human appearance and leads to many ambiguities, and the latter suffers from projection errors due to the lack of accurate height of the human torso and head. In this paper, we propose a new pedestrian representation scheme based on human point clouds modeling. Specifically, using ray tracing for holistic human depth estimation, we model pedestrians as upright, thin cardboard point clouds on the ground. Then, we aggregate the point clouds of the pedestrian cardboard across multiple views for a final decision. Compared with existing representations, the proposed method explicitly leverages human appearance and reduces projection errors significantly by relatively accurate height estimation. On four standard evaluation benchmarks, our method achieves very competitive results. Our code and data will be open-sourced.",https://openaccess.thecvf.com/content/ACCV2024/html/Ma_Multiview_Detection_with_Cardboard_Human_Modeling_ACCV_2024_paper.html
Class-Aware Contrastive Learning for Fine-Grained Skeleton-Based Action Recognition,"Graph convolutional networks have significantly advanced skeleton-based action recognition by efficiently processing non-mesh skeleton sequences. However, existing methods struggle with fine-grained action recognition due to the high similarity of samples across categories. In this paper, we propose a class-aware contrastive learning framework designed to emphasize subtle motion feature differences. Our approach enhances discriminative capability for fine-grained action recognition by refining negative sample selection in contrastive learning to prioritize samples from similar categories. Furthermore, our framework incorporates global context from multiple sequences during the graph learning process and utilizes memory banks to store rich instance information, enriching cross-sequence context understanding. Our method achieves remarkable performance compared to state-of-the-art methods on the NTU RGB+D, NW-UCLA, and FineGym datasets. Codes are available at: https://github.com/PRIS-CV/Class-Aware-Contrastive-Learning-for-Action-Recognition.",https://openaccess.thecvf.com/content/ACCV2024/html/Bian_Class-Aware_Contrastive_Learning_for_Fine-Grained_Skeleton-Based_Action_Recognition_ACCV_2024_paper.html
HAHA: Highly Articulated Gaussian Human Avatars with Textured Mesh Prior,"We present HAHA - a novel approach for animatable human avatar generation from monocular input videos. The proposed method relies on learning the trade-off between the use of Gaussian splatting and a textured mesh for efficient and high fidelity rendering. We demonstrate its efficiency to animate and render full-body human avatars controlled via the SMPL-X parametric model. Our model learns to apply Gaussian splatting only in areas of the SMPL-X mesh where it is necessary, like hair and out-of-mesh clothing. This results in a minimal number of Gaussians being used to represent the full avatar, and reduced rendering artifacts. This allows us to handle the animation of small body parts such as fingers that are traditionally disregarded. We demonstrate the effectiveness of our approach on two open datasets: SnapshotPeople and X-Humans. Our method demonstrates on par reconstruction quality to the state-of-the-art on SnapshotPeople, while using less than a third of Gaussians. HAHA outperforms previous state-of-the-art on novel poses from X-Humans both quantitatively and qualitatively.",https://openaccess.thecvf.com/content/ACCV2024/html/Svitov_HAHA_Highly_Articulated_Gaussian_Human_Avatars_with_Textured_Mesh_Prior_ACCV_2024_paper.html
Moving Object Segmentation: All You Need Is SAM (and Flow),"The objective of this paper is motion segmentation -- discovering and segmenting the moving objects in a video. This is a much studied area with numerous careful, and sometimes complex, approaches and training schemes including: self-supervised learning, learning from synthetic datasets, object-centric representations, amodal representations, and many more. Our interest in this paper is to determine if the Segment Anything model (SAM) can contribute to this task. We investigate two models for combining SAM with optical flow that harness the segmentation power of SAM with the ability of flow to discover and group moving objects. In the first model, we adapt SAM to take optical flow, rather than RGB, as an input. In the second, SAM takes RGB as an input, and flow is used as a segmentation prompt. These surprisingly simple methods, without any further modifications, outperform all previous approaches by a considerable margin in both single and multi-object benchmarks.  We also extend these frame-level segmentations to sequence-level segmentations that maintain object identity. Again, this simple model achieves outstanding performance across multiple moving object segmentation benchmarks.",https://openaccess.thecvf.com/content/ACCV2024/html/Xie_Moving_Object_Segmentation_All_You_Need_Is_SAM_and_Flow_ACCV_2024_paper.html
EQ-CBM: A Probabilistic Concept Bottleneck with Energy-based Models and Quantized Vectors,"The demand for reliable AI systems has intensified the need for interpretable deep neural networks. Concept bottleneck models (CBMs) have gained attention as an effective approach by leveraging human-understandable concepts to enhance interpretability. However, existing CBMs face challenges due to deterministic concept encoding and reliance on inconsistent concepts, leading to inaccuracies. We propose EQ-CBM, a novel framework that enhances CBMs through probabilistic concept encoding using energy-based models (EBMs) with quantized concept activation vectors (qCAVs). EQ-CBM effectively captures uncertainties, thereby improving prediction reliability and accuracy. By employing qCAVs, our method selects homogeneous vectors during concept encoding, enabling more decisive task performance and facilitating higher levels of human intervention. Empirical results using benchmark datasets demonstrate that our approach outperforms the state-of-the-art in both concept and task accuracy.",https://openaccess.thecvf.com/content/ACCV2024/html/Kim_EQ-CBM_A_Probabilistic_Concept_Bottleneck_with_Energy-based_Models_and_Quantized_ACCV_2024_paper.html
SpikeGS: Learning 3D Gaussian Fields from Continuous Spike Stream,"A spike camera is a specialized high-speed visual sensor that offers advantages such as high temporal resolution and high dynamic range compared to conventional frame cameras. These features provide the camera with significant advantages in many computer vision tasks. However, the tasks of novel view synthesis based on spike cameras remain underdeveloped. Although there are existing methods for learning neural radiance fields from spike stream, they either lack robustness in extremely noisy, low-quality lighting conditions or suffer from high computational complexity due to the deep fully connected neural networks and ray marching rendering strategies used in neural radiance fields, making it difficult to recover fine texture details. In contrast, the latest advancements in 3DGS have achieved high-quality real-time rendering by optimizing the point cloud representation into Gaussian ellipsoids. Building on this, we introduce SpikeGS, the method to learn 3D Gaussian fields solely from spike stream. We designed a differentiable spike stream rendering framework based on 3DGS, incorporating noise embedding and spiking neurons. By leveraging the multi-view consistency of 3DGS and the tile-based multi-threaded parallel rendering mechanism, we achieved high-quality real-time rendering results. Additionally, we introduced a spike rendering loss function that generalizes under varying illumination conditions. Our method can reconstruct view synthesis results with fine texture details from a continuous spike stream captured by a moving spike camera, while demonstrating high robustness in extremely noisy low-light scenarios. Experimental results on both real and synthetic datasets demonstrate that our method surpasses existing approaches in terms of rendering quality and speed.",https://openaccess.thecvf.com/content/ACCV2024/html/Yu_SpikeGS_Learning_3D_Gaussian_Fields_from_Continuous_Spike_Stream_ACCV_2024_paper.html
M-RAT: a Multi-grained Retrieval Augmentation Transformer for Image Captioning,"Current encoder-decoder methods for image captioning mainly consist of an object detection module (two-stage), or rely on big models with large-scale datasets to improve the effectiveness, which leads to increasing computation costs and cannot introduce new external knowledge. In this paper, we propose a novel end-to-end method Multi-grained Retrieval Augmentation Transformer (M-RAT) that innovatively fuses retrieved text derived from a changeable datastore with input visual feature through a Multi-modal Aligned Encoder, and introduce a specialized attention mechanism, Multi-MSA, to exploit both local and global interactions for delicate fine-grained details. Additionally, we enhance the decoder generation ability by employing low-level and high-level fused embeddings. Experiments demonstrate that M-RAT achieves comparable performance to state-of-the-art baselines with remarkable accuracy and details, as well as showing excellent domain adaptability for novel objects.",https://openaccess.thecvf.com/content/ACCV2024/html/Song_M-RAT_a_Multi-grained_Retrieval_Augmentation_Transformer_for_Image_Captioning_ACCV_2024_paper.html
Cross-Modality Complementary Learning for Video-based Cloth-Changing Person Re-Identification,"Video-based Cloth-Changing Person Re-ID (VCCRe-ID) is a real-world Re-ID problem where individuals are observed in settings with a high likelihood of clothing changes between observations. To tackle this problem, capturing cloth-invariant modalities remains more effective than texture-based approaches. However, previous works extracted these modalities separately and directly leveraged the learned features for Re-ID, which is not effective since viewpoint changes and occlusion cause severe ambiguity in these modalities. To address this limitation, we propose a dual-branch framework that couples cloth-invariant modalities (i.e. shape and gait) with appearance by novelly exploiting the complementary relationship across them. In this work, we design a texture branch that enables body shape to complement the ambiguity in appearance caused by illumination variations or occlusions. Then texture and gait features are mutually learned at multiple levels, which helps to exchange beneficial information across branches for more discriminative person representations. We build a large-scale video-based cloth-changing dataset that contains the most cloth variations and is the first benchmark that mimics the real-world similar-clothing scenario. Extensive experiments show that our proposed framework outperforms state-of-the-art methods by a large margin. Code and dataset will be available at https://github.com/dustin-nguyen-qil/CCL-VCCReID.",https://openaccess.thecvf.com/content/ACCV2024/html/Nguyen_Cross-Modality_Complementary_Learning_for_Video-based_Cloth-Changing_Person_Re-Identification_ACCV_2024_paper.html
ULTRON: Unifying Local Transformer and Convolution for Large-scale Image Retrieval,"In large-scale image retrieval, the primary goal is to extract discriminative features and embed them into global image representations. Previous methods based on CNNs effectively learn local features and create robust representations, leading to strong performance. Transformers that excel in learning global context, however, often struggle to extract fine details and therefore do not perform well in large-scale landmark recognition. In this paper, we propose a novel hybrid architecture named ULTRON, which combines transformer blocks with local self-attention and a convolution-based encoder. Our local transformer block contains an advanced self-attention mechanism that enhances the spatial context awareness of key features and updates the value features by considering broader information within fixed-size regional windows. In addition, we have designed a channel-wise dilated convolution that adjusts dilation per channel, enabling effective multiscale feature learning while robustly capturing local features. We focus on learning local contexts throughout the entire network and effectively blending these contexts in the attention-based pooling process. This approach generates a powerful global representation that includes local information, relying solely on classification loss without requiring additional modules to capture local features. Experimental results demonstrate that our model outperforms previous works due to the effective integration of local and global information.",https://openaccess.thecvf.com/content/ACCV2024/html/Kweon_ULTRON_Unifying_Local_Transformer_and_Convolution_for_Large-scale_Image_Retrieval_ACCV_2024_paper.html
KhmerST: A Low-Resource Khmer Scene Text Detection and Recognition Benchmark,"Developing effective scene text detection and recognition models hinges on extensive training data, which can be both laborious and costly to obtain, especially for low-resourced languages. Conventional methods tailored for Latin characters often falter with non-Latin scripts due to challenges like character stacking, diacritics, and variable character widths without clear word boundaries. In this paper, we introduce the first Khmer scene-text dataset, featuring 1,544 expert-annotated images, including 997 indoor and 547 outdoor scenes. This diverse dataset includes flat text, raised text, poorly illuminated text, distant and partially obscured text. Annotations provide line-level text and polygonal bounding box coordinates for each scene.The benchmark includes baseline models for scene-text detection and recognition tasks, providing a robust starting point for future research endeavors. The KhmerST dataset is publicly accessible.",https://openaccess.thecvf.com/content/ACCV2024/html/Nom_KhmerST_A_Low-Resource_Khmer_Scene_Text_Detection_and_Recognition_Benchmark_ACCV_2024_paper.html
On Unsupervised Partial Shape Correspondence,"While dealing with matching shapes to their parts, we often apply a tool known as functional maps. The idea is to translate the shape matching problem into ""convenient"" spaces by which matching is performed algebraically by solving a least squares problem. Here, we argue that such formulations, though popular in this field, introduce errors in the estimated match when partiality is invoked. Such errors are unavoidable even  for advanced feature extraction networks, and they can be shown to escalate with increasing degrees of shape partiality, adversely affecting the learning capability of such systems. To circumvent these limitations, we propose a novel approach for partial shape matching. Our study of functional maps led us to a novel method that establishes direct correspondence between partial and full shapes through feature matching bypassing the need for functional map intermediate spaces. The Gromov distance between metric spaces leads to the construction of the first part of our loss functions. For regularization we use two options: a term based on the area preserving property of the mapping, and a relaxed version that avoids the need to resort to functional maps. The proposed approach shows superior performance on the SHREC'16 dataset, outperforming existing unsupervised methods for partial shape matching. Notably, it achieves state-of-the-art results on the SHREC'16 HOLES benchmark, superior also compared to supervised methods. We demonstrate the benefits of the proposed unsupervised method when applied to a new dataset PFAUST for part-to-full shape correspondence and make the dataset publicly available.",https://openaccess.thecvf.com/content/ACCV2024/html/Bracha_On_Unsupervised_Partial_Shape_Correspondence_ACCV_2024_paper.html
Learning Non-Uniform Step Sizes for Neural Network Quantization,"Quantization of neural networks enables faster inference, reduced memory usage, and lower energy consumption, all of which are crucial for deploying AI algorithms on devices. However, quantization may degrade performance compared to full-precision models as precision decreases. While prior research has primarily focused on uniformly quantizing network weights and activations, capturing the long-tail distributions of these quantities imposes a challenge. To address this issue, this paper introduces a non-uniform learned step-size quantization (nuLSQ) approach. It optimizes individual step sizes for quantizing weights and activations. Evaluations on CIFAR-10/100 and ImageNet datasets, using ResNet, MobileNetV2, Swin-T, and ConvNeXT with 2-, 3-, and 4-bit precisions, demonstrate that nuLSQ  outperforms other quantization methods. The code is available at https://github.com/DensoITLab/nuLSQ.",https://openaccess.thecvf.com/content/ACCV2024/html/Gongyo_Learning_Non-Uniform_Step_Sizes_for_Neural_Network_Quantization_ACCV_2024_paper.html
